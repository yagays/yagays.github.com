<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Wolfeyes Bioinformatics beta]]></title>
  <link href="http://yagays.github.io/atom.xml" rel="self"/>
  <link href="http://yagays.github.io/"/>
  <updated>2013-07-16T07:43:07+09:00</updated>
  <id>http://yagays.github.io/</id>
  <author>
    <name><![CDATA[yag_ays]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[RNA-Seqの数理―生成モデルによる発現量推定（2）Count Based ModelとRPKM]]></title>
    <link href="http://yagays.github.io/blog/2013/07/16/mathematical-models-for-transcriptome-quantification-2/"/>
    <updated>2013-07-16T00:00:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/16/mathematical-models-for-transcriptome-quantification-2</id>
    <content type="html"><![CDATA[<ul>
<li>まとめ：<a href="http://yagays.github.io/blog/2013/07/14/mathematical-models-for-transcriptome-quantification-0/">RNA-Seqの数理―生成モデルによる発現量推定：アーカイブ - Wolfeyes Bioinformatics beta</a></li>
<li>前回：<a href="http://yagays.github.io/blog/2013/07/15/mathematical-models-for-transcriptome-quantification-1/">RNA-Seqの数理―生成モデルによる発現量推定（1）RNA-Seqの発現量推定とRPKM - Wolfeyes Bioinformatics beta</a></li>
</ul>


<h3>Count Based Model</h3>

<p>今回はCount Based Modelと呼ばれる最も基本的な生成モデルを作って発現量推定を定式化し，それがRPKMの計算式と同じことを示してみよう．なお，以降の内容はPachter 2011をベースにしている．</p>

<blockquote><p>Models for transcript quantification from RNA-Seq. q-bio.GN, (2011).</p>

<p><a href="http://arxiv.org/abs/1104.3889v2">http://arxiv.org/abs/1104.3889v2</a></p></blockquote>

<h3>Count Based Modelにおける仮定</h3>

<p>Count based Modelは非常にシンプルなモデルだ．単純化のために以下のような条件を置いて，RNA-Seqの発現量推定をモデル化している．</p>

<ul>
<li>NGSで得られるショートリードはシングルエンド</li>
<li>ショートリードはただ1箇所だけにマッピングされる</li>
<li>すべての転写物はただ1つのアイソフォームを持つ</li>
</ul>


<p>遺伝子の構造が単純な生物において初期のNGS解析をしていると思えばいいだろう．現実的にはこのような状態というのはあり得ないが，転写物からショートリードが生成されるという核の部分はしっかりと残している．</p>

<h3>Count Based Modelの生成過程</h3>

<p>生成モデルというものは，まず初めに観測データであるショートリードの生成過程を立てる．今回のCount Based Modelでは</p>

<blockquote><p><strong>トランスクリプトーム全体からある転写物が選ばれ，任意の開始点からショートリードがシーケンスされた</strong></p></blockquote>

<p>と考える．<a href="http://yagays.github.io/blog/2013/07/15/mathematical-models-for-transcriptome-quantification-1/">前回の記事で概要を示した</a>ように，RNA-Seqの実験プロトコルは幾つかの複雑な段階を踏んでショートリードをシーケンスしていた．しかしながら，その多くの段階は偶然の連続によって，たまたま転写物がフラグメントとしてある部分から断片化され，たまたまシーケンスされてショートリードとして出力されたとみなすことができる．これらを確率的な事象としてみなすことによって，生成モデルとして確率的な視点からシーケンスデータが生じた背景にあるRNA-Seqの発現量というものを推定することができる．</p>

<br/>


<p>さて，長ったらしい文章での前置きはここまでにして，ここから具体的に生成過程を数式に落としこんでいく．まず最初にこれまで言葉で説明してきたものを変数や集合として定義をし，事象の起こりやすさを尤度として表現する．概念と数式の対応関係が少し分かりにくいかもしれないが，もし要領が掴めないようならこの後に書いてある具体例のところの図も参照しつつ丁寧に追ってほしい．</p>

<h3>尤度の定式化</h3>

<p>転写物の集合<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=T+" alt="T " />に含まれる転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />は，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Ctilde%7Bl_t%7D+" alt="\tilde{l_t} " />の有効配列長を持ち，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Csum_%7Bt%5Cin+T%7D+%5Crho_t+%3D+1+" alt="\sum_{t\in T} \rho_t = 1 " />となるような<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />という相対的な発現量を持つ．RNA-Seqによって得られたショートリードを<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f+" alt="f " />，その全体を<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=F+" alt="F " />とし，転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />にマッピングされたショートリードの集合を<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=F_t+" alt="F_t " />とする．</p>

<p>このとき，ショートリード<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f+" alt="f " />が転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />から生じる確率は，以下の式で表すことができる．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f%5Cin+t%29%3D+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_t%7D%7D+" alt="p(f\in t)= \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_t}} " /></p>

<p>次に，選ばれた転写物のどの位置からショートリードがシーケンスされるかはランダムに選ばれるため，この確率<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f%5Cin+t%29+" alt="p(f\in t) " />に<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+" alt="\frac{1}{\tilde{l_t}} " />を掛けあわせた値が，ショートリード<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f+" alt="f " />が転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />のある開始点から生成される確率となる．</p>

<p>このとき，観測されたショートリード全体<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=F+" alt="F " />の尤度は，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />の関数として以下のように表される．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29%3D+%5Cprod_%7Bt%5Cin+T%7D%5Cprod_%7Bf%5Cin+F_t%7D+%5Cbigg%28+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_r%7D%7D%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Cbigg%29+" alt="\mathcal{L}(\rho)= \prod_{t\in T}\prod_{f\in F_t} \bigg( \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_r}}\frac{1}{\tilde{l_t}} \bigg) " /></p>

<p>そして，転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />にマッピングされたショートリードの本数<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=X_t+" alt="X_t " />を使って，以下のように書き換えられる．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29%3D+%5Cprod_%7Bt%5Cin+T%7D+%5Cbigg%28+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_r%7D%7D%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Cbigg%29%5E%7BX_t%7D+" alt="\mathcal{L}(\rho)= \prod_{t\in T} \bigg( \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_r}}\frac{1}{\tilde{l_t}} \bigg)^{X_t} " /></p>

<p>これがCount Based Modelにおける尤度である．</p>

<h3>具体例を用いて尤度を導出する</h3>

<h4>確率の考え方</h4>

<p>といっても，これだけでは何のことか分からないので，具体例を使って詳しく解説していこう．単純化のために，以下の図のような3つの転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t_1%2C+t_2%2C+t_3+" alt="t_1, t_2, t_3 " />と6本のショートリード<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f_1%2Cf_2%5Cdots%2Cf_6+" alt="f_1,f_2\dots,f_6 " />が以下のようにマッピングされたという状態を想定する．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/generative-models_1.png" alt="" /></p>

<p>3つの転写物にはそれぞれ発現量を持っている．例えば，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t_1+" alt="t_1 " />は20，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t_2+" alt="t_2 " />は5，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t_3+" alt="t_3 " />は15という具合だ．今回は発現量を確率値として考えるため，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cfrac%7B20%7D%7B40%7D%2C%5Cfrac%7B5%7D%7B40%7D%2C%5Cfrac%7B15%7D%7B40%7D%2C+" alt="\frac{20}{40},\frac{5}{40},\frac{15}{40}, " />のように相対値として表す．これが<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />であり，総和が1になる理由だ．この<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />は今回の最尤推定法で最終的に求めたい値である．</p>

<p>さて，ショートリード<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f_1+" alt="f_1 " />が転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t_1+" alt="t_1 " />から生成される確率は，先ほどの定式化で<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f%5Cin+t%29%3D+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_t%7D%7D+" alt="p(f\in t)= \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_t}} " />としていた．これはある転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />の長さと相対的な発現量を掛けあわせたものを，全体で足しあわせた値で割っており，ただの発現量の比率だけでなく転写物の長さも関係しているということを表している．ランダムに一つ選んでシーケンスするということを考えれば，多く転写されていてなおかつ長い転写物のほうが選ばれやすいのは明らかだろう．この式は，そういった意味を数式として厳密に表現しているだけなのだ．</p>

<p>次に，転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />が選ばれた後に，今度はその転写物のどの位置の配列が読まれたかということを考える．これは端っこの方の配列が読まれたのか真ん中の方の配列が読まれたのかといった情報だ．このときに出てくるのが有効配列長Effective lengthと呼ばれる値で，ショートリードの長さを<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=m+" alt="m " />としたときの<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Ctilde%7Bl_t%7D%3Dl_t+-+m+%2B+1+" alt="\tilde{l_t}=l_t - m + 1 " />で表される．これは単純に長さ<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=l_t+" alt="l_t " />から長さ<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=m+" alt="m " />の配列を取ってくる時の場合の数である．今回のモデルでは，シーケンスされる場所は場所に依存せず一様なランダムで決まるとしているので，有効配列長で割り算をしている．</p>

<p>そして，これをすべてのショートリードである<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f_1+" alt="f_1 " />から<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f_6+" alt="f_6 " />まですべて掛けあわせることで，今回得られたショートリードが生成される確率というものを求めることができる．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29%3D%5Cfrac%7Bp%28f_1%5Cin+t_1%29%7D%7B%5Ctilde%7Bl_1%7D%7D%5Ctimes%5Cfrac%7Bp%28f_2%5Cin+t_1%29%7D%7B%5Ctilde%7Bl_1%7D%7D%5Ctimes%5Cfrac%7Bp%28f_3%5Cin+t_1%29%7D%7B%5Ctilde%7Bl_1%7D%7D+" alt="\mathcal{L}(\rho)=\frac{p(f_1\in t_1)}{\tilde{l_1}}\times\frac{p(f_2\in t_1)}{\tilde{l_1}}\times\frac{p(f_3\in t_1)}{\tilde{l_1}} " /><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Ctimes%5Cfrac%7Bp%28f_4%5Cin+t_2%29%7D%7B%5Ctilde%7Bl_2%7D%7D%5Ctimes%5Cfrac%7Bp%28f_5%5Cin+t_2%29%7D%7B%5Ctilde%7Bl_2%7D%7D%5Ctimes%5Cfrac%7Bp%28f_6%5Cin+t_3%29%7D%7B%5Ctilde%7Bl_3%7D%7D+" alt="\times\frac{p(f_4\in t_2)}{\tilde{l_2}}\times\frac{p(f_5\in t_2)}{\tilde{l_2}}\times\frac{p(f_6\in t_3)}{\tilde{l_3}} " /></p>

<p>この確率<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29+" alt="\mathcal{L}(\rho) " />が尤度と呼ばれる．尤度とは「もっともらしさ」という意味だ．上で仮定したような<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=f_1%2Cf_2%5Cdots%2Cf_6+" alt="f_1,f_2\dots,f_6 " />のショートリードのデータが実際に手元に得られたのだから，それが起こる確率が高かったに違いない，ということを尤度では考えている．</p>

<h4>尤度式の展開</h4>

<p>では次に，この式をもう少し綺麗にしていこう．まずは2行目で転写物ごとに式をまとめ，3行目ですべての転写物を一つの式で表している．そして最後に<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f%5Cin+t%29+" alt="p(f\in t) " />を置き換えると，先ほどの<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29%3D+%5Cprod_%7Bt%5Cin+T%7D%5Cprod_%7Bf%5Cin+F_t%7D+%5Cbigg%28+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_r%7D%7D%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Cbigg%29+" alt="\mathcal{L}(\rho)= \prod_{t\in T}\prod_{f\in F_t} \bigg( \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_r}}\frac{1}{\tilde{l_t}} \bigg) " />まで導出することができる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/generative-models_2.png" alt="" /></p>

<p>そして最後のまとめとして，転写物ごとにマッピングされたショートリードの生成確率というのは同じであることを利用して，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=X_t+" alt="X_t " />でまとめる．
つまり，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f_1%5Cin+t_1%29+%3D++p%28f_2%5Cin+t_1%29+%3D++p%28f_3%5Cin+t_1%29" alt="p(f_1\in t_1) =  p(f_2\in t_1) =  p(f_3\in t_1)" />なので，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=p%28f_1%5Cin+t_1%29+%5Ctimes+p%28f_2%5Cin+t_1%29+%5Ctimes+p%28f_3%5Cin+t_1%29+%3D+%28p%28f%5Cin+t_1%29%29%5E%7BX_1%7D+" alt="p(f_1\in t_1) \times p(f_2\in t_1) \times p(f_3\in t_1) = (p(f\in t_1))^{X_1} " />と変形している．こうすることにより，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Crho%29%3D+%5Cprod_%7Bt%5Cin+T%7D+%5Cbigg%28+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_r%7D%7D%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Cbigg%29%5E%7BX_t%7D+" alt="\mathcal{L}(\rho)= \prod_{t\in T} \bigg( \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_r}}\frac{1}{\tilde{l_t}} \bigg)^{X_t} " />となる．</p>

<h3>尤度を最大にするパラメータを解析的に求める</h3>

<p>では，数式として表された尤度を最大化する値を求めよう．まずは上の式を少し整理するところから始める．<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />という新たな変数を用意して<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />を<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />に置き換える．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+%3D+p%28f%5Cin+t%29%3D+%5Cfrac%7B%5Crho_t+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D%5Crho_r+%5Ctilde%7Bl_t%7D%7D+" alt="\alpha_t = p(f\in t)= \frac{\rho_t \tilde{l_t}}{\sum_{r\in T}\rho_r \tilde{l_t}} " /></p>

<p>この<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />を用いて，先ほどの尤度式を書き換えると</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D%28%5Calpha_t%29%3D%5Cprod_%7Bt%5Cin+T%7D%5Cbigg%28+%5Cfrac%7B%5Calpha_t%7D%7B%5Ctilde%7Bl_t%7D%7D%5Cbigg%29%5E%7BX_t%7D+%5Cpropto+%5Cprod_%7Bt%5Cin+T%7D+%7B%5Calpha_t%7D%5E%7BX_t%7D+" alt="\mathcal{L}(\alpha_t)=\prod_{t\in T}\bigg( \frac{\alpha_t}{\tilde{l_t}}\bigg)^{X_t} \propto \prod_{t\in T} {\alpha_t}^{X_t} " /></p>

<p>となる．ここで，尤度<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D+" alt="\mathcal{L} " />の変数が<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />になっていることに注意．最尤推定の目的としては尤度<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cmathcal%7BL%7D+" alt="\mathcal{L} " />を最大化する<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />を求めたいだけなので，関係ない<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=l_t+" alt="l_t " />は消している．</p>

<p>そして，この尤度を最大にする<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Calpha_t+" alt="\alpha_t " />をラグランジュの未定乗数法を用いて求めると，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Calpha_t%7D+%3D+%5Cfrac%7BX_t%7D%7BN%7D+" alt="\hat{\alpha_t} = \frac{X_t}{N} " />となる．以下の図は，ラグランジュの未定乗数法の導出部分の詳細．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/generative-models_3.png" alt="" /></p>

<p>このときの<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />は<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+%3D+%5Cfrac%7B%5Calpha_t+%2F+%5Ctilde%7Bl_t%7D%7D%7B%5Csum_%7Br%5Cin+T%7D+%5Calpha_t+%2F+%5Ctilde%7Bl_t%7D%7D+" alt="\rho_t = \frac{\alpha_t / \tilde{l_t}}{\sum_{r\in T} \alpha_t / \tilde{l_t}} " />で表されるので，先ほど求めた<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Calpha_t%7D+" alt="\hat{\alpha_t} " />を代入して，</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Crho_t%7D+%3D+%5Cfrac%7BX_t%7D%7BN%7D+%5Ccdot+%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Ccdot+%5Cbigg%28+%5Cfrac%7B1%7D%7B%5Csum_%7Br%5Cin+T%7D%5Cfrac%7BX_r%7D%7BN+l_r%7D%7D+%5Cbigg%29+" alt="\hat{\rho_t} = \frac{X_t}{N} \cdot \frac{1}{\tilde{l_t}} \cdot \bigg( \frac{1}{\sum_{r\in T}\frac{X_r}{N l_r}} \bigg) " /></p>

<p>と表すことができる．これが，Count Based Modelによって求められた発現量<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Crho_t+" alt="\rho_t " />の推定値である．</p>

<h3>Count Based Modelの解析解とRPKMとの関係</h3>

<p>さて，ようやくRPKMとの関連の話に入ることができる．上で求めた<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Crho_t%7D+" alt="\hat{\rho_t} " />の式を少し式変形する．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Crho_t%7D+%3D+%5Cfrac%7BX_t%7D%7BN%7D+%5Ccdot+%5Cfrac%7B1%7D%7B%5Ctilde%7Bl_t%7D%7D+%5Ccdot+%5Cbigg%28+%5Cfrac%7B1%7D%7B%5Csum_%7Br%5Cin+T%7D%5Cfrac%7BX_r%7D%7BN+l_r%7D%7D+%5Cbigg%29+" alt="\hat{\rho_t} = \frac{X_t}{N} \cdot \frac{1}{\tilde{l_t}} \cdot \bigg( \frac{1}{\sum_{r\in T}\frac{X_r}{N l_r}} \bigg) " /></p>

<p>この式をよく見ると，後半の<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Cbigg%28+%5Cfrac%7B1%7D%7B%5Csum_%7Br%5Cin+T%7D%5Cfrac%7BX_r%7D%7BN+l_r%7D%7D+%5Cbigg%29+" alt="\bigg( \frac{1}{\sum_{r\in T}\frac{X_r}{N l_r}} \bigg) " />は全ての転写物に関して計算しているだけなので，ただの定数である．ということで，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Crho_t%7D+" alt="\hat{\rho_t} " />に関係してくる転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />の変数だけを抜き出すと，</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Chat%7B%5Crho_t%7D+%5Cpropto+%5Cfrac%7BX_t%7D%7B%5Ctilde%7Bl_t%7D+N%7D+" alt="\hat{\rho_t} \propto \frac{X_t}{\tilde{l_t} N} " /></p>

<p>ここに<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=10%5E9+" alt="10^9 " />を掛ければ，まさにRPKMの式<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=RPKM+%5C++for+%5C++transcript+%5C+t+%3D+10%5E9+%5Ctimes%5Cfrac%7BX_t%7D%7Bl_t+N%7D" alt="RPKM \  for \  transcript \ t = 10^9 \times\frac{X_t}{l_t N}" />と全く同じ式になっている！！！</p>

<p>ということで，Count Based Modelにおける最尤推定によって求められた解析解から推定される発現量と，マッピングされたショートリードの数だけで計算されたRPKMの発現量は，基本的には同じ計算式によって求められていることが求められた．</p>

<h3>まとめ</h3>

<p>後半はだいぶ駆け足になってしまったが，Count Based Modelの導出とRPKMとの関係を解説してみた．生成モデルというややこしい数式を立てておいて結局RPKMと同じかよと思うかもしれないが，生成モデルはここから様々な仮定をモデルに組み込むことによって，より複雑かつ現実に則した一般的なモデルへと改良していくことができる．今回のCount Based Modelはその枠組みとして，非常に基本的な考え方である．しかしながら，実際にはこの方法だけを使ってソフトウェアが作られたり論文として出ているわけでもなく，あまり詳しい解説が無い部分だと思われる．今回はLior Pachter氏のarXiv.orgに投稿されたレビューをもとに具体例をはさみながらモデルを読み解いていったが，次回以降はもう少しCufflinksで使われているような技術も導入しつつ，より複雑なモデルへと進んでいきたい．</p>

<h3>参考</h3>

<ul>
<li><a href="http://arxiv.org/abs/1104.3889v2">[1104.3889v2] Models for transcript quantification from RNA-Seq</a></li>
<li><a href="http://fgcz-intranet.uzh.ch/publish/BiostatLecture/RNA-seq-quantification.pdf">http://fgcz-intranet.uzh.ch/publish/BiostatLecture/RNA-seq-quantification.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RNA-Seqの数理―生成モデルによる発現量推定（1）RNA-Seqの発現量推定とRPKM]]></title>
    <link href="http://yagays.github.io/blog/2013/07/15/mathematical-models-for-transcriptome-quantification-1/"/>
    <updated>2013-07-15T00:06:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/15/mathematical-models-for-transcriptome-quantification-1</id>
    <content type="html"><![CDATA[<ul>
<li>まとめ：<a href="http://yagays.github.io/blog/2013/07/14/mathematical-models-for-transcriptome-quantification-0/">RNA-Seqの数理―生成モデルによる発現量推定：アーカイブ - Wolfeyes Bioinformatics beta</a></li>
</ul>


<p>生成モデルの話を始める前に，まずはRNA-Seqの発現量としてよく知られているRPKMという指標からスタートし，生成モデルの話へと繋げていこう．</p>

<h3>RNA-Seqの発現量推定の基本</h3>

<p>RNA-Seqで得られるデータは，ある長さの配列（ショートリード）とクオリティスコアのセットして表現される．それをリファレンスとなるゲノム配列の中から探しだして，読まれた配列が何の転写物由来かを見つける作業をマッピングという．このマッピングをNGSから出力された数千万本/数億本のショートリードに対して行い，ある遺伝子の部位にどのくらい貼り付いたかをカウントすることで特定の転写物がどれくらい発現していたかを推定するという方法が，RNA-Seqの発現量推定では基本となる．</p>

<p><a href="http://www.nature.com/nrg/journal/v10/n1/fig_tab/nrg2484_F1.html" target="_blank"><img src="http://dl.dropboxusercontent.com/u/142306/b/nrg2484-f1.jpg" alt="CCC License Number: 3187871036736 "></a></p>

<p>この図はRNA-Seqの実験の流れを模式的に表している．まず対象となるトランスクリプトームであるmRNAを取り出し，断片化やアダプターの付与を経てシーケンスライブラリを作成する．次にNGSによって断片が大量にシーケンスされ，いわゆるATCGからなる文字列で構成されたショートリードが得られる．そしてゲノム配列にマッピングすることによって，RNAの発現量が推定される．この図では，ゲノム配列の各塩基ごとにどれくらいの本数のショートリードが貼り付いたかをカウントすることによって，RNA expression levelを図示している．Nucleotide positionによって特定の領域の発現量が欠損していたり遺伝子の末端で発現量が少なくなっているのには，マッピングの際に生じる幾つかの理由がある．まず，図中のJunction readsで示されている点線の箇所は，イントロンのスプライシングが起きた領域であり，mRNAの配列をゲノム配列にマッピングするために起こる現象である．そのため，イントロンの領域だけ発現量が無いように見える．また，Poly-A tailが配列の一部として混入することでショートリード自体がゲノム配列にマッピングできない場合もあり，末端の発現量が落ちる原因の一つになっている．</p>

<p>では，このようにして求められた値を転写物ごとの発現量として一つの値で表現するには，どうすれば良いのか．ここで，RPKMという考え方が登場する．</p>

<h3>RPKMの考え方</h3>

<p>RNA-Seqの発現量の値として最初に考えられ今でも広く使われているRPKMという指標は，配列の本数を数えるという方法で転写物ごとの発現量を求めている．RPKMはreads per kilobase of exon per million mapped sequence readsの略称で，マッピングされたショートリードの数をエキソンの長さとシーケンサで読まれた配列の総数で正規化した値である．RPKMの計算方法を式で表すと以下のようになる．</p>

<p><img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=RPKM+%5C++for+%5C++transcript+%5C+t+%3D+10%5E9+%5Ctimes%5Cfrac%7BX_t%7D%7Bl_t+N%7D" alt="RPKM \  for \  transcript \ t = 10^9 \times\frac{X_t}{l_t N}" /></p>

<p>ここで，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=X_t+" alt="X_t " />は転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />にマッピングされたショートリードの本数，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=l_t+" alt="l_t " />は転写物<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=t+" alt="t " />の長さ(bp)，<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=N+" alt="N " />はマッピングされたショートリードの総数を表している．</p>

<p>さて，このRPKMという発現量の考え方の裏には以下のような仮定がある．</p>

<ul>
<li>配列数が長い転写物からはショートリードが多く読まれる</li>
<li>全体で読まれるショートリードの数が多いと，マッピングされるショートリードも多くなる</li>
</ul>


<p>NGSによるRNA-Seqでは，特定の量のサンプルの中に含まれるトランスクリプトームをランダムにシーケンスするため，出てくるデータというのはあくまでも相対的な量となってしまう．そのため，このような仮定を置くことで転写物の発現や実験サンプルなどの違いを補正している．RPKMはある遺伝子の領域内に含まれるショートリードの本数を数え，転写物の長さと全体のデータ数で割って10<sup>9</sup> を掛けることによって簡単に求められるため，計算のしやすさと直感的な分かりやすさがある．</p>

<h3>RPKMと生成モデルとの関係</h3>

<p>このように，RPKMはマッピングされた配列の本数を数えるという単純な方法で発現量を推定している．一方で，これから考えていくことになる生成モデルによる発現量推定は，データが観測される事象を確率的に推定することでデータが与えられたときのトランスクリプトームの発現量というものを求める．これらは一見して全く違うもののように見えるのだが，<strong>実際には最も単純な生成モデルはRPKMと基本的な考え方が同じになる</strong>．つまり，RPKMも単純な生成モデルによる推定方法も，発現量はマッピングされたショートリードの本数と，転写物の長さおよびマッピングされたショートリードの総数の逆数によって決まるということだ．</p>

<p>それを確かめるために，次回はCount Based Modelと呼ばれる生成モデルを作って発現量推定を定式化し，それがRPKMの計算式と同じ意味を持つということを示してみよう．</p>

<ul>
<li>次：<a href="http://yagays.github.io/blog/2013/07/16/mathematical-models-for-transcriptome-quantification-2/">RNA-Seqの数理―生成モデルによる発現量推定（2）Count Based ModelとRPKM - Wolfeyes Bioinformatics beta</a></li>
</ul>


<h3>参考</h3>

<ul>
<li><a href="http://www.nature.com/nmeth/journal/v5/n7/full/nmeth.1226.html">Mapping and quantifying mammalian transcriptomes by RNA-Seq - Nature Methods</a></li>
<li><a href="http://www.nature.com/nrg/journal/v10/n1/full/nrg2484.html">RNA-Seq: a revolutionary tool for transcriptomics : Article : Nature Reviews Genetics</a></li>
<li><a href="http://arxiv.org/abs/1104.3889v2">[1104.3889v2] Models for transcript quantification from RNA-Seq</a></li>
<li><a href="http://fgcz-intranet.uzh.ch/publish/BiostatLecture/RNA-seq-quantification.pdf">http://fgcz-intranet.uzh.ch/publish/BiostatLecture/RNA-seq-quantification.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RNA-Seqの数理―生成モデルによる発現量推定：アーカイブ]]></title>
    <link href="http://yagays.github.io/blog/2013/07/14/mathematical-models-for-transcriptome-quantification-0/"/>
    <updated>2013-07-14T10:43:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/14/mathematical-models-for-transcriptome-quantification-0</id>
    <content type="html"><![CDATA[<p>この「RNA-Seqの数理」シリーズでは，次世代シーケンサを用いたRNA-Seqにおける発現量推定の数理モデルを理解することを目的とする．</p>

<p>副題にある「生成モデル」とは，観測データの生成過程を確率的にモデル化し，データが与えられたときの事後確率を用いて分類したいクラスや予測したい値を推定する方法のことを指す．今回のRNA-Seqにおける生成モデルでは，次世代シーケンサで読み取られた配列が，どのようにして細胞内のトランスクリプトームから実験によって読み取られて観測されたかという一連の流れを，生成モデルとして表現する．そして確率的なもっともらしさやパラメータの推定をおこない，トランスクリプトームの発現量を求める．</p>

<h3>このシリーズについて</h3>

<p>次世代シーケンサによるRNA-Seqの発現量推定といっても，実験対象は大腸菌レベルから人に至るまでゲノムサイズや遺伝子数は多種多様であり，実験機器も各メーカーごとに独自に改良が加えられてきた．しかしながら，RNA-Seqはトランスクリプトームの配列を超並列に大量に解読するという点で，基本的な原理は生物種やプラットフォームに依存しない．まずはこの共通部分に関して単純なモデルを作成し，そこから個別の生物の特性や実験方法に合わせた複雑なモデルを構築していく．</p>

<p>今回のシリーズは前提条件として，発現量推定の対象とする種をゲノム既知なモデル生物に限定する．つまり，ヒトやマウス，シロイヌナズナ，線虫など，すでにゲノム配列が解読されていて，なおかつ遺伝子の配列や機能がよく研究されている種を対象にするということだ．この仮定をおかないと，そもそもデータ処理の時点でショートリードのマッピングができなかったり，モデルが立てられないということになりかねない．とはいうものの，仮定を立てたからといってマッピングの曖昧さは解消されないし，アイソフォームの存在や選択的スプライシングなど複雑な転写メカニズムはいくらでも考えられるので，それらは追々モデルに組み込んでいく．</p>

<h3>注意</h3>

<p>このシリーズの記事で書かれていることはすべて個人的な勉強記録であって，数理的な解釈や導出の厳密性を保証するものではない．勘違いや間違いなどが多分に含まれている可能性があるので，参考にされる際には十分に注意していただきたい．もし間違いや誤字脱字等見つけられた方は，コメント欄などで指摘していただければ幸い．</p>

<h3>このシリーズの目標</h3>

<p>このシリーズの目標（そして個人的な勉強の目標）としては，</p>

<ul>
<li>CufflinksやRSEMなどのスタンダードとなっている発現量推定の数理的背景を理解する</li>
<li>発現差異解析(DE)などの複数サンプルにまたがる発現量比較の手法の理解につなげる</li>
</ul>


<p>ことを考えている．基本的には論文に記載されている内容をベースに進めていくので，論文のmethodの理解が最終的な目標となるだろう．</p>

<h3>記事まとめ</h3>

<ul>
<li><a href="http://yagays.github.io/blog/2013/07/15/mathematical-models-for-transcriptome-quantification-1/">RNA-Seqの数理―生成モデルによる発現量推定（1）RNA-Seqの発現量推定とRPKM - Wolfeyes Bioinformatics beta</a></li>
<li><a href="http://yagays.github.io/blog/2013/07/16/mathematical-models-for-transcriptome-quantification-2/">RNA-Seqの数理―生成モデルによる発現量推定（2）Count Based ModelとRPKM - Wolfeyes Bioinformatics beta</a></li>
</ul>


<h3>参考</h3>

<ul>
<li>(Cufflinksの論文)<a href="http://www.nature.com/nbt/journal/v28/n5/full/nbt.1621.html">Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation : Nature Biotechnology : Nature Publishing Group</a></li>
<li>(RSEMの論文)<a href="http://www.biomedcentral.com/1471-2105/12/323">BMC Bioinformatics | Full text | RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ディレクトリの書き込み権限が無いとファイルの消去ができない]]></title>
    <link href="http://yagays.github.io/blog/2013/07/13/directory-permission/"/>
    <updated>2013-07-13T15:40:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/13/directory-permission</id>
    <content type="html"><![CDATA[<p>久しぶりに初歩的なところでハマったのでメモ．多人数が使用するサーバ上で，何故かrmすることのできないファイルがあった．パーミッションも所有者もグループも特に変なところは見当たらず，ファイルが壊れているわけでもGUIで他のプログラムが使用中というわけでもない．なのに，rmしようとすると以下のようなエラーが出てしまう（例示しているコマンドラインは手元で再現した状況）．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ rm hehehe
</span><span class='line'>remove hehehe? y
</span><span class='line'>rm: hehehe: Permission denied</span></code></pre></td></tr></table></div></figure>


<p>sudo権限を持っていないのでなんともしがたいなーと思いつつ，適当にディレクトリ名を&#8221;fuga_broken&#8221;に変更して放置していたのだけれども，たまたま機会があって人に相談したら「ディレクトリの書き込み権限が無いんじゃない？」と言われて，そこでようやく気がついた．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -l
</span><span class='line'>合計 0
</span><span class='line'>dr-xr-xr-x 3 yag_ays staff 102  7 13 15:27 fuga/</span></code></pre></td></tr></table></div></figure>


<p>というわけで，ディレクトリの書き込み権限がなかったせいで，そのディレクトリ内のファイルの消去ができなかっただけだったというオチ．なぜ書き込み権限が無くなっていたのかはいまだに不明だ．対象となるディレクトリをchmodで書き込み権限を付加したら，あっさりファイルを削除することができた．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ chmod +w fuga
</span><span class='line'>$ rm fuga/hehehe
</span><span class='line'>remove fuga/hehehe? y
</span><span class='line'>$ </span></code></pre></td></tr></table></div></figure>


<p>なんともまあ呆気なく解決してしまった．普段からディレクトリのパーミッションはまったく変えないので，問題解決の際にディレクトリにまで考えが及ばなく，久々に自分の経験の浅さを実感した．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tophat/Cufflinksなどの開発者であるCole TrapnellによるRNA-Seqの遺伝子発現解析に関する講義動画]]></title>
    <link href="http://yagays.github.io/blog/2013/07/12/stat115-cole-trapnell/"/>
    <updated>2013-07-12T03:49:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/12/stat115-cole-trapnell</id>
    <content type="html"><![CDATA[<p>非常に良い講義資料およびその講義ビデオを見つけたので紹介する．これはハーバードの大学院およびハーバード公衆衛生大学院のComputational Biologyの講義で，マイクロアレイ/次世代シーケンサの発現解析に始まり，ChIP-Seqやモチーフ検索，GWAやSNP解析に至るまで，Computational Biologyにおける様々な研究対象に関する講義が行われている．</p>

<p><a href="http://www.stat115.org/index.html">STAT115 Introduction to Computational Biology</a></p>

<blockquote><p>This course is an introduction to state-of-the-arts concepts, methodologies and practices in computational biology. After taking this course, you should be able to:</p>

<ul>
<li><p>Use appropriate commands or write your own scripts to manipulate your own genomic data</p></li>
<li><p>Exploit appropriate pipeline or software to analyze your own high-throughput gene expression microarray and/or RNA-Seq data and high-throughput sequencing transcriptional regulation data</p></li>
<li><p>Apply statistical or mathematical models to your own population/human genetics data</p></li>
<li><p>Criticize and/or extend the computational analysis or statistical models in the literature</p></li>
<li><p>Use R, Python, and/or Shell scripts and basic concepts in genomics and genetics to perform your own bioinformatics and computational biology research</p></li>
</ul>


<p><a href="http://www.stat115.org/syllabus.html">http://www.stat115.org/syllabus.html</a></p></blockquote>

<p>このなかで，Cole TrapnellがRNA-Seqの遺伝子発現解析に関してレクチャーしている．Cole Trapnellといえば，<a href="http://tophat.cbcb.umd.edu/">TopHat</a>や<a href="http://cufflinks.cbcb.umd.edu/">Cufflinks</a>の開発者の中心人物のひとりだ．これらのツールに関する論文の第一著者でもある．その彼が，RNA-Seqの定量化や発現量推定などの一連の流れを，自身が開発に参加した&#8221;Tuxedo Tools&#8221;（Bowtie, Tophat, Cufflinks, CummeRbundの総称）を例に紹介しているのがこの動画だ．主にマッピング後の発現量推定に関して，基本的なFPKMの求め方であったり，Cufflinksのisoformの取り扱いであったりを，少し数式も交えながら詳しく解説している．</p>

<p>講義資料：<a href="http://www.stat115.org/lectures/stat115_rnaseq.pdf">http://www.stat115.org/lectures/stat115_rnaseq.pdf</a></p>

<iframe src="http://player.vimeo.com/video/60609764" width="500" height="281" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p> <p><a href="http://vimeo.com/60609764">lecture9-I</a> from <a href="http://vimeo.com/stat115">stat115</a> on <a href="https://vimeo.com">Vimeo</a>.</p></p>

<iframe src="http://player.vimeo.com/video/60609783" width="500" height="281" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p> <p><a href="http://vimeo.com/60609783">lecture9-II</a> from <a href="http://vimeo.com/stat115">stat115</a> on <a href="https://vimeo.com">Vimeo</a>.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TimeTreeで生物種間の分岐年代を調べる]]></title>
    <link href="http://yagays.github.io/blog/2013/07/10/timetree/"/>
    <updated>2013-07-10T14:03:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/10/timetree</id>
    <content type="html"><![CDATA[<p>2種類の生物がどれくらい系統的に近いか遠いかといった情報は，生物の分類をWikipediaで確認したり系統樹を眺めたりと色々と確認する方法があるが，ではどれくらい前に2種類の生物が分岐したかという情報は意外と確認するのが難しい．そういった分岐年代を簡単に確認，比較することができるのがTimeTreeというサイトだ．</p>

<p><a href="http://www.timetree.org/">TimeTree :: The Timescale of Life</a></p>

<h3>生物種を入力する</h3>

<p>TimeTreeウェブサイトの左上にある検索ボックスに，分岐年代を辿りたい生物種を入力する．通常は<em>Canis lupus</em>と<em>Felis catus</em>のような学名を入力するのだけれども，一般的な名称であるdogやcatも自動で変換してくれるようだ．もし学名がハッキリ分かっているのなら，そちらを入力したほうが確実だろう．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/timetree1.png" alt="" /></p>

<h3>結果</h3>

<p>今回のような検索クエリを入力した結果，イヌ(<em>Canis lupus familiaris</em>)とネコ(<em>Felis catus</em>)の分岐年代は55.1 Million Years Agoと推定された．ただしこの値というのは，様々な分岐年代の研究に関してイヌとネコが当てはまるものを複数集めてきて，最終的に平均を取った値となっている．個別の研究の推定結果というのは，図の左側の時系列の箇所に黒の点で表示されているように，実際には42Myaから67Myaまで幅があるようだ．このように複数のソースから分岐年代を確認できるのもTimeTreeの大きな特徴となっている．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/timetree2.png" alt="" /></p>

<p>上図の黒の点で示した個別の研究というのは，下のリストからそれぞれ確認することができる．Taxon Aがイヌ，Taxon Bがネコの検索クエリにそれぞれ対応しており，研究によってはイヌ亜目Caniformiaやネコ亜目Feliformiaのような目レベルで確認されているものや，クマ科Ursidaeやアザラシ科Phocidaeといった少し系統的に離れた分類群との比較も列挙されている（クマやアザラシはどちらもネコ目）．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/timetree4.png" alt="" /></p>

<p>このように，ウェブページに表示されているリストから詳細なページに飛ぶことができるほか，「Molecular Time Estimates」にある「Download as CSV」をクリックすると，根拠となる論文や出版年のリストをコンマ区切りテキストとしてダウンロードすることができる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/timetree3.png" alt="" /></p>

<h3>参考</h3>

<ul>
<li><a href="http://www.timetree.org/">TimeTree :: The Timescale of Life</a></li>
<li>(TimeTreeプロジェクトに関して)<a href="http://www.evolutionsbiologie.uni-konstanz.de/genombaum/Kuraku-ElasmoReview-Japanese.pdf">http://www.evolutionsbiologie.uni-konstanz.de/genombaum/Kuraku-ElasmoReview-Japanese.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ゲノムアセンブリにおいて最適なk-merを推定するKmerGenieを試してみた]]></title>
    <link href="http://yagays.github.io/blog/2013/07/07/genome-assembly-kmergenie/"/>
    <updated>2013-07-07T15:17:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/07/genome-assembly-kmergenie</id>
    <content type="html"><![CDATA[<p>KmerGenieという，シーケンスデータのみを用いてアセンブラに依存しない形でアセンブルに指定する最適なk-merの値を求めることができるソフトウェアが最近出てきたので，ちょっと使ってみた．</p>

<p><a href="http://kmergenie.bx.psu.edu/">KmerGenie</a></p>

<p>そういえば以前<a href="http://yagays.github.io/blog/2012/06/06/velvetk/">VelvetK</a>という，これも同じようにk-merパラメータをデータセットから推定するツールについて書いたが，これは名前の通りVelvetに特化したソフトウェアだった．2007年くらいに公開され使われ始めたVelvetは，NGSの普及と相まってかなり長い間ゲノムアセンブラとして広く使われてきた印象がある．その後ABySSやMIRA，ALLPATHS-LG，Rayなどの様々なゲノムアセンブラが出てきたが，それらに対してVelvetKのようなk-mer最適化のようなツールは無かったようで，そのあたりアセンブラに非依存というのがKmerGenieの売りのようだ．</p>

<h3>KmerGenieの仕組み</h3>

<p>KmerGenieがどういう仕組みで最適なk-merを推定しているかは，以下のスライドやarXiv.orgで公開されている論文に詳しく書かれている．</p>

<ul>
<li><a href="http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf">http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf</a></li>
<li><a href="http://arxiv.org/abs/1304.5665">[1304.5665] Informed and Automated k-Mer Size Selection for Genome Assembly</a></li>
</ul>


<p>これによると，単純に言ってしまえば最適なk-merの出現頻度というものはきれいな正規分布に従うと仮定した上で，k-merの値を色々変えて出現頻度のヒストグラムを作成し，生成モデルを立ててフィッティングをして最適なk-merの値を推定しているようだ．実際には，正規分布を仮定したゲノム配列の分布とパレート分布を仮定したエラーの配列の分布が混ざった混合分布を考えて，その混合の重みなども考慮したパラメータ最適化をしているらしい．</p>

<h3>KmerGenieのインストール</h3>

<p>まずはKmerGenieをインストールする．ダウンロードしたファイルを展開すると既にディレクトリにkmergenieという実行ファイルがあるが，makeをしないと利用することができないので注意．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://kmergenie.bx.psu.edu/kmergenie-1.5397.tar.gz
</span><span class='line'>$ cd kmergenie-1.5397
</span><span class='line'>$ make</span></code></pre></td></tr></table></div></figure>


<h3>KmerGenieの実行</h3>

<p>それでは実際にfastqファイルに対してKmerGenieを実行してみる．</p>

<h4>1. ゲノムアセンブリのチュートリアルで使用されたE. coliのゲノムシーケンス</h4>

<p>まずは2013年の6月に行われた<a href="http://bioinformatics.msu.edu/ngs-summer-course-2013">MSU NGS Summer Course 2013</a>のチュートリアルで使用されたサンプルデータを使ってみる．これはE. coliのゲノムシーケンスで，元は<a href="http://www.ncbi.nlm.nih.gov/pubmed/21926975">Chitsaz et al.</a>が出したデータの一部のようだ．中身は約4.7Mの70bpのシングルリード．</p>

<p><a href="http://ged.msu.edu/angus/tutorials-2013/assembling-ecoli-with-velvet.html">Assembling E. coli sequences with Velvet — ANGUS 2.0 documentation</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -O https://s3.amazonaws.com/public.ged.msu.edu/ecoli_ref-5m-trim.fastq.gz
</span><span class='line'>$ gunzip ecoli_ref-5m-trim.fastq.gz</span></code></pre></td></tr></table></div></figure>


<p>それではecoli_ref-5m-trim.fastqに対してKmerGenieを実行する．実行後に作られる各k-merでのカウントデータや分布をプロットしたpdfはカレントディレクトリにそのまま出力されるので，もし必要ならディレクトリを別途作ってそこで実行したほうが良いだろう．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie ecoli_ref-5m-trim.fastq
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~130 M distinct 41-mers are in the reads
</span><span class='line'>K-mer sampling: 1/100
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 61 51 41 31 21 
</span><span class='line'>-----------------------------------------------------------------------------------------------------------------------------Total time Wallclock  77.7066 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>estimation of the best k so far: 51
</span><span class='line'>refining estimation around [45; 57], with a step of 2
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~139 M distinct 39-mers are in the reads
</span><span class='line'>K-mer sampling: 1/100
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 57 55 53 51 49 47 45 
</span><span class='line'>-----------------------------------------------------------------------------------------------------------------------------Total time Wallclock  56.4315 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 45
</span><span class='line'>fitting histogram for k = 47
</span><span class='line'>fitting histogram for k = 49
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 53
</span><span class='line'>fitting histogram for k = 55
</span><span class='line'>fitting histogram for k = 57
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>table of predicted num. of genomic k-mers: histograms.dat
</span><span class='line'>best k: 55</span></code></pre></td></tr></table></div></figure>


<p>今回の場合，KmerGenieによる最適なk-merの値は55となった．出力結果で示されるk-mer頻度のプロットを並べて見ると，確かにkが小さいときは少しいびつになっており，k=55あたりで一番正規分布っぽくなっている…??ことがわかる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/kmergenie1.png" alt="" /></p>

<p>ただ，このサンプルーデータを解析しているチュートリアルではk-merを31〜35にしてvelvetでアセンブルしていることから，今回の55という推定値は少し大きめのような気がする．</p>

<h3>2. Assemblathon2で使用されたセキセイインコのゲノムシーケンス</h3>

<p>次はもう少し大きいデータで試してみよう．Assemblathon2というアセンブリツールの性能を競うコンペで使用されたセキセイインコのゲノムシーケンスの中からDuke Illumina GAIIx runsというデータを使ってみる．中身は76bpのペアエンドで合計で126M readsある．</p>

<p><a href="http://bioshare.bioinformatics.ucdavis.edu/Data/hcbxz0i7kg/Parrot/illumina_duke_runs/">Index of /Data/hcbxz0i7kg/Parrot/illumina_duke_runs</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ lftp -e "mirror" http://bioshare.bioinformatics.ucdavis.edu/Data/hcbxz0i7kg/Parrot/illumina_duke_runs/ 
</span><span class='line'>$ cat *.fastq &gt; budgie.fastq</span></code></pre></td></tr></table></div></figure>


<p>Assemblathonのサイトによるとセキセイインコのゲノムは二倍体らしいので，今回はKmerGenieの「&#8211;diploid」のパラメータを付けて実行する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie --diploid budgie.fastq
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~3721 M distinct 46-mers are in the reads
</span><span class='line'>K-mer sampling: 1/1000
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 71 61 51 41 31 21
</span><span class='line'>---------------------------------------------------------------------------------------------------------------------------Total time Wallclock  3035.49 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>fitting histogram for k = 71
</span><span class='line'>estimation of the best k so far: 21
</span><span class='line'>refining estimation around [15; 27], with a step of 2
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~6335 M distinct 24-mers are in the reads
</span><span class='line'>K-mer sampling: 1/1000
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 27 25 23 21 19 17 15
</span><span class='line'>---------------------------------------------------------------------------------------------------------------------------Total time Wallclock  2305.61 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 15
</span><span class='line'>fitting histogram for k = 17
</span><span class='line'>fitting histogram for k = 19
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 23
</span><span class='line'>fitting histogram for k = 25
</span><span class='line'>fitting histogram for k = 27
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>fitting histogram for k = 71
</span><span class='line'>table of predicted num. of genomic k-mers: histograms.dat
</span><span class='line'>best k: 21</span></code></pre></td></tr></table></div></figure>


<p>データ量が多いと計算時間もかなり大きくなるようで，出力にも少し書かれている通り，今回は1時間半くらいかかった．</p>

<p>さて，今回の場合はKmerGenieによる最適なk-merの値は21となった．出力結果で示されるk-mer頻度のプロットを並べて見てみると，kが大きい値の時には確かに正規分布として近似できないくらいなだらかな曲線になっているものの，K=21の最適値付近でもその様子はあまり変わっていないような感じがする．そもそも可視化の都合もあって横幅が揃えられていなかったりと色々と問題はあるが，とりあえずは今回のデータから推定された最適値は21だった．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/kmergenie2.png" alt="" /></p>

<h3>KmerGenieで使用できるパラメータ</h3>

<p>以上のように，KmerGenieは幾つかのk-merの値で実行して最適と思われる値を自動で出力するが，他にも手動でパラメータを指定することでk-merの範囲や刻みを変更することができる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie
</span><span class='line'>KmerGenie
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>    kmergenie &lt;read_file&gt; [options]
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>    --diploid    use the diploid model
</span><span class='line'>    --one-pass   skip the second pass
</span><span class='line'>    -k &lt;value&gt;   largest k-mer size to consider (default: 121)
</span><span class='line'>    -l &lt;value&gt;   smallest k-mer size to consider (default: 15)
</span><span class='line'>    -s &lt;value&gt;   interval between consecurive kmer sizes (default: 10)
</span><span class='line'>    -e &lt;value&gt;   k-mer sampling (default: auto-detected power of 10)"
</span><span class='line'>    -o &lt;prefix&gt;  prefix of the output files (default: histograms)"</span></code></pre></td></tr></table></div></figure>


<h3>まとめ</h3>

<p>このように，KmerGenieはシーケンスデータのみを用いてアセンブラに依存しない形で，アセンブルに指定する最適なk-merの値を求めることができる．推定された値が最良のアセンブルに繋がるかは実際に確認してみないと判断できないが，大まかな目安であったりk-mer決定プロセスの理由付けなどに力を発揮するだろう．今回は結果に対してそれほど詳細な評価を行わなかったが，<a href="http://arxiv.org/abs/1304.5665">arXiv.orgで公開されている論文</a>の方では<a href="http://gage.cbcb.umd.edu/index.html">GAGE</a>のデータセットに対してアセンブル結果を含めた性能比較をしているので，そちらも参考にしていただきたい．</p>

<h3>参考</h3>

<ul>
<li><a href="http://kmergenie.bx.psu.edu/">KmerGenie</a></li>
<li><a href="http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf">http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf</a></li>
<li><a href="http://arxiv.org/abs/1304.5665">[1304.5665] Informed and Automated k-Mer Size Selection for Genome Assembly</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[だれかicanhazpdfできる？？？]]></title>
    <link href="http://yagays.github.io/blog/2013/07/06/icanhazpdf/"/>
    <updated>2013-07-06T17:02:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/06/icanhazpdf</id>
    <content type="html"><![CDATA[<p>#icanhazpdfというTwitterのハッシュタグがある．これが何を意味しているかは，実際に使われている例を見てもらったほうが早いだろう．</p>

<p><a href="https://twitter.com/search/%23icanhazpdf">Twitter / Search - #icanhazpdf</a></p>

<p>つまり，読みたい論文があるのに所属している大学/研究所/企業がライセンスを契約していないからpdfが取れない！という時に，</p>

<ul>
<li><p>&#8220;<strong>この論文のPDF取れる人いますか？ &#8220;hoge et al. piyopiyo&#8221; http://xxxxxx… #icanhazpdf</strong>&#8221;</p></li>
<li><p>&#8220;<strong>だれか#icanhazpdf できる？ http://xxxxx…</strong>&#8221;</p></li>
</ul>


<p>といった感じでTwitterに投稿するというわけだ．これを見た知り合いか誰かがpdfをメールで送ってさえくれれば，気になる論文がチェックできる！</p>

<p>とまあ，これはヤバいだろというのは誰が見ても明らかで，Twitterで検索した結果を見ても，実際にこのハッシュタグに疑問を投げかけている人も多数存在する．ここでは法やモラルの問題に踏み込みたくないので判断は各自に任せるとして，近年の論文誌の購読価格の上昇であったりオープンアクセスの流れなどが影響しているのは確かだろう．まあ，Twitterの登場によって今まで内輪で行われてきた行為が表面化してきているだけというのが一番大きいのだろうけれども．</p>

<h3>icanhaz&#8230;の元ネタ</h3>

<p>icanhazという意味不明な英文法の元ネタは以下の猫の画像．tumblrによく流れてくるようないわゆる画像に文字を付ける遊びの一種で，本来は&#8221;I Can Has Cheezburger?&#8221;だったようだ．このタイトルを冠したネタサイトがあるほか，Wikipediaにも個別記事が作られている．</p>

<p><a href="http://www.sodahead.com/fun/lolcats-do-you-have-a-favorite-lol-cat-or-other-lol-picture/blog-326585/?link=ibaf&imgurl=http://images.sodahead.com/slideshows/000000783/icanhascheeseburger-14422465099_xlarge.jpeg&q="><img src="http://images.sodahead.com/slideshows/000000783/icanhascheeseburger-14422465099_xlarge.jpeg"><br> pics on Sodahead</a></p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/I_Can_Has_Cheezburger%3F">I Can Has Cheezburger? - Wikipedia, the free encyclopedia</a></li>
<li><a href="http://news.cnet.com/8301-13772_3-10023722-52.html">The history of I Can Has Cheezburger | Geek Gestalt - CNET News</a></li>
</ul>


<h3>その他icanhazpdfの参考サイト</h3>

<ul>
<li><a href="http://eurekajournals.org/eureka/index.php?title=Icanhazpdf">Icanhazpdf - Eureka Journal Watch</a></li>
<li><a href="http://www.samuelpean.com/icanhazpdf-reddit-scholar-pirateuniversity-org-aaaaarg-org-how-scientist-community-bypasses-journals-paywalls/">#ICanHazPDF, Reddit Scholar, PirateUniversity.org &amp; AAAAARG.org: How scientific community bypasses journals paywalls | Samuel PÉAN</a></li>
<li><a href="http://neuroconscience.com/2013/01/16/join-papester-collective-1-0-how-to-reply-to-icanhazpdf-in-3-seconds/">How to reply to #icanhazpdf in 3 seconds | Neuroconscience</a></li>
<li><a href="http://current.ndl.go.jp/node/20709">ハーバード大学、価格高騰する学術雑誌の購読中止を視野に入れた対策案を教員等に提示 | カレントアウェアネス・ポータル</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zenithのホッチキスを買った]]></title>
    <link href="http://yagays.github.io/blog/2013/07/03/zenith-stapler/"/>
    <updated>2013-07-03T19:52:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/03/zenith-stapler</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=B007RIHVOG" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>Zenithのホッチキスを買った．前々から一目惚れしていたものの値段のせいあって買うのをためらっていたのだが，いつも使っているホッチキスで上手く綴ることができなくなってきたので買い替えとばかりに思い切って購入．最近ご無沙汰だった文房具熱が再発しそうで少し怖い．</p>

<ul>
<li><a href="http://freedesign.jp/stationery/ktzsp001.php">BALMA ZENITH STAPLER 591 - バルマ ゼニス ステープラー</a></li>
</ul>


<p>第一印象としては，とにかく使い心地が良い．見た目からはデカくてゴツくて荒っぽい印象なのだけれども，やはり通常のホッチキスと比べてそもそも構造が違うのが，使用感に大きく影響してくる．とにかく力を入れなくても簡単に紙を綴ることができるので，だからといって楽とは少し違うのだけれども，普通に使う分には全く問題がない．正確には計っていないが10枚くらいなら余裕で綴ることができるし，頑張れば20枚程度はいけそうな感じがする．あと個人的に気に入っているのが綴るときの音で，鉄の擦れる音やバネの軋む音が心地良い．</p>

<p>まだあまり使っていないので，これからが楽しみだ．Zenithのホッチキスで綴た論文を，頑張って読もう…．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/zenith.jpg" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["Zero Dark Thirty"のBlu-rayを買った]]></title>
    <link href="http://yagays.github.io/blog/2013/07/02/blu-ray-zero-dark-thirty/"/>
    <updated>2013-07-02T00:36:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/02/blu-ray-zero-dark-thirty</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=B00B1E6FF8" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>映画&#8221;Zero Dark Thirty&#8221;の北米版Blu-rayを買った．映画として完成度が高いというのは言うまでもないものの，個人的な思い入れが強い作品でもある．この映画で描かれるCIAの女性捜査官には，何かを追い求める執念であったり不屈の精神に強く惹かれるところがある．それをもう一度確かめたくて，9月の日本版の発売を待たず海外版を購入してしまったというわけだ．当然ながら日本語字幕は無い．</p>

<p>今回購入したBlu-ray/DVD Comboには，Blu-rayとDVDが1枚ずつ入っている．Blu-rayは日本と北米ともに同じリージョンなのでPS3などのプレイヤーで普通に観ることができる．DVDはリージョンが違うため日本のプレイヤーでは通常は見ることができないため，PCなどの環境を別途用意する必要がある．(<a href="http://www.amazon.com/gp/help/customer/display.html?ie=UTF8&amp;nodeId=3193231">参考</a>)</p>

<p>さて，今回初めて北米版のDVDを買って色々知ったのだが，映画の音声に通常の英語版に加えて，Descriptive Serviceというモードが付いている．これはどうやら視覚障害を持つ方向けのサービスのようだ．</p>

<p><a href="http://en.wikipedia.org/wiki/Descriptive_Video_Service">Descriptive Video Service - Wikipedia, the free encyclopedia</a></p>

<p>実際に聞いてみるとわかりやすいのだが，Descriptive Serviceはいわゆるオーディオブックのようなものだ．映画オリジナルのセリフの間に，情景の説明や誰が何をしたといったことがナレーションとして入っている．画面に表示される文字や暗転したといったことも含めて，セリフでカバーできない画面の内容が英語音声で説明されるので，画面を観ることができない人でも映画を楽しめるようになっている．本来はそのような用途で使用されるものなのだが，このナレーションは聞き取りやすい声で口調もそれほど速くないため，最近はこれを流しながら英語を聞く耳を鍛えている．Descriptive Serviceは当然ながら字幕に出すことができないので，英語学習に向いているとは言えないものの，好きな映画を見ながらセリフ以外のところで英語の学習ができるので，個人的に気に入っている．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「世界基準で夢をかなえる私の勉強法」読了]]></title>
    <link href="http://yagays.github.io/blog/2013/07/01/review-kitagawa-be-yourself-and-carry-on/"/>
    <updated>2013-07-01T22:56:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/01/review-kitagawa-be-yourself-and-carry-on</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=434402334X" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p><a href="http://www.amazon.co.jp/gp/product/4106104695/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4106104695&linkCode=as2&tag=yagays-22">ハーバード白熱日本史教室 (新潮新書)</a><img src="http://ir-jp.amazon-adsystem.com/e/ir?t=yagays-22&l=as2&o=9&a=4106104695" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />で知られる北川智子氏の人生をなぞるようにして，彼女の勉強に対する姿勢や方法を書き起こした本．高校時代にカナダに留学した時から始まり，そのままカナダの大学へと進学，専攻を変えつつアメリカの大学院を渡り歩き，そしてハーバードでのカレッジ・フェローとイギリスのケンブリッジ大への挑戦という各段階において，どのように勉強をこなし成果を上げてきたかという経験が語られる．国を変え，専攻を変え，研究対象を変える．彼女はそのようにして興味を追いかけハードルを乗り越えながら勉強してきたという．その中で培われてきた独自の勉強法が，彼女の体験とともに紹介される．</p>

<p>本書では様々な勉強法が紹介されるが，全体に共通するのは恐ろしいほどの勉強量である．文章の端々に表れる異常なまでの勉強時間，勉強量，集中力，アウトプット，そのどれもが，字面だけでは到底推し量ることのできないほどの苦痛と困難に満ちていたであろうことを，どうしても感じとってしまう．例えば，1週間の日程表は午前7時から始まり27時まで書かれている．文字通り100杯以上のコーヒーを飲み，博論を書き続けて気付けば5日経っていたこともあったという．そのような過去の苦労に対して，どちらかというとリアルに表現することなくソフトなタッチで書かれているために，つい大げさだと受け流してしまうかもしれない．しかしながら，その経験談の裏にある彼女の人生が辿ってきた険しい道というものは，明らかに存在する．勉強法という小手先の技術というべきものが，巨大な努力量と綿密な下準備というものによって支えられているということを，ある意味気づかせないようにしているとも取れる．</p>

<p>みんな，誰もが効率というものに取り憑かれて，少ない労力で多くの成果を得たいと思っている．活躍する人の真似をすれば少しは自分もそこに追いつけるかもしれないと思っている．と同時に，成果に繋げるには生活すべてを勉強に注ぎ込む必要があるのだとも薄々気づいている．自分に何ができるかという答えなき問題に対する不安や諦観には，とやかく人に言われるよりも，試行錯誤を繰り返して自分で見つけ出すしかない．しかしながら，他人の人生と教訓を学ぶことは，決して悪い方法ではないと僕は思っている．他人が経験した他人の人生を自分の中で上手く反芻することができれば，それは何かしらの手がかりにはなる．そういった意味で，本書で語られる北川智子氏の人生は，多くの日本人よりはるかにかけ離れたものであり，現在の日本の価値観におけるロールモデルであり，実現不可能な人生ではないことを示してくれる実例となっている．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["RNA-Seq unleashed"によるTrinityのワークフローの解説]]></title>
    <link href="http://yagays.github.io/blog/2013/06/26/rna-seq-unleashed-trinity/"/>
    <updated>2013-06-26T13:14:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/26/rna-seq-unleashed-trinity</id>
    <content type="html"><![CDATA[<p><a href="http://evomics.org/learning/genomics/trinity/">Trinity | Evolution and Genomics</a>経由で知ったのだが，Trinityのワークフローに関してより分かりやすい図が2011年のNature BiotechnologyのNews and Viewsに掲載されていたので，ここで少し紹介する．この図はTrinityのアセンブリアルゴリズムの一連のワークフローを表しており，アセンブリ結果の解釈も含めて解説されているのが大きな特徴だ．Trinityが対象にしているトランスクリプトームのアセンブリは，アイソフォームや選択的スプライシングなどの影響を受けて良く似た配列が多数出てくるため，一意に決まらない場合が多い．そういったケースに対応するために，Trinityではアセンブルのグラフを作成して考えられる候補を幾つか出力するのだけれども，実はこの結果の解釈が一番難しい．多数の類似配列が出力で得られた時にそれがどういう可能性から出てきたものなのかを考える上で，この図の一番下の解説はとても参考になる．実際はこんなに上手くいかないだろうとは思うものの，個別のアイソフォームとして配列をクラスタリングしたり，そもそもTrinityが何を考慮してアセンブルしているかを確認する際には役立つだろう．</p>

<blockquote><p>Iyer, M. K. &amp; Chinnaiyan, A. M. RNA-Seq unleashed. <em>Nat. Biotechnol.</em> <strong>29</strong>, 599–600 (2011).</p>

<p><a href="http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html">http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html</a></p></blockquote>

<p><a href="http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html" target="_blank"><img src="http://dl.dropboxusercontent.com/u/142306/b/rna-seq_unleashed.gif" alt="CCC License Number: 3172280756988"></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Biopythonで塩基配列をアミノ酸配列にする]]></title>
    <link href="http://yagays.github.io/blog/2013/06/25/biopython-translation/"/>
    <updated>2013-06-25T09:59:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/25/biopython-translation</id>
    <content type="html"><![CDATA[<ul>
<li><a href="http://insectcell.exblog.jp/20703555/">awkとsedで塩基配列をアミノ酸配列にする : したっぱ昆虫細胞研究者のメモ</a></li>
<li><a href="https://twitter.com/chalkless/status/349052297982132225">Twitter / chalkless: @n0rr $ perl -MBio::Seq -le &#8230;</a></li>
<li><a href="https://twitter.com/dritoshi/status/349056205160841216">Twitter / dritoshi: @chalkless @n0rr &#8230;</a></li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">Bio.Seq</span> <span class="kn">import</span> <span class="n">Seq</span>
</span><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="n">mRNA</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="s">&quot;TATGAAAGT&quot;</span><span class="p">)</span>
</span><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="n">mRNA</span><span class="o">.</span><span class="n">translate</span><span class="p">()</span>
</span><span class='line'><span class="n">Seq</span><span class="p">(</span><span class="s">&#39;YES&#39;</span><span class="p">,</span> <span class="n">ExtendedIUPACProtein</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Why Biopython?</h3>

<p><a href="http://xkcd.com/353/">xkcd: Python</a></p>

<p><img src="http://imgs.xkcd.com/comics/python.png" alt="http://xkcd.com/353/" /></p>

<h3>Ref.</h3>

<ul>
<li><a href="http://biopython.org/DIST/docs/tutorial/Tutorial.html">Biopython Tutorial and Cookbook</a></li>
<li><a href="http://yagays.github.io/blog/2013/04/20/install-biopython/">Mac OS Xで手っ取り早くBiopythonをインストールして使えるようにする - Wolfeyes Bioinformatics beta</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2013 Web Server issue]]></title>
    <link href="http://yagays.github.io/blog/2013/06/24/nar-web-server-issue-2013/"/>
    <updated>2013-06-24T14:20:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/24/nar-web-server-issue-2013</id>
    <content type="html"><![CDATA[<p><a href="http://nar.oxfordjournals.org/content/41/W1.toc" target="_blank"><img src="http://dl.dropboxusercontent.com/u/142306/b/W1.cover.gif" align="right" alt="http://nar.oxfordjournals.org/content/41/W1.cover-expansion"></a></p>

<p>今年のWeb Server issue 2013が既に閲覧できるようになっていた．</p>

<p><a href="http://nar.oxfordjournals.org/">Nucleic Acid Research誌</a>は少し特殊な特別号を毎年出していて，1月にはDatabase issueが，7月にはWeb Server issueが発刊されるのが定番になっている．これらの特集号は，他の論文誌では論文になりにくいようなデータベース構築やウェブサービス系を論文として発表する場として，バイオインフォ系にとっては重要な投稿先の一つといえる．特に，ウェブサービスのアップデートなども同様の投稿ができるようになっており，新規性としては少し弱いウェブサービスの保守管理や機能追加に関しても評価されるのが大きな特徴だ（ただし2年間のインターバルが必要）．</p>

<p><strong>今年のWeb Server issue：<a href="http://nar.oxfordjournals.org/content/41/W1.toc">Table of Contents — 1 July 2013, 41 (W1)</a></strong></p>

<p>今年は合計で95本のウェブサービスに関する論文が採択されたようだ．Editorialによると</p>

<blockquote><p>For the 2013 issue, 293 summaries were submitted and 129, or 44%, were approved for manuscript submission. Of those approved, 95, or 73%, were ultimately accepted for publication.</p>

<p><a href="http://nar.oxfordjournals.org/content/41/W1/W1.full">http://nar.oxfordjournals.org/content/41/W1/W1.full</a></p></blockquote>

<p>ということで，293本の投稿のうち最終的に95本が採択され，全体で見れば約32%の採択率となっている．</p>

<br/>


<p>来年のWeb Server Issue 2014の締め切りは今年いっぱいまで(31 Dec. 2013)となっている．それまでに完全に動くウェブサービスと1ページのproposalを書いて投稿し，そこで通れば1ヶ月でmanuscriptを書いて7月に掲載という流れのようだ．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tophatにおいて使用するスレッド数を上げすぎるとファイルディスクリプタの上限でエラーになる]]></title>
    <link href="http://yagays.github.io/blog/2013/06/23/tophat-too-many-open-files/"/>
    <updated>2013-06-23T10:33:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/23/tophat-too-many-open-files</id>
    <content type="html"><![CDATA[<p>久しぶりにTophatをいじったら一発目でエラーで落ちてしまい，出鼻をくじかれてしまった…．原因を調べてみると，どうやらTophatのコマンド実行時に指定する-pオプションの数字を調子に乗って上げすぎたのが良くなかったらしい．といっても計算機のリソースに問題があったわけではなく，ユーザが使えるシステムリソースの制限を超えてしまったために，Tophatが停止してしまったようだ．この問題はTophatの公式サイトのFAQでも取り上げられているが，このBlogでもエラーメッセージとともに原因と対策を書いておこうと思う．</p>

<h3>Tophatの標準エラー出力のメッセージ</h3>

<p>今回の場合は，Tophatのログの最後で以下のようなエラーメッセージが表示される．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[...]
</span><span class='line'>[2013-06-23 00:22:33] Mapping right_kept_reads_seg3 to genome chlamydomonas_236 with Bowtie2 (3/4)
</span><span class='line'>[2013-06-23 00:33:09] Mapping right_kept_reads_seg4 to genome chlamydomonas_236 with Bowtie2 (4/4)
</span><span class='line'>[2013-06-23 00:44:57] Searching for junctions via segment mapping
</span><span class='line'>        [FAILED]
</span><span class='line'>Error: segment-based junction search failed with err =1
</span><span class='line'>Error opening SAM file output/tmp/right_kept_reads_seg1.bam</span></code></pre></td></tr></table></div></figure>


<p>これだけでは何のことだかよくわからないが，Tophatが出力するログのlogs/segment_juncs.logを見ると，このエラーに関してもう少し詳細な記述が出力されている．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    Loading scaffold_53...done
</span><span class='line'>    Loading scaffold_54...done
</span><span class='line'>    Loading ...done
</span><span class='line'>&gt;&gt; Performing segment-search:
</span><span class='line'>Loading left segment hits...
</span><span class='line'>done.
</span><span class='line'>Loading right segment hits...
</span><span class='line'>open: Too many open files
</span><span class='line'>Error opening SAM file output/tmp/right_kept_reads_seg2.bam</span></code></pre></td></tr></table></div></figure>


<p>これを見ると，どうやらsegment_juncsの実行中に「open: Too many open files」が原因で実行が落ちたようだ．</p>

<h3>原因と対策</h3>

<p>このエラーに関しては，実はTophatの公式サイトのFAQに「What should I do if I see a message like &#8220;Too many open files&#8221;?」という，まさに先ほどのエラーメッセージの内容そのままの項がある．</p>

<blockquote><p>This usually happens when using &#8220;-p&#8221; option with a large value (many threads). TopHat may produce many intermediate files, the number of which is proportional to this value; sometimes the number of the files may go over the maximum number of files a process is allowed to open. The solution is to raise the limit to a higher number (e.g. 10000). For Mac, you can change this using a command, &#8220;sudo sysctl -w kern.maxfiles=10240&#8221;.</p>

<p><a href="http://tophat.cbcb.umd.edu/faq.shtml#many_file">TopHat :: Center for Bioinformatics and Computational Biology</a></p></blockquote>

<p>ざっと要約すると「Tophatは中間ファイルを大量に作るから時々許容数超えちゃうんだよね．扱えるファイル数の上限上げるか，Macなら次のコマンドで対処してね」ということになる．さすがにこれだけではよく分からないと思うので，SEQanswersの以下のスレッドも参考にしつつ，もう少し詳しく見ていこうと思う．</p>

<p><a href="http://seqanswers.com/forums/showthread.php?t=21316">Tophat segment junction error 1, invalid BAM binary header - SEQanswers</a></p>

<h3>limitとファイルディスクリプタ数の制限</h3>

<p>さて，公式サイトのFAQにおいてファイル数の上限といった言葉が出てきたように，LinuxやMacではユーザごとに使用することのできる各種システムリソースに制限が設けられている．具体的にはユーザが使用できるCPU数やメモリの容量，プロセスの数などがそれに該当するのだが，その中に<strong>「ファイルディスクリプタ数」</strong>という項目があり，1プロセスが同時に開くことのできるファイル数の上限を定めている．これは主にひとつの計算機を複数人が使用するマルチユーザシステムにおいて，一人がリソースを独占するのを防いだりアプリケーションの暴走を止めたりするのに役立つのだが，今回のような大規模な計算を実行する際にはこれが邪魔になってしまう．</p>

<p>システムリソースの制限を確認したい場合には，「ulimit -a」または「limit」というコマンドを使う．例えば，私の環境では以下のようになる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ソフトリミット
</span><span class='line'>$ ulimit -a
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             8192
</span><span class='line'>-c: core file size (blocks)         0
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       1024
</span><span class='line'>-n: file descriptors                1024
</span><span class='line'>-l: locked-in-memory size (kbytes)  64
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 127413
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>ここで表示されるのは厳密にはソフトリミットと呼ばれ，ユーザごとに設定されている制限である．一方でハードリミットと呼ばれる制限もあり，これは管理者(root)が定める制限となっている．ソフトリミットは，このハードリミットの範囲内でしか自由に制限値を変更することはできない．ハードリミットを確認したい場合には，-Hオプションを付ける．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ハードリミット
</span><span class='line'>$ ulimit -aH
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             unlimited
</span><span class='line'>-c: core file size (blocks)         unlimited
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       127413
</span><span class='line'>-n: file descriptors                4096
</span><span class='line'>-l: locked-in-memory size (kbytes)  64
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 127413
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>さて，今回問題になっているファイルディスクリプタ数は「-n: file descriptors」で表示されている．上の例の場合，ソフトリミットでは1024，ハードリミットでは4096となっている．つまり，Tophatは1024個以上のファイルを1スレッドで開こうとしたために，このソフトリミットに引っかかってしまったようだ．</p>

<h3>フィアルディスクリプタ数の制限への対策</h3>

<p>この問題を回避するには主に2つの方法がある．</p>

<p>1．Tophatの-pオプションの値を小さくする</p>

<p>2．ソフトリミットのファイルディスクリプタの値を大きくする</p>

<p>まず1.では，Tophatが使用するスレッド数を少なくすることで，ファイルディスクリプタ数の上限に引っかからなくするというもの．一度-pオプションを無くして実行してみれば，おそらく今回のエラーには引っかからなくなるだろう．一度に開くファイル数が少なくなりほぼ確実に実行できるようにはなるが，並列処理数が減ってしまうのでTophatの実行時間は長くなってしまう．</p>

<p>そこで2.のようにの制限を無くして，-pオプションはそのままにファイルディスクリプタの上限を回避するという方法もある．実行コマンドや実行時間はそのままにエラーを回避することができる一方で，上限を上げたからといってもTophatがそれ以上の同時ファイルオープンをしてしまえば同様のエラーに引っかかってしまうほか，制限を上げたことにより計算機に負荷がかかる恐れもある．つまり，時と場合によっては成功するが確証は無いという感じだろうか．</p>

<p>ちなみに，私の場合はファイルディスクリプタ数を上げても以下のような別のエラーが出て実行できなかった．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Error: ReadStream::getRead() called with out-of-order id#!</span></code></pre></td></tr></table></div></figure>


<p>ということで，結論としてはTophatの実行時間との兼ね合いを考えて，どちらかを選択したほうが良いだろう．素直に-pオプションの値を下げるほうが無難な気がする．</p>

<p>ちなみに，制限値を引き上げるには，ulimitで以下のように値を変更する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ulimit -n 2000
</span><span class='line'>$ limit
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             8192
</span><span class='line'>-c: core file size (blocks)         0
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       1024
</span><span class='line'>-n: file descriptors                2000
</span><span class='line'>-l: locked-in-memory size (kbytes)  unlimited
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 16545839
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>ulimitの後ろに該当するパラメータの値を指定することによって，上限を引き上げることができる．ただし，先ほど述べたようにハードリミットより上は指定することができないので注意が必要になる．</p>

<h3>まとめ</h3>

<p>Tophatのエラー「open: Too many open files」は，スレッドが一度に開くことのできるファイルディスクリプタ数がソフトリミットの上限に引っかかってしまったために起こる．-pオプションの値を下げて実行するか，ソフトリミットのファイルディスクリプタの上限を引き上げることによって回避することができる．まずは，-pオプションを指定せずに実行してみよう．</p>

<h4>参考</h4>

<ul>
<li><a href="http://tophat.cbcb.umd.edu/faq.shtml">TopHat :: Center for Bioinformatics and Computational Biology</a></li>
<li><a href="http://seqanswers.com/forums/showthread.php?t=21316">Tophat segment junction error 1, invalid BAM binary header - SEQanswers</a></li>
<li><a href="http://x68000.q-e-d.net/~68user/unix/pickup?limit">UNIXの部屋 コマンド検索:limit (*BSD/Linux)</a></li>
<li><a href="http://yumewaza.yumemi.co.jp/2010/07/limitsconf.html">ファイルディスクリプタ数の上限変更とlimits.confの罠 (ゆめ技：ゆめみスタッフブログ)</a></li>
</ul>


<h4>実行環境</h4>

<ul>
<li>OS：RHEL 6.3</li>
<li>Tophat：v2.0.8b</li>
<li>Bowtie：version 2.1.0</li>
<li>Samtools：0.1.19.0</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SRA Toolkitを使ってsraファイルからfastqファイルに変換する(更新版)]]></title>
    <link href="http://yagays.github.io/blog/2013/06/20/fastq-dump-sratoolkit/"/>
    <updated>2013-06-20T11:26:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/20/fastq-dump-sratoolkit</id>
    <content type="html"><![CDATA[<p>久しぶりにfastq-dumpを使ったら少し仕様が変わっていたのでメモ．だいぶ前に別の場所で書いた記事は，既に古くなってしまっている．何もせず記事を放置しておくのも申し訳ないし，どうやらfastq-dumpでググると上位にくるようなので，ここいらで更新しておかないと…．</p>

<p>(deprecated) <a href="http://g86.dbcls.jp/~yag/wordpress/archives/959">Wolf Ears » SRA Toolkitを使ってsraファイルからfastqファイルに変換する</a></p>

<h3>SRA Toolkitのインストール</h3>

<p>sraからfastqに変換するプログラムは，SRA Toolkitの中にあるfastq-dumpを使う．まずはNCBIのサイトからOS環境に合わせてコンパイルされたSRA Toolkitをダウンロードしてインストールする．</p>

<p><a href="http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software">Software : Software : Sequence Read Archive : NCBI/NLM/NIH</a></p>

<p>コンパイルされたバイナリとしては，Linux系列のCentOSやUbuntu，MacOSやMS Windowsに合わせたものが用意されている．私の環境はRed Hat Enterpriseなので，とりあえず「CentOS Linux 64 bit architecture」を選択する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.3.2-5/sratoolkit.2.3.2-5-centos_linux64.tar.gz
</span><span class='line'>$ tar zxvf sratoolkit.2.3.2-5-centos_linux64.tar.gz </span></code></pre></td></tr></table></div></figure>


<p>今回使用するfastq-dumpは展開したディレクトリのbin以下に入っている．以下のように実行して動作するか確認する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>  sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] &lt;path [path...]&gt;
</span><span class='line'>  sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] [ -A ] &lt;accession&gt;
</span><span class='line'>
</span><span class='line'>Use option --help for more information
</span><span class='line'>
</span><span class='line'>sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump : 2.3.2</span></code></pre></td></tr></table></div></figure>


<h3>fastq-dumpの使い方</h3>

<p>基本的な使い方としては，以下のようにfastq-dumpにsraファイルを指定すればよい&#8230;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>


<p>…のだが，ここでひとつ注意が必要になる．上のコマンドを指定すると，データがシングルエンドでもペアエンドでも同様に一つのファイルとして出力されてしまう．実際に変換されたfastqファイルの中身を見てみると，上のDRR002191.sraは本当は90bpのペアエンドなのだが，以下のように結合された配列として出力される．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ head -n 4 DRR002191.fastq
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=180
</span><span class='line'>NCAATGGCAATAGCAATGCATTGAAATGAAAAGGCATTTACCAGGAGCAGGAAAGCCAGAAAGAGGAGCAGTGNNCNNGGAGTTGCNNNNTGCTTGGCTTCATCTTTTGCAATTCTGCATCTTTTGCATTTCCCTTCTCGCTCTGCTGCTCGCTCGCCTTTCTTNNNCGNCCTTTTCCCC
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=180
</span><span class='line'>#1=DDFFFHFFHHIIJJJIJJIJJIJBCHHIHGCC&gt;FGIGIHDH?HJIEGEGI@FEHIJIIE@HI9=EH=CDD#################B@CFFDDDDFHHHGIJJIJJGGIGJIIHGHIGIJIHIJIEIIHIJJIGGHHEGI@DGCGEFHHIGIEFEFFBCB################</span></code></pre></td></tr></table></div></figure>


<p>ということで，きちんとペアエンドを2つのfastqファイルに変換するには，&#8211;split-filesというオプションを使用する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump --split-files ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>


<p>これで，きちんと2つのfastqファイルが出力される．個別のfastqファイルを見ると，やはり先程の変換はペアエンドの配列を結合して出力されていることがわかる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls *.fastq
</span><span class='line'>DRR002191_1.fastq  DRR002191_2.fastq
</span><span class='line'>
</span><span class='line'>$ head -n 4 DRR002191_1.fastq DRR002191_2.fastq
</span><span class='line'>==&gt; DRR002191_1.fastq &lt;==
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>NCAATGGCAATAGCAATGCATTGAAATGAAAAGGCATTTACCAGGAGCAGGAAAGCCAGAAAGAGGAGCAGTGNNCNNGGAGTTGCNNNN
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>#1=DDFFFHFFHHIIJJJIJJIJJIJBCHHIHGCC&gt;FGIGIHDH?HJIEGEGI@FEHIJIIE@HI9=EH=CDD#################
</span><span class='line'>
</span><span class='line'>==&gt; DRR002191_2.fastq &lt;==
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>TGCTTGGCTTCATCTTTTGCAATTCTGCATCTTTTGCATTTCCCTTCTCGCTCTGCTGCTCGCTCGCCTTTCTTNNNCGNCCTTTTCCCC
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>B@CFFDDDDFHHHGIJJIJJGGIGJIIHGHIGIJIHIJIEIIHIJJIGGHHEGI@DGCGEFHHIGIEFEFFBCB################</span></code></pre></td></tr></table></div></figure>


<h3>その他の注意点</h3>

<p>fastq-dumpに指定するsraファイルは，きちんとパスを指定しないといけないようだ．以下のようなエラーが出る場合は，絶対パスを指定するか，カレントディレクトリにsraファイルがある場合にはファイル名の前に「<strong>./</strong>」を付け加える．</p>

<h4>パスが解決できなくて実行できない例</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump DRR002191.sra
</span><span class='line'>2013-06-20T02:11:42 fastq-dump.2.3.2 err: name not found while resolving tree within virtual file system module - failed to open 'DRR002191.sra'
</span><span class='line'>Written 0 spots total</span></code></pre></td></tr></table></div></figure>


<h4>絶対パスを指定するか./を付け加える</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump /path/to/DRR002191.sra
</span><span class='line'>$ fastq-dump ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>




<br/>


<p>また，他のオプションなどの情報はfastq-dumpのhelpから見ることができる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump --help
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>  /home/is/yuki-ok/biolocal/src/sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] &lt;path [path...]&gt;
</span><span class='line'>  /home/is/yuki-ok/biolocal/src/sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] [ -A ] &lt;accession&gt;
</span><span class='line'>
</span><span class='line'>INPUT
</span><span class='line'>  -A|--accession &lt;accession&gt;       Replaces accession derived from &lt;path&gt; in
</span><span class='line'>                                   filename(s) and deflines (only for single
</span><span class='line'>                                   table dump)
</span><span class='line'>  --table &lt;table-name&gt;             Table name within cSRA object, default is
</span><span class='line'>                                   "SEQUENCE"
</span><span class='line'>
</span><span class='line'>[...]</span></code></pre></td></tr></table></div></figure>


<h4>環境</h4>

<ul>
<li>NCBI SRA Toolkit : May 9 2013, version 2.3.2-5 release</li>
<li>fastq-dump : 2.3.2</li>
</ul>


<h4>参考</h4>

<ul>
<li><a href="http://www.ncbi.nlm.nih.gov/sra/?term=DRR002191">Whole genome resequencing of Masaru Tomita - SRA - NCBI</a></li>
<li><a href="http://trace.ddbj.nig.ac.jp/DRASearch/run?acc=DRR002191">DRR002191 - DRA Search</a></li>
<li><a href="ftp://ftp.ddbj.nig.ac.jp/ddbj_database/dra/fastq/DRA000/DRA000583/DRX001619">/ddbj_database/dra/fastq/DRA000/DRA000583/DRX001619 のインデックス</a> (こちらではまだfastq.bz2で配布されている)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacPortsで使用していない過去のバージョンのパッケージを消す＆ちょっと可視化]]></title>
    <link href="http://yagays.github.io/blog/2013/06/19/macports-distfile-clean/"/>
    <updated>2013-06-19T04:29:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/19/macports-distfile-clean</id>
    <content type="html"><![CDATA[<p>MacBook Airの128GBしかない非力なSSDもそろそろ満杯になってきた．最近ではスワップが溜まっては再起動するということを繰り返していたので，色々と容量の食っているファイルをちまちま消していた．そういった地道な努力をしつつ無駄なキャッシュや不必要なファイルなどをちゃんと調べてみると，どうやらMacPortsが10GB近く占めていることがわかり，しかもバージョンアップによって使用されなくなったgccやllvm，boostなどの古いバージョンのせいだとわかったので，思い切って消してみることにした．</p>

<h3>古いバージョンのパッケージを消す</h3>

<p>MacPortsでは，バージョンが上がってアップデートする際に，インストールされているバージョンのパッケージをInactiveとし最新バージョンをActiveにするという方法で，過去のバージョンのパッケージが残される仕組みになっている．ということで，Inactiveなパッケージを全て消去するために，以下のコマンドを使用する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo port -u uninstall</span></code></pre></td></tr></table></div></figure>


<p>これでInactiveなパッケージを全て消去することができる．なお，古いバージョンとして残っていたパッケージを消すということは気軽にバージョンを変更したりできなくなるということなので，もしバージョンアップによる不具合に対応する必要が出てくると後々面倒になることを注意しておきたい．</p>

<h3>結果</h3>

<p><strong>/opt/localが10GBから5GB</strong>になり，かなりのダイエットに成功した！容量の少ないMacBook Airでも，これでもうちょっと耐えられそう．</p>

<h3>DaisyDiskによる可視化</h3>

<p>さて，ここからは完全に蛇足なのだが，最近使用しているのDaisyDiskという可視化ソフトウェアがなかなか気に入っているので，今回のファイル消去の前後で/opt/localがどれだけ変化するかを確かめてみた．DaisyDiskは本来はインタラクティブに操作することができて，気になる箇所をクリックしたりして拡大することができるのだが，今回は前後の画像比較だけとなっている．使い心地が気になる人は以下のリンクから公式ページに飛んで紹介動画を見ていただきたい．</p>

<p><a href="http://www.daisydiskapp.com/">DaisyDisk - Analyze disk usage and free up disk space on Mac</a></p>

<h4>Before</h4>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/macports_before.png" alt="" /></p>

<h4>After</h4>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/macports_after.png" alt="" /></p>

<p>まず全体の容量が10GBから5GBに変わっており，それぞれの面積は単純に割合を示していることに注意．Inactiveなパッケージを消去する前は少数の比較的大きな領域がいくつも見られる．具体的には，左側の外縁から2つ目のピンクの領域がgccやllvm，boost，emacsなどのパッケージのディレクトリを表しており，一番外縁にある灰色がそれぞれのパッケージのファイルになっている．一つのパッケージに複数の灰色で区切られた細かいディレクトリが含まれているのは，それだけアップデートして古いバージョンのパッケージが溜まっていったからだ．そしてInactiveなパッケージを消去した後は，複数の小さな領域が組み合わさっていることがわかり，それぞれのパッケージに含まれるファイルの容量が小さくなったことを示している．</p>

<h4>参考</h4>

<ul>
<li><a href="http://guide.macports.org/">MacPorts Guide</a></li>
<li><a href="http://d.hatena.ne.jp/kanonji/20091025/1256495516">MacPortsをちゃんと使うために調べてみた - kanonjiの日記</a></li>
<li><a href="http://d.hatena.ne.jp/t_mimori/20101020/1287568644">Macでディスク容量が足りなくなってきたら1【portをcleanしよう】 - En blanc et noir</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[スライドメモ："Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson"]]></title>
    <link href="http://yagays.github.io/blog/2013/06/18/slide-diffexp-denovo-assembled-transcriptomes/"/>
    <updated>2013-06-18T13:43:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/18/slide-diffexp-denovo-assembled-transcriptomes</id>
    <content type="html"><![CDATA[<p>de novoトランスクリプトームアセンブリに関して配列クラスタリングを解説しているスライドがあったので，ちょっと読んで大まかな流れを追ってみた．ただし後半のクラスタリングの具体的な部分は少し割愛しているほか，内容の正確性は保証できないので注意．</p>

<h2>de novoトランスクリプトームアセンブリの発現差異解析</h2>

<iframe src="http://www.slideshare.net/slideshow/embed_code/18507979?rel=0" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/AustralianBioinformatics/differential-expression-analysis-of-de-novo-assembled-transcriptomes" title="Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson" target="_blank">Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson</a> </strong> from <strong><a href="http://www.slideshare.net/AustralianBioinformatics" target="_blank">Australian Bioinformatics Network</a></strong> </div></p>

<h4>非モデル生物におけるRNA-Seq</h4>

<ul>
<li>非モデル生物におけるRNA-Seq；トランスクリプトームのde novoアセンブル

<ul>
<li>ゲノムアノテーションやゲノム配列が無い状態での解析</li>
</ul>
</li>
</ul>


<h4>トランスクリプトームアセンブラ</h4>

<p>ゲノムアセンブリにおいてはカバレッジに合わせてk-merの長さを最適化していたが，トランスクリプトームアセンブリでは遺伝子ごとに発現量が違うためカバレッジに大きな幅がある</p>

<ul>
<li>解決方法1：ゲノムアセンブラを使って異なるk-merのアセンブル結果を組み合わせる

<ul>
<li><a href="http://www.bcgsc.ca/platform/bioinfo/software/trans-abyss">Trans-ABySS</a>や<a href="http://www.ebi.ac.uk/~zerbino/oases/">Oases</a></li>
</ul>
</li>
<li>解決方法2：トランスクリプトームに特化したアセンブラで単一のk-merでアセンブルする

<ul>
<li><a href="http://trinityrnaseq.sourceforge.net/">Trinity</a></li>
</ul>
</li>
</ul>


<h4>リード数の増加によるアセンブル配列の増加</h4>

<p>横軸がNGSで得られたリード数，縦軸がアセンブルされた配列数を表しており，データ数が増加するにしたがってアセンブルされる配列数も線形に増加する．</p>

<p>スライドでは著者名が間違っているが，ここで引用している図は以下の論文のもの．</p>

<blockquote><p>Francis, W. R. et al. A comparison across non-model animals suggests an optimal sequencing depth for de novo transcriptome assembly. BMC Genomics 14, 167 (2013).</p>

<p><a href="http://www.biomedcentral.com/1471-2164/14/167">http://www.biomedcentral.com/1471-2164/14/167</a></p></blockquote>

<p>余談だが，この論文によるとデータ数が増加するとアセンブルされた配列数が増えるものの，アセンブルされた配列の平均長やN50は途中でサチることが確認されている．この論文の結論としてはデータ数は20M〜30M付近で十分だよねという感じらしい．</p>

<h4>De Bruijn Graphの複雑性</h4>

<p>シーケンスエラーやヘテロ接合箇所などの僅かな配列の違いも異なる配列として出力</p>

<h4>カバレッジの変動</h4>

<p>単一遺伝子内でNGSのショートリードのカバレッジに差があると，複数本の細切れの配列になってしまうことがある</p>

<h4>アイソフォーム単位で発現解析するか遺伝子単位で発現解析するか？</h4>

<ul>
<li>アイソフォーム単位

<ul>
<li>扱う配列が多くなるので大変</li>
<li>発現量解析のときに複数箇所にマッピングされる配列が生じるので発現量推定が大変</li>
<li>全ての転写物に関してアイソフォームがあるわけではない</li>
</ul>
</li>
<li>遺伝子単位

<ul>
<li>スプライシングなどを無視することになる</li>
<li>どうやって複数の転写物を幾つかの遺伝子にまとめるのか？</li>
</ul>
</li>
</ul>


<h4>どうやってアセンブルされた転写物を遺伝子単位にクラスタリングするのか</h4>

<p>アセンブルされた転写物のクラスタリングにおいて決定打は無いものの，幾つか方法はある（配列で共通している箇所を見つけてクラスタリングするとか）</p>

<h4>クラスタリングする際に使える情報</h4>

<ul>
<li>アセンブルする際に出力されるlocus/componentの情報</li>
<li>CD-HITやBlastclustなどの配列相同性によるクラスタリングツール</li>
</ul>


<h4>クラスタリングにはTP・TN・FP・FNを数えて適合率Precisionや再現率Recallを見る</h4>

<p>どうやらTrinityのクラスタリングは良くて，OasesとCD-HIT-ESTの組み合わせは良くないらしい（Trinity&#8217;s clusteringはRSEMのこと？）</p>

<ul>
<li>CD-HIT-ESTは適合率は高いが再現率は低い

<ul>
<li>配列情報しか使用しないので精度が低い</li>
</ul>
</li>
<li>考えられる他の方法

<ul>
<li>発現量が低い領域は重みを軽くしたい</li>
<li>サンプル間で発現量の違う配列は区別したい</li>
<li>ペアエンドリードを考慮したい</li>
</ul>
</li>
</ul>


<h4>(クラスタリングの具体的な部分は省略)</h4>

<h4>どのようにしてリード数から発現量に変換するのか</h4>

<ul>
<li>TrinityやOasesが推奨する方法

<ul>
<li>アセンブルされた配列に対してマッピング（複数箇所にマップされてもいい）</li>
<li>複数箇所にマップされた配列も考慮して発現量を求めるプログラムを使う(RSEMとか)</li>
</ul>
</li>
</ul>


<p>実際に行われている方法としてよくあるのは，一番長くアセンブルされた配列に代表させてマッピングして発現量を推定するというもの</p>

<h4>まとめ</h4>

<ul>
<li><strong>Q1. なんでそんなにアセンブルされた転写物が出てくるの？</strong>

<ul>
<li>既にアノテーションされている転写物よりも多くアセンブルされるから</li>
<li>de novoトランスクリプトームアセンブリはそもそも難しいから（完全長のアセンブリは目指しているのだけれども）</li>
<li>インタージェニックやノンコーティングの転写物も多くでてくるから</li>
</ul>
</li>
<li><strong>Q2. アイソフォーム単位か遺伝子単位か</strong>

<ul>
<li>遺伝子単位の方がアイソフォーム単位より良さそう</li>
</ul>
</li>
<li><strong>Q3. どうやってアセンブルされた転写物から遺伝子にクラスタリングするか</strong>

<ul>
<li>Trinityのクラスタリングは良くて，OasesとCD-HIT-ESTの組み合わせは良くない</li>
</ul>
</li>
<li><strong>Q4. どのようにしてリード数から発現量に変換するのか</strong>

<ul>
<li>3つの異なる方法で検証したが似た結果を示した</li>
<li>正確なクラスタリングによる結果を得るほうが他のパイプラインを使って発現量差異を見るほうがインパクトがある（と主張している）</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「知の逆転」読了]]></title>
    <link href="http://yagays.github.io/blog/2013/06/02/review-interview-and-intelligence/"/>
    <updated>2013-06-02T11:20:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/02/review-interview-and-intelligence</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4140883952" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>すごい人たちに自分のことについて話して貰いたいという欲求は，ある意味下世話な心理でありながら，インタビューという対話の一つの特徴でもある．それが現在も第一線で活躍し続ける研究者であれば，なおさら気になるものだ．本書「知の逆転」は，著名な学者6人のインタビューをまとめた本であり，執筆した本や今までの研究内容と絡めながら現在の主張を分かりやすく簡潔に追いかける内容となっている．聞き手が日本人であることも影響してか，所々に日本に関連した意見も見られる．彼らがどのように世界を捉え，そして日本を見ているのかを垣間見ることができるちょっと変わった面白い本でもある．</p>

<p>本書でインタビューに応じている人物は，</p>

<ul>
<li>「<a href="http://www.amazon.co.jp/gp/product/4794218788/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4794218788&linkCode=as2&tag=yagays-22">銃・病原菌・鉄</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4794218788" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ジャレド・ダイアモンド</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/400600253X/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=400600253X&linkCode=as2&tag=yagays-22">生成文法の企て</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=400600253X" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ノーム・チョムスキー</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/415050251X/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=415050251X&linkCode=as2&tag=yagays-22">火星の人類学者</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=415050251X" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>オリバー・サックス</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/4782800541/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4782800541&linkCode=as2&tag=yagays-22">心の社会</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4782800541" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>マービン・ミンスキー</strong></li>
<li><a href="http://www.akamai.com/">Akamai</a>創設者でCEOの<strong>トム・レイトン</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/4062577925/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4062577925&linkCode=as2&tag=yagays-22">二重らせん</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4062577925" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ジェームズ・ワトソン</strong></li>
</ul>


<p>と，知っている人ならばこちらが萎縮してしまうなほどの豪華な面々だ．それぞれが自分の研究分野を開拓し，その後の著作や起業，数々の科学への貢献を経て一般にも認識されうる人物ばかりである．その彼らがインタビューを通して語るのは，もちろん今まで積み上げてきた研究成果でありながら，同時にこれからの未来を予想し「今現在」の世界に向けた提言でもある．</p>

<p>本書は，そのような彼らを知らない人にピッタリの入門書でもあると同時に，たとえ著作を読んだことのある人にとっても十分に興味深い内容となっている．とにかく上で述べた名前に心当たりがあるなら，本書はまさにオススメだ．自分から見てもかなりミーハーな精神だとは思うが，それに足る並びではないだろうか．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[段落の冒頭に強い接続語を使ってはいけない…?]]></title>
    <link href="http://yagays.github.io/blog/2013/06/01/paragraph-heading/"/>
    <updated>2013-06-01T10:57:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/01/paragraph-heading</id>
    <content type="html"><![CDATA[<p>ここ1週間ほどご無沙汰だったので，最近考えているをちょっと書きだしてみる．</p>

<p>最近は同じ研究室の院生の学振書類を手伝ったり，大学院入学希望者の小論文にアドバイスしたりと，人の書いた文書にひたすら赤を入れる日々だったのだが，やはり添削というのは難しい．まず第一には文章を書いた人間の意図を最大限汲み取り，一方では段落や文章の流れを整え単語の使い方を統一するためにバッサバッサとメスを入れ，原形をとどめつつも改善策を提示していかなければならない．ハッキリと論理的に指摘できる部分もあれば，「この言葉の使い方はなんかダサい」みたいな主観的な意見が入ることもある．個人的には徹底して型にはめて形式化された文章にすべきだというスタンスなのだが，逆に書き手の意図さえ伝われば，たとえ文章が全部ひとつなぎになっていようが口語的口調であったり文学的口調になっていようがそれでもいいという人も中にはいるわけで，あまり押し付けがましいのも良くないかなと思う．そして何より，添削しているお前の書く文章はどうなんだと言い返されそうで，自分のやっていることが本当に正しいのかどうか怪しいのが一番怖い．</p>

<p>まあこんなコトを思いつつも，やってくれと言われれば無慈悲に赤を入れるわけだけれども，たまたま見つけた修論の指導を目的とした配布資料にナルホドと思いつつも少し引っかかることが書いてあったので，ここで紹介する．</p>

<p><a href="http://www-utheal.phys.s.u-tokyo.ac.jp/~maxima/NetEd/To_M2/To_M2.pdf">http://www-utheal.phys.s.u-tokyo.ac.jp/~maxima/NetEd/To_M2/To_M2.pdf</a></p>

<p>この中の「2.2 段落（paragraph）」は非常によくまとまっている．英語では特にパラグラフ・リーディングなどで紹介されることが多い段落の構成法は，文章を書く上で何より意識しなければいけない「決まり事」だ．私も文章をあらかた書いたあとに形を整える作業をする際には，このような体裁に特に気をつけている．</p>

<p>この資料では段落を作る上でやって良いこと悪いことといった約束がいくつか紹介されており，それらは納得のいくものばかりなのだが，その中で一つ気になる項目があった．それは「段落の冒頭に強い接続語を使ってはいけない」という部分で，前後の文章を強く結ぶ言葉は段落の冒頭に使うにはふさわしくないということらしい．これに関しては確かに納得のいく部分もある反面，本当に使ってはいけないと断言してしまっていいのだろうかという感覚もある．特に逆接は禁止するほど悪い方法ではないと個人的に思っていて，前の話題にかぶせる形で議論の対象を変えるのに便利に使っている節がある．おそらくこの使い方は段落をひとまとまりとしてみた時の列挙に近い感覚なので，それは確かに段落に分けるべきではないかもしれない．しかしながら，あまりに膨らんだ内容を無理矢理一つに詰め込むのも良くないので，何かしらの方法で分けるべきだとは思う．やはりそこは，逆接を使わずにいきなり新しい話題でスタートしつつ，ところどころに前の段落との関連をいれていくべきなのだろうか．その方法もできなくはないけど，別に逆接を使ってもいいんじゃない？と思ってしまう．具体例が無い状態でアレコレ言うのは生産性が無いというのは分かっていても，なんとなく気になっている．</p>

<p>以上のようなことをこの1週間の間ぼやーっと考えていたわけだけれども，いまだに結論は出ていない．型にはまった文章を書くというのは，読み手が想定する文章を提示することであって，それは論理構造や正確さ以上に読みやすさという重要な役割を果たしていると思っているのだが，読みやすさの感覚は人それぞれなので，なかなかに統一が難しい．</p>

<br/>


<p>（余談）それにしても，ここの文章はどういうアレで書けばいいのか未だによく分からない．表現の境界線を行ったり来たりという感じで，まあ実験場のようなイメージなので，ここで下手糞な文章を書いてても勘弁して欲しい…．</p>
]]></content>
  </entry>
  
</feed>
