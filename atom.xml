<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Wolfeyes Bioinformatics beta]]></title>
  <link href="http://yagays.github.io/atom.xml" rel="self"/>
  <link href="http://yagays.github.io/"/>
  <updated>2013-07-07T15:22:28+09:00</updated>
  <id>http://yagays.github.io/</id>
  <author>
    <name><![CDATA[yag_ays]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ゲノムアセンブリにおいて最適なk-merを推定するKmerGenieを試してみた]]></title>
    <link href="http://yagays.github.io/blog/2013/07/07/genome-assembly-kmergenie/"/>
    <updated>2013-07-07T15:17:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/07/genome-assembly-kmergenie</id>
    <content type="html"><![CDATA[<p>KmerGenieという，シーケンスデータのみを用いてアセンブラに依存しない形でアセンブルに指定する最適なk-merの値を求めることがでできるソフトウェアが最近出てきたので，ちょっと使ってみた．</p>

<p><a href="http://kmergenie.bx.psu.edu/">KmerGenie</a></p>

<p>そういえば以前<a href="http://yagays.github.io/blog/2012/06/06/velvetk/">VelvetK</a>という，これも同じようにk-merパラメータをデータセットから推定するツールについて書いたが，これは名前の通りVelvetに特化したソフトウェアだった．2007年くらいに公開され使われ始めたVelvetは，NGSの普及と相まってかなり長い間ゲノムアセンブラの代表例として使われてきた印象がある．その後ABySSやMIRA，ALLPATHS-LG，Rayなどの様々なゲノムアセンブラ出てきたが，それらに対してVelvetKのようなk-mer最適化のようなツールは無かったようで，そのあたりアセンブラに非依存というのがKmerGenieの売りのようだ．</p>

<h3>KmerGenieの仕組み</h3>

<p>KmerGenieがどういう仕組みで最適なk-merを推定しているかは，以下のスライドやarXiv.orgで公開されている論文に詳しく書かれている．</p>

<ul>
<li><a href="http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf">http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf</a></li>
<li><a href="http://arxiv.org/abs/1304.5665">[1304.5665] Informed and Automated k-Mer Size Selection for Genome Assembly</a></li>
</ul>


<p>これによると，単純に言ってしまえば最適なk-merの出現頻度というものはきれいな正規分布に従うと仮定した上でk-merの値を色々変えて出現頻度のヒストグラムを作成し，生成モデルを立ててフィッティングをした上で最適なk-merの値を推定しているようだ．実際には，正規分布を仮定したゲノム配列の分布と，パレート分布を仮定したエラーの配列の分布が混ざった混合分布を考えて，その混合の重みなども考慮したパラメータ最適化をしているらしい．</p>

<h3>KmerGenieのインストール</h3>

<p>まずはKmerGenieをインストールする．ダウンロードしたファイルを展開すると既にディレクトリにkmergenieという実行ファイルがあるが，makeをしないと利用することができないので注意．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://kmergenie.bx.psu.edu/kmergenie-1.5397.tar.gz
</span><span class='line'>$ cd kmergenie-1.5397
</span><span class='line'>$ make</span></code></pre></td></tr></table></div></figure>


<h3>KmerGenieの実行</h3>

<p>それでは実際にfastqファイルに対してKmerGenieを実行してみる．</p>

<h4>1. ゲノムアセンブリのチュートリアルで使用されたE. coliのゲノムシーケンス</h4>

<p>まずは2013年の6月に行われた<a href="http://bioinformatics.msu.edu/ngs-summer-course-2013">MSU NGS Summer Course 2013</a>のチュートリアルで使用されたサンプルデータを使用する．これはE. coliのゲノムシーケンスで，元は<a href="http://www.ncbi.nlm.nih.gov/pubmed/21926975">Chitsaz et al.</a>が出したデータの一部のようだ．中身は約4.7Mの70bpのシングルリード．</p>

<p><a href="http://ged.msu.edu/angus/tutorials-2013/assembling-ecoli-with-velvet.html">Assembling E. coli sequences with Velvet — ANGUS 2.0 documentation</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -O https://s3.amazonaws.com/public.ged.msu.edu/ecoli_ref-5m-trim.fastq.gz
</span><span class='line'>$ gunzip ecoli_ref-5m-trim.fastq.gz</span></code></pre></td></tr></table></div></figure>


<p>それではecoli_ref-5m-trim.fastqに対してKmerGenieを実行する．実行後に出力される各k-merでのカウントデータや分布をプロットしたpdfはカレントディレクトリにそのまま出力されるので，もし必要ならディレクトリを別途作ってそこで実行したほうが良いだろう．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie ecoli_ref-5m-trim.fastq
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~130 M distinct 41-mers are in the reads
</span><span class='line'>K-mer sampling: 1/100
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 61 51 41 31 21 
</span><span class='line'>-----------------------------------------------------------------------------------------------------------------------------Total time Wallclock  77.7066 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>estimation of the best k so far: 51
</span><span class='line'>refining estimation around [45; 57], with a step of 2
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~139 M distinct 39-mers are in the reads
</span><span class='line'>K-mer sampling: 1/100
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 57 55 53 51 49 47 45 
</span><span class='line'>-----------------------------------------------------------------------------------------------------------------------------Total time Wallclock  56.4315 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 45
</span><span class='line'>fitting histogram for k = 47
</span><span class='line'>fitting histogram for k = 49
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 53
</span><span class='line'>fitting histogram for k = 55
</span><span class='line'>fitting histogram for k = 57
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>table of predicted num. of genomic k-mers: histograms.dat
</span><span class='line'>best k: 55</span></code></pre></td></tr></table></div></figure>


<p>今回の場合，KmerGenieによる最適なk-merの値は55となった．出力結果で示されるk-mer頻度のプロットを並べて見ると，確かにkが小さいときは少しいびつになっており，k=55あたりで一番正規分布っぽくなっている…??ことがわかる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/kmergenie1.png" alt="" /></p>

<p>ただ，このサンプルーデータを解析しているチュートリアルでは，k-merを31〜35にしてvelvetでアセンブルしていることから，今回の55という推定値は少し大きめのような気がする．</p>

<h3>2. Assemblathon2で使用されたセキセイインコのゲノムシーケンス</h3>

<p>次はもう少し大きいデータで試してみよう．Assemblathon2というアセンブリツールの性能を競うコンペで使用されたセキセイインコのゲノムシーケンスの中からDuke Illumina GAIIx runsというデータを使ってみる．中身は76bpのペアエンドで合計で126M readsある．</p>

<p><a href="http://bioshare.bioinformatics.ucdavis.edu/Data/hcbxz0i7kg/Parrot/illumina_duke_runs/">Index of /Data/hcbxz0i7kg/Parrot/illumina_duke_runs</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ lftp -e "mirror" http://bioshare.bioinformatics.ucdavis.edu/Data/hcbxz0i7kg/Parrot/illumina_duke_runs/ 
</span><span class='line'>$ cat *.fastq &gt; budgie.fastq</span></code></pre></td></tr></table></div></figure>


<p>Assemblathonのサイトによるとセキセイインコのゲノムは二倍体らしいので，今回はKmerGenieの「&#8211;diploid」のパラメータを付けて実行する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie --diploid budgie.fastq
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~3721 M distinct 46-mers are in the reads
</span><span class='line'>K-mer sampling: 1/1000
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 71 61 51 41 31 21
</span><span class='line'>---------------------------------------------------------------------------------------------------------------------------Total time Wallclock  3035.49 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>fitting histogram for k = 71
</span><span class='line'>estimation of the best k so far: 21
</span><span class='line'>refining estimation around [15; 27], with a step of 2
</span><span class='line'>running histogram estimation
</span><span class='line'>Linear estimation: ~6335 M distinct 24-mers are in the reads
</span><span class='line'>K-mer sampling: 1/1000
</span><span class='line'>| processing                                                                                         |
</span><span class='line'>[going to estimate histograms for values of k: 27 25 23 21 19 17 15
</span><span class='line'>---------------------------------------------------------------------------------------------------------------------------Total time Wallclock  2305.61 s
</span><span class='line'>fitting model to histograms to estimate best k
</span><span class='line'>fitting histogram for k = 15
</span><span class='line'>fitting histogram for k = 17
</span><span class='line'>fitting histogram for k = 19
</span><span class='line'>fitting histogram for k = 21
</span><span class='line'>fitting histogram for k = 23
</span><span class='line'>fitting histogram for k = 25
</span><span class='line'>fitting histogram for k = 27
</span><span class='line'>fitting histogram for k = 31
</span><span class='line'>fitting histogram for k = 41
</span><span class='line'>fitting histogram for k = 51
</span><span class='line'>fitting histogram for k = 61
</span><span class='line'>fitting histogram for k = 71
</span><span class='line'>table of predicted num. of genomic k-mers: histograms.dat
</span><span class='line'>best k: 21</span></code></pre></td></tr></table></div></figure>


<p>データ量が多いと計算時間もかなり大きくなるようで，出力にも少し書かれている通り，今回は1時間半くらいかかった．</p>

<p>さて，今回の場合はKmerGenieによる最適なk-merの値は21となった．出力結果で示されるk-mer頻度のプロットを並べて見てみると，kが大きい値の時には確かに正規分布として近似できないくらいなだらかな曲線になっているものの，K=21の最適値付近でもその様子はあまり変わっていないような感じがする．そもそも可視化の都合もあって横幅が揃えられていなかったりと色々と問題はあるが，とりあえずは今回のデータから推定された最適値は21だった．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/kmergenie2.png" alt="" /></p>

<h3>KmerGenieで使用できるパラメータ</h3>

<p>以上のように，KmerGenieは幾つかのk-merの値で実行して最適と思われる値を自動で出力するが，他にも手動でパラメータを指定することでk-merの範囲や刻みを変更することができる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ kmergenie
</span><span class='line'>KmerGenie
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>    kmergenie &lt;read_file&gt; [options]
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>    --diploid    use the diploid model
</span><span class='line'>    --one-pass   skip the second pass
</span><span class='line'>    -k &lt;value&gt;   largest k-mer size to consider (default: 121)
</span><span class='line'>    -l &lt;value&gt;   smallest k-mer size to consider (default: 15)
</span><span class='line'>    -s &lt;value&gt;   interval between consecurive kmer sizes (default: 10)
</span><span class='line'>    -e &lt;value&gt;   k-mer sampling (default: auto-detected power of 10)"
</span><span class='line'>    -o &lt;prefix&gt;  prefix of the output files (default: histograms)"</span></code></pre></td></tr></table></div></figure>


<h3>まとめ</h3>

<p>このように，KmerGenieはシーケンスデータのみを用いてアセンブラに依存しない形で，アセンブルに指定する最適なk-merの値を求めることができる．推定された値が最良のアセンブルに繋がるかは実際に確認してみないと判断できないが，大まかな目安であったりk-mer決定プロセスの理由付けなどに力を発揮するだろう．今回は結果に対してそれほど詳細な評価を行わなかったが，<a href="http://arxiv.org/abs/1304.5665">arXiv.orgで公開されている論文</a>の方では<a href="http://gage.cbcb.umd.edu/index.html">GAGE</a>のデータセットに対してアセンブル結果を含めた性能比較をしているので，そちらも参考にしていただきたい．</p>

<h3>参考</h3>

<ul>
<li><a href="http://kmergenie.bx.psu.edu/">KmerGenie</a></li>
<li><a href="http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf">http://ged.msu.edu/angus/tutorials-2013/files/rayan-2013-june-18-msu.pdf</a></li>
<li><a href="http://arxiv.org/abs/1304.5665">[1304.5665] Informed and Automated k-Mer Size Selection for Genome Assembly</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[だれかicanhazpdfできる？？？]]></title>
    <link href="http://yagays.github.io/blog/2013/07/06/icanhazpdf/"/>
    <updated>2013-07-06T17:02:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/06/icanhazpdf</id>
    <content type="html"><![CDATA[<p>#icanhazpdfというTwitterのハッシュタグがある．これが何を意味しているかは，実際に使われている例を見てもらったほうが早いだろう．</p>

<p><a href="https://twitter.com/search/%23icanhazpdf">Twitter / Search - #icanhazpdf</a></p>

<p>つまり，読みたい論文があるのに所属している大学/研究所/企業がライセンスを契約していないからpdfが取れない！という時に，</p>

<ul>
<li><p>&#8220;<strong>この論文のPDF取れる人いますか？ &#8220;hoge et al. piyopiyo&#8221; http://xxxxxx… #icanhazpdf</strong>&#8221;</p></li>
<li><p>&#8220;<strong>だれか#icanhazpdf できる？ http://xxxxx…</strong>&#8221;</p></li>
</ul>


<p>といった感じでTwitterに投稿するというわけだ．これを見た知り合いか誰かがpdfをメールで送ってさえくれれば，気になる論文がチェックできる！</p>

<p>とまあ，これはヤバいだろというのは誰が見ても明らかで，Twitterで検索した結果を見ても，実際にこのハッシュタグに疑問を投げかけている人も多数存在する．ここでは法やモラルの問題に踏み込みたくないので判断は各自に任せるとして，近年の論文誌の購読価格の上昇であったりオープンアクセスの流れなどが影響しているのは確かだろう．まあ，Twitterの登場によって今まで内輪で行われてきた行為が表面化してきているだけというのが一番大きいのだろうけれども．</p>

<h3>icanhaz&#8230;の元ネタ</h3>

<p>icanhazという意味不明な英文法の元ネタは以下の猫の画像．tumblrによく流れてくるようないわゆる画像に文字を付ける遊びの一種で，本来は&#8221;I Can Has Cheezburger?&#8221;だったようだ．このタイトルを冠したネタサイトがあるほか，Wikipediaにも個別記事が作られている．</p>

<p><a href="http://www.sodahead.com/fun/lolcats-do-you-have-a-favorite-lol-cat-or-other-lol-picture/blog-326585/?link=ibaf&imgurl=http://images.sodahead.com/slideshows/000000783/icanhascheeseburger-14422465099_xlarge.jpeg&q="><img src="http://images.sodahead.com/slideshows/000000783/icanhascheeseburger-14422465099_xlarge.jpeg"><br> pics on Sodahead</a></p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/I_Can_Has_Cheezburger%3F">I Can Has Cheezburger? - Wikipedia, the free encyclopedia</a></li>
<li><a href="http://news.cnet.com/8301-13772_3-10023722-52.html">The history of I Can Has Cheezburger | Geek Gestalt - CNET News</a></li>
</ul>


<h3>その他icanhazpdfの参考サイト</h3>

<ul>
<li><a href="http://eurekajournals.org/eureka/index.php?title=Icanhazpdf">Icanhazpdf - Eureka Journal Watch</a></li>
<li><a href="http://www.samuelpean.com/icanhazpdf-reddit-scholar-pirateuniversity-org-aaaaarg-org-how-scientist-community-bypasses-journals-paywalls/">#ICanHazPDF, Reddit Scholar, PirateUniversity.org &amp; AAAAARG.org: How scientific community bypasses journals paywalls | Samuel PÉAN</a></li>
<li><a href="http://neuroconscience.com/2013/01/16/join-papester-collective-1-0-how-to-reply-to-icanhazpdf-in-3-seconds/">How to reply to #icanhazpdf in 3 seconds | Neuroconscience</a></li>
<li><a href="http://current.ndl.go.jp/node/20709">ハーバード大学、価格高騰する学術雑誌の購読中止を視野に入れた対策案を教員等に提示 | カレントアウェアネス・ポータル</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zenithのホッチキスを買った]]></title>
    <link href="http://yagays.github.io/blog/2013/07/03/zenith-stapler/"/>
    <updated>2013-07-03T19:52:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/03/zenith-stapler</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=B007RIHVOG" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>Zenithのホッチキスを買った．前々から一目惚れしていたものの値段のせいあって買うのをためらっていたのだが，いつも使っているホッチキスで上手く綴ることができなくなってきたので買い替えとばかりに思い切って購入．最近ご無沙汰だった文房具熱が再発しそうで少し怖い．</p>

<ul>
<li><a href="http://freedesign.jp/stationery/ktzsp001.php">BALMA ZENITH STAPLER 591 - バルマ ゼニス ステープラー</a></li>
</ul>


<p>第一印象としては，とにかく使い心地が良い．見た目からはデカくてゴツくて荒っぽい印象なのだけれども，やはり通常のホッチキスと比べてそもそも構造が違うのが，使用感に大きく影響してくる．とにかく力を入れなくても簡単に紙を綴ることができるので，だからといって楽とは少し違うのだけれども，普通に使う分には全く問題がない．正確には計っていないが10枚くらいなら余裕で綴ることができるし，頑張れば20枚程度はいけそうな感じがする．あと個人的に気に入っているのが綴るときの音で，鉄の擦れる音やバネの軋む音が心地良い．</p>

<p>まだあまり使っていないので，これからが楽しみだ．Zenithのホッチキスで綴た論文を，頑張って読もう…．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/zenith.jpg" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["Zero Dark Thirty"のBlu-rayを買った]]></title>
    <link href="http://yagays.github.io/blog/2013/07/02/blu-ray-zero-dark-thirty/"/>
    <updated>2013-07-02T00:36:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/02/blu-ray-zero-dark-thirty</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=B00B1E6FF8" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>映画&#8221;Zero Dark Thirty&#8221;の北米版Blu-rayを買った．映画として完成度が高いというのは言うまでもないものの，個人的な思い入れが強い作品でもある．この映画で描かれるCIAの女性捜査官には，何かを追い求める執念であったり不屈の精神に強く惹かれるところがある．それをもう一度確かめたくて，9月の日本版の発売を待たず海外版を購入してしまったというわけだ．当然ながら日本語字幕は無い．</p>

<p>今回購入したBlu-ray/DVD Comboには，Blu-rayとDVDが1枚ずつ入っている．Blu-rayは日本と北米ともに同じリージョンなのでPS3などのプレイヤーで普通に観ることができる．DVDはリージョンが違うため日本のプレイヤーでは通常は見ることができないため，PCなどの環境を別途用意する必要がある．(<a href="http://www.amazon.com/gp/help/customer/display.html?ie=UTF8&amp;nodeId=3193231">参考</a>)</p>

<p>さて，今回初めて北米版のDVDを買って色々知ったのだが，映画の音声に通常の英語版に加えて，Descriptive Serviceというモードが付いている．これはどうやら視覚障害を持つ方向けのサービスのようだ．</p>

<p><a href="http://en.wikipedia.org/wiki/Descriptive_Video_Service">Descriptive Video Service - Wikipedia, the free encyclopedia</a></p>

<p>実際に聞いてみるとわかりやすいのだが，Descriptive Serviceはいわゆるオーディオブックのようなものだ．映画オリジナルのセリフの間に，情景の説明や誰が何をしたといったことがナレーションとして入っている．画面に表示される文字や暗転したといったことも含めて，セリフでカバーできない画面の内容が英語音声で説明されるので，画面を観ることができない人でも映画を楽しめるようになっている．本来はそのような用途で使用されるものなのだが，このナレーションは聞き取りやすい声で口調もそれほど速くないため，最近はこれを流しながら英語を聞く耳を鍛えている．Descriptive Serviceは当然ながら字幕に出すことができないので，英語学習に向いているとは言えないものの，好きな映画を見ながらセリフ以外のところで英語の学習ができるので，個人的に気に入っている．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「世界基準で夢をかなえる私の勉強法」読了]]></title>
    <link href="http://yagays.github.io/blog/2013/07/01/review-kitagawa-be-yourself-and-carry-on/"/>
    <updated>2013-07-01T22:56:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/07/01/review-kitagawa-be-yourself-and-carry-on</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=434402334X" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p><a href="http://www.amazon.co.jp/gp/product/4106104695/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4106104695&linkCode=as2&tag=yagays-22">ハーバード白熱日本史教室 (新潮新書)</a><img src="http://ir-jp.amazon-adsystem.com/e/ir?t=yagays-22&l=as2&o=9&a=4106104695" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />で知られる北川智子氏の人生をなぞるようにして，彼女の勉強に対する姿勢や方法を書き起こした本．高校時代にカナダに留学した時から始まり，そのままカナダの大学へと進学，専攻を変えつつアメリカの大学院を渡り歩き，そしてハーバードでのカレッジ・フェローとイギリスのケンブリッジ大への挑戦という各段階において，どのように勉強をこなし成果を上げてきたかという経験が語られる．国を変え，専攻を変え，研究対象を変える．彼女はそのようにして興味を追いかけハードルを乗り越えながら勉強してきたという．その中で培われてきた独自の勉強法が，彼女の体験とともに紹介される．</p>

<p>本書では様々な勉強法が紹介されるが，全体に共通するのは恐ろしいほどの勉強量である．文章の端々に表れる異常なまでの勉強時間，勉強量，集中力，アウトプット，そのどれもが，字面だけでは到底推し量ることのできないほどの苦痛と困難に満ちていたであろうことを，どうしても感じとってしまう．例えば，1週間の日程表は午前7時から始まり27時まで書かれている．文字通り100杯以上のコーヒーを飲み，博論を書き続けて気付けば5日経っていたこともあったという．そのような過去の苦労に対して，どちらかというとリアルに表現することなくソフトなタッチで書かれているために，つい大げさだと受け流してしまうかもしれない．しかしながら，その経験談の裏にある彼女の人生が辿ってきた険しい道というものは，明らかに存在する．勉強法という小手先の技術というべきものが，巨大な努力量と綿密な下準備というものによって支えられているということを，ある意味気づかせないようにしているとも取れる．</p>

<p>みんな，誰もが効率というものに取り憑かれて，少ない労力で多くの成果を得たいと思っている．活躍する人の真似をすれば少しは自分もそこに追いつけるかもしれないと思っている．と同時に，成果に繋げるには生活すべてを勉強に注ぎ込む必要があるのだとも薄々気づいている．自分に何ができるかという答えなき問題に対する不安や諦観には，とやかく人に言われるよりも，試行錯誤を繰り返して自分で見つけ出すしかない．しかしながら，他人の人生と教訓を学ぶことは，決して悪い方法ではないと僕は思っている．他人が経験した他人の人生を自分の中で上手く反芻することができれば，それは何かしらの手がかりにはなる．そういった意味で，本書で語られる北川智子氏の人生は，多くの日本人よりはるかにかけ離れたものであり，現在の日本の価値観におけるロールモデルであり，実現不可能な人生ではないことを示してくれる実例となっている．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["RNA-Seq unleashed"によるTrinityのワークフローの解説]]></title>
    <link href="http://yagays.github.io/blog/2013/06/26/rna-seq-unleashed-trinity/"/>
    <updated>2013-06-26T13:14:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/26/rna-seq-unleashed-trinity</id>
    <content type="html"><![CDATA[<p><a href="http://evomics.org/learning/genomics/trinity/">Trinity | Evolution and Genomics</a>経由で知ったのだが，Trinityのワークフローに関してより分かりやすい図が2011年のNature BiotechnologyのNews and Viewsに掲載されていたので，ここで少し紹介する．この図はTrinityのアセンブリアルゴリズムの一連のワークフローを表しており，アセンブリ結果の解釈も含めて解説されているのが大きな特徴だ．Trinityが対象にしているトランスクリプトームのアセンブリは，アイソフォームや選択的スプライシングなどの影響を受けて良く似た配列が多数出てくるため，一意に決まらない場合が多い．そういったケースに対応するために，Trinityではアセンブルのグラフを作成して考えられる候補を幾つか出力するのだけれども，実はこの結果の解釈が一番難しい．多数の類似配列が出力で得られた時にそれがどういう可能性から出てきたものなのかを考える上で，この図の一番下の解説はとても参考になる．実際はこんなに上手くいかないだろうとは思うものの，個別のアイソフォームとして配列をクラスタリングしたり，そもそもTrinityが何を考慮してアセンブルしているかを確認する際には役立つだろう．</p>

<blockquote><p>Iyer, M. K. &amp; Chinnaiyan, A. M. RNA-Seq unleashed. <em>Nat. Biotechnol.</em> <strong>29</strong>, 599–600 (2011).</p>

<p><a href="http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html">http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html</a></p></blockquote>

<p><a href="http://www.nature.com/nbt/journal/v29/n7/full/nbt.1915.html" target="_blank"><img src="http://dl.dropboxusercontent.com/u/142306/b/rna-seq_unleashed.gif" alt="CCC License Number: 3172280756988"></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Biopythonで塩基配列をアミノ酸配列にする]]></title>
    <link href="http://yagays.github.io/blog/2013/06/25/biopython-translation/"/>
    <updated>2013-06-25T09:59:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/25/biopython-translation</id>
    <content type="html"><![CDATA[<ul>
<li><a href="http://insectcell.exblog.jp/20703555/">awkとsedで塩基配列をアミノ酸配列にする : したっぱ昆虫細胞研究者のメモ</a></li>
<li><a href="https://twitter.com/chalkless/status/349052297982132225">Twitter / chalkless: @n0rr $ perl -MBio::Seq -le &#8230;</a></li>
<li><a href="https://twitter.com/dritoshi/status/349056205160841216">Twitter / dritoshi: @chalkless @n0rr &#8230;</a></li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">Bio.Seq</span> <span class="kn">import</span> <span class="n">Seq</span>
</span><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="n">mRNA</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="s">&quot;TATGAAAGT&quot;</span><span class="p">)</span>
</span><span class='line'><span class="o">&gt;&gt;&gt;</span> <span class="n">mRNA</span><span class="o">.</span><span class="n">translate</span><span class="p">()</span>
</span><span class='line'><span class="n">Seq</span><span class="p">(</span><span class="s">&#39;YES&#39;</span><span class="p">,</span> <span class="n">ExtendedIUPACProtein</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Why Biopython?</h3>

<p><a href="http://xkcd.com/353/">xkcd: Python</a></p>

<p><img src="http://imgs.xkcd.com/comics/python.png" alt="http://xkcd.com/353/" /></p>

<h3>Ref.</h3>

<ul>
<li><a href="http://biopython.org/DIST/docs/tutorial/Tutorial.html">Biopython Tutorial and Cookbook</a></li>
<li><a href="http://yagays.github.io/blog/2013/04/20/install-biopython/">Mac OS Xで手っ取り早くBiopythonをインストールして使えるようにする - Wolfeyes Bioinformatics beta</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2013 Web Server issue]]></title>
    <link href="http://yagays.github.io/blog/2013/06/24/nar-web-server-issue-2013/"/>
    <updated>2013-06-24T14:20:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/24/nar-web-server-issue-2013</id>
    <content type="html"><![CDATA[<p><a href="http://nar.oxfordjournals.org/content/41/W1.toc" target="_blank"><img src="http://dl.dropboxusercontent.com/u/142306/b/W1.cover.gif" align="right" alt="http://nar.oxfordjournals.org/content/41/W1.cover-expansion"></a></p>

<p>今年のWeb Server issue 2013が既に閲覧できるようになっていた．</p>

<p><a href="http://nar.oxfordjournals.org/">Nucleic Acid Research誌</a>は少し特殊な特別号を毎年出していて，1月にはDatabase issueが，7月にはWeb Server issueが発刊されるのが定番になっている．これらの特集号は，他の論文誌では論文になりにくいようなデータベース構築やウェブサービス系を論文として発表する場として，バイオインフォ系にとっては重要な投稿先の一つといえる．特に，ウェブサービスのアップデートなども同様の投稿ができるようになっており，新規性としては少し弱いウェブサービスの保守管理や機能追加に関しても評価されるのが大きな特徴だ（ただし2年間のインターバルが必要）．</p>

<p><strong>今年のWeb Server issue：<a href="http://nar.oxfordjournals.org/content/41/W1.toc">Table of Contents — 1 July 2013, 41 (W1)</a></strong></p>

<p>今年は合計で95本のウェブサービスに関する論文が採択されたようだ．Editorialによると</p>

<blockquote><p>For the 2013 issue, 293 summaries were submitted and 129, or 44%, were approved for manuscript submission. Of those approved, 95, or 73%, were ultimately accepted for publication.</p>

<p><a href="http://nar.oxfordjournals.org/content/41/W1/W1.full">http://nar.oxfordjournals.org/content/41/W1/W1.full</a></p></blockquote>

<p>ということで，293本の投稿のうち最終的に95本が採択され，全体で見れば約32%の採択率となっている．</p>

<br/>


<p>来年のWeb Server Issue 2014の締め切りは今年いっぱいまで(31 Dec. 2013)となっている．それまでに完全に動くウェブサービスと1ページのproposalを書いて投稿し，そこで通れば1ヶ月でmanuscriptを書いて7月に掲載という流れのようだ．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tophatにおいて使用するスレッド数を上げすぎるとファイルディスクリプタの上限でエラーになる]]></title>
    <link href="http://yagays.github.io/blog/2013/06/23/tophat-too-many-open-files/"/>
    <updated>2013-06-23T10:33:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/23/tophat-too-many-open-files</id>
    <content type="html"><![CDATA[<p>久しぶりにTophatをいじったら一発目でエラーで落ちてしまい，出鼻をくじかれてしまった…．原因を調べてみると，どうやらTophatのコマンド実行時に指定する-pオプションの数字を調子に乗って上げすぎたのが良くなかったらしい．といっても計算機のリソースに問題があったわけではなく，ユーザが使えるシステムリソースの制限を超えてしまったために，Tophatが停止してしまったようだ．この問題はTophatの公式サイトのFAQでも取り上げられているが，このBlogでもエラーメッセージとともに原因と対策を書いておこうと思う．</p>

<h3>Tophatの標準エラー出力のメッセージ</h3>

<p>今回の場合は，Tophatのログの最後で以下のようなエラーメッセージが表示される．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[...]
</span><span class='line'>[2013-06-23 00:22:33] Mapping right_kept_reads_seg3 to genome chlamydomonas_236 with Bowtie2 (3/4)
</span><span class='line'>[2013-06-23 00:33:09] Mapping right_kept_reads_seg4 to genome chlamydomonas_236 with Bowtie2 (4/4)
</span><span class='line'>[2013-06-23 00:44:57] Searching for junctions via segment mapping
</span><span class='line'>        [FAILED]
</span><span class='line'>Error: segment-based junction search failed with err =1
</span><span class='line'>Error opening SAM file output/tmp/right_kept_reads_seg1.bam</span></code></pre></td></tr></table></div></figure>


<p>これだけでは何のことだかよくわからないが，Tophatが出力するログのlogs/segment_juncs.logを見ると，このエラーに関してもう少し詳細な記述が出力されている．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    Loading scaffold_53...done
</span><span class='line'>    Loading scaffold_54...done
</span><span class='line'>    Loading ...done
</span><span class='line'>&gt;&gt; Performing segment-search:
</span><span class='line'>Loading left segment hits...
</span><span class='line'>done.
</span><span class='line'>Loading right segment hits...
</span><span class='line'>open: Too many open files
</span><span class='line'>Error opening SAM file output/tmp/right_kept_reads_seg2.bam</span></code></pre></td></tr></table></div></figure>


<p>これを見ると，どうやらsegment_juncsの実行中に「open: Too many open files」が原因で実行が落ちたようだ．</p>

<h3>原因と対策</h3>

<p>このエラーに関しては，実はTophatの公式サイトのFAQに「What should I do if I see a message like &#8220;Too many open files&#8221;?」という，まさに先ほどのエラーメッセージの内容そのままの項がある．</p>

<blockquote><p>This usually happens when using &#8220;-p&#8221; option with a large value (many threads). TopHat may produce many intermediate files, the number of which is proportional to this value; sometimes the number of the files may go over the maximum number of files a process is allowed to open. The solution is to raise the limit to a higher number (e.g. 10000). For Mac, you can change this using a command, &#8220;sudo sysctl -w kern.maxfiles=10240&#8221;.</p>

<p><a href="http://tophat.cbcb.umd.edu/faq.shtml#many_file">TopHat :: Center for Bioinformatics and Computational Biology</a></p></blockquote>

<p>ざっと要約すると「Tophatは中間ファイルを大量に作るから時々許容数超えちゃうんだよね．扱えるファイル数の上限上げるか，Macなら次のコマンドで対処してね」ということになる．さすがにこれだけではよく分からないと思うので，SEQanswersの以下のスレッドも参考にしつつ，もう少し詳しく見ていこうと思う．</p>

<p><a href="http://seqanswers.com/forums/showthread.php?t=21316">Tophat segment junction error 1, invalid BAM binary header - SEQanswers</a></p>

<h3>limitとファイルディスクリプタ数の制限</h3>

<p>さて，公式サイトのFAQにおいてファイル数の上限といった言葉が出てきたように，LinuxやMacではユーザごとに使用することのできる各種システムリソースに制限が設けられている．具体的にはユーザが使用できるCPU数やメモリの容量，プロセスの数などがそれに該当するのだが，その中に<strong>「ファイルディスクリプタ数」</strong>という項目があり，1プロセスが同時に開くことのできるファイル数の上限を定めている．これは主にひとつの計算機を複数人が使用するマルチユーザシステムにおいて，一人がリソースを独占するのを防いだりアプリケーションの暴走を止めたりするのに役立つのだが，今回のような大規模な計算を実行する際にはこれが邪魔になってしまう．</p>

<p>システムリソースの制限を確認したい場合には，「ulimit -a」または「limit」というコマンドを使う．例えば，私の環境では以下のようになる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ソフトリミット
</span><span class='line'>$ ulimit -a
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             8192
</span><span class='line'>-c: core file size (blocks)         0
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       1024
</span><span class='line'>-n: file descriptors                1024
</span><span class='line'>-l: locked-in-memory size (kbytes)  64
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 127413
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>ここで表示されるのは厳密にはソフトリミットと呼ばれ，ユーザごとに設定されている制限である．一方でハードリミットと呼ばれる制限もあり，これは管理者(root)が定める制限となっている．ソフトリミットは，このハードリミットの範囲内でしか自由に制限値を変更することはできない．ハードリミットを確認したい場合には，-Hオプションを付ける．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ハードリミット
</span><span class='line'>$ ulimit -aH
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             unlimited
</span><span class='line'>-c: core file size (blocks)         unlimited
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       127413
</span><span class='line'>-n: file descriptors                4096
</span><span class='line'>-l: locked-in-memory size (kbytes)  64
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 127413
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>さて，今回問題になっているファイルディスクリプタ数は「-n: file descriptors」で表示されている．上の例の場合，ソフトリミットでは1024，ハードリミットでは4096となっている．つまり，Tophatは1024個以上のファイルを1スレッドで開こうとしたために，このソフトリミットに引っかかってしまったようだ．</p>

<h3>フィアルディスクリプタ数の制限への対策</h3>

<p>この問題を回避するには主に2つの方法がある．</p>

<p>1．Tophatの-pオプションの値を小さくする</p>

<p>2．ソフトリミットのファイルディスクリプタの値を大きくする</p>

<p>まず1.では，Tophatが使用するスレッド数を少なくすることで，ファイルディスクリプタ数の上限に引っかからなくするというもの．一度-pオプションを無くして実行してみれば，おそらく今回のエラーには引っかからなくなるだろう．一度に開くファイル数が少なくなりほぼ確実に実行できるようにはなるが，並列処理数が減ってしまうのでTophatの実行時間は長くなってしまう．</p>

<p>そこで2.のようにの制限を無くして，-pオプションはそのままにファイルディスクリプタの上限を回避するという方法もある．実行コマンドや実行時間はそのままにエラーを回避することができる一方で，上限を上げたからといってもTophatがそれ以上の同時ファイルオープンをしてしまえば同様のエラーに引っかかってしまうほか，制限を上げたことにより計算機に負荷がかかる恐れもある．つまり，時と場合によっては成功するが確証は無いという感じだろうか．</p>

<p>ちなみに，私の場合はファイルディスクリプタ数を上げても以下のような別のエラーが出て実行できなかった．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Error: ReadStream::getRead() called with out-of-order id#!</span></code></pre></td></tr></table></div></figure>


<p>ということで，結論としてはTophatの実行時間との兼ね合いを考えて，どちらかを選択したほうが良いだろう．素直に-pオプションの値を下げるほうが無難な気がする．</p>

<p>ちなみに，制限値を引き上げるには，ulimitで以下のように値を変更する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ulimit -n 2000
</span><span class='line'>$ limit
</span><span class='line'>-t: cpu time (seconds)              unlimited
</span><span class='line'>-f: file size (blocks)              unlimited
</span><span class='line'>-d: data seg size (kbytes)          unlimited
</span><span class='line'>-s: stack size (kbytes)             8192
</span><span class='line'>-c: core file size (blocks)         0
</span><span class='line'>-m: resident set size (kbytes)      unlimited
</span><span class='line'>-u: processes                       1024
</span><span class='line'>-n: file descriptors                2000
</span><span class='line'>-l: locked-in-memory size (kbytes)  unlimited
</span><span class='line'>-v: address space (kbytes)          unlimited
</span><span class='line'>-x: file locks                      unlimited
</span><span class='line'>-i: pending signals                 16545839
</span><span class='line'>-q: bytes in POSIX msg queues       819200
</span><span class='line'>-e: max nice                        0
</span><span class='line'>-r: max rt priority                 0</span></code></pre></td></tr></table></div></figure>


<p>ulimitの後ろに該当するパラメータの値を指定することによって，上限を引き上げることができる．ただし，先ほど述べたようにハードリミットより上は指定することができないので注意が必要になる．</p>

<h3>まとめ</h3>

<p>Tophatのエラー「open: Too many open files」は，スレッドが一度に開くことのできるファイルディスクリプタ数がソフトリミットの上限に引っかかってしまったために起こる．-pオプションの値を下げて実行するか，ソフトリミットのファイルディスクリプタの上限を引き上げることによって回避することができる．まずは，-pオプションを指定せずに実行してみよう．</p>

<h4>参考</h4>

<ul>
<li><a href="http://tophat.cbcb.umd.edu/faq.shtml">TopHat :: Center for Bioinformatics and Computational Biology</a></li>
<li><a href="http://seqanswers.com/forums/showthread.php?t=21316">Tophat segment junction error 1, invalid BAM binary header - SEQanswers</a></li>
<li><a href="http://x68000.q-e-d.net/~68user/unix/pickup?limit">UNIXの部屋 コマンド検索:limit (*BSD/Linux)</a></li>
<li><a href="http://yumewaza.yumemi.co.jp/2010/07/limitsconf.html">ファイルディスクリプタ数の上限変更とlimits.confの罠 (ゆめ技：ゆめみスタッフブログ)</a></li>
</ul>


<h4>実行環境</h4>

<ul>
<li>OS：RHEL 6.3</li>
<li>Tophat：v2.0.8b</li>
<li>Bowtie：version 2.1.0</li>
<li>Samtools：0.1.19.0</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SRA Toolkitを使ってsraファイルからfastqファイルに変換する(更新版)]]></title>
    <link href="http://yagays.github.io/blog/2013/06/20/fastq-dump-sratoolkit/"/>
    <updated>2013-06-20T11:26:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/20/fastq-dump-sratoolkit</id>
    <content type="html"><![CDATA[<p>久しぶりにfastq-dumpを使ったら少し仕様が変わっていたのでメモ．だいぶ前に別の場所で書いた記事は，既に古くなってしまっている．何もせず記事を放置しておくのも申し訳ないし，どうやらfastq-dumpでググると上位にくるようなので，ここいらで更新しておかないと…．</p>

<p>(deprecated) <a href="http://g86.dbcls.jp/~yag/wordpress/archives/959">Wolf Ears » SRA Toolkitを使ってsraファイルからfastqファイルに変換する</a></p>

<h3>SRA Toolkitのインストール</h3>

<p>sraからfastqに変換するプログラムは，SRA Toolkitの中にあるfastq-dumpを使う．まずはNCBIのサイトからOS環境に合わせてコンパイルされたSRA Toolkitをダウンロードしてインストールする．</p>

<p><a href="http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software">Software : Software : Sequence Read Archive : NCBI/NLM/NIH</a></p>

<p>コンパイルされたバイナリとしては，Linux系列のCentOSやUbuntu，MacOSやMS Windowsに合わせたものが用意されている．私の環境はRed Hat Enterpriseなので，とりあえず「CentOS Linux 64 bit architecture」を選択する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.3.2-5/sratoolkit.2.3.2-5-centos_linux64.tar.gz
</span><span class='line'>$ tar zxvf sratoolkit.2.3.2-5-centos_linux64.tar.gz </span></code></pre></td></tr></table></div></figure>


<p>今回使用するfastq-dumpは展開したディレクトリのbin以下に入っている．以下のように実行して動作するか確認する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>  sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] &lt;path [path...]&gt;
</span><span class='line'>  sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] [ -A ] &lt;accession&gt;
</span><span class='line'>
</span><span class='line'>Use option --help for more information
</span><span class='line'>
</span><span class='line'>sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump : 2.3.2</span></code></pre></td></tr></table></div></figure>


<h3>fastq-dumpの使い方</h3>

<p>基本的な使い方としては，以下のようにfastq-dumpにsraファイルを指定すればよい&#8230;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>


<p>…のだが，ここでひとつ注意が必要になる．上のコマンドを指定すると，データがシングルエンドでもペアエンドでも同様に一つのファイルとして出力されてしまう．実際に変換されたfastqファイルの中身を見てみると，上のDRR002191.sraは本当は90bpのペアエンドなのだが，以下のように結合された配列として出力される．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ head -n 4 DRR002191.fastq
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=180
</span><span class='line'>NCAATGGCAATAGCAATGCATTGAAATGAAAAGGCATTTACCAGGAGCAGGAAAGCCAGAAAGAGGAGCAGTGNNCNNGGAGTTGCNNNNTGCTTGGCTTCATCTTTTGCAATTCTGCATCTTTTGCATTTCCCTTCTCGCTCTGCTGCTCGCTCGCCTTTCTTNNNCGNCCTTTTCCCC
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=180
</span><span class='line'>#1=DDFFFHFFHHIIJJJIJJIJJIJBCHHIHGCC&gt;FGIGIHDH?HJIEGEGI@FEHIJIIE@HI9=EH=CDD#################B@CFFDDDDFHHHGIJJIJJGGIGJIIHGHIGIJIHIJIEIIHIJJIGGHHEGI@DGCGEFHHIGIEFEFFBCB################</span></code></pre></td></tr></table></div></figure>


<p>ということで，きちんとペアエンドを2つのfastqファイルに変換するには，&#8211;split-filesというオプションを使用する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump --split-files ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>


<p>これで，きちんと2つのfastqファイルが出力される．個別のfastqファイルを見ると，やはり先程の変換はペアエンドの配列を結合して出力されていることがわかる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls *.fastq
</span><span class='line'>DRR002191_1.fastq  DRR002191_2.fastq
</span><span class='line'>
</span><span class='line'>$ head -n 4 DRR002191_1.fastq DRR002191_2.fastq
</span><span class='line'>==&gt; DRR002191_1.fastq &lt;==
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>NCAATGGCAATAGCAATGCATTGAAATGAAAAGGCATTTACCAGGAGCAGGAAAGCCAGAAAGAGGAGCAGTGNNCNNGGAGTTGCNNNN
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>#1=DDFFFHFFHHIIJJJIJJIJJIJBCHHIHGCC&gt;FGIGIHDH?HJIEGEGI@FEHIJIIE@HI9=EH=CDD#################
</span><span class='line'>
</span><span class='line'>==&gt; DRR002191_2.fastq &lt;==
</span><span class='line'>@DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>TGCTTGGCTTCATCTTTTGCAATTCTGCATCTTTTGCATTTCCCTTCTCGCTCTGCTGCTCGCTCGCCTTTCTTNNNCGNCCTTTTCCCC
</span><span class='line'>+DRR002191.1 FCD0BMAACXX:5:1101:1133:1889# length=90
</span><span class='line'>B@CFFDDDDFHHHGIJJIJJGGIGJIIHGHIGIJIHIJIEIIHIJJIGGHHEGI@DGCGEFHHIGIEFEFFBCB################</span></code></pre></td></tr></table></div></figure>


<h3>その他の注意点</h3>

<p>fastq-dumpに指定するsraファイルは，きちんとパスを指定しないといけないようだ．以下のようなエラーが出る場合は，絶対パスを指定するか，カレントディレクトリにsraファイルがある場合にはファイル名の前に「<strong>./</strong>」を付け加える．</p>

<h4>パスが解決できなくて実行できない例</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump DRR002191.sra
</span><span class='line'>2013-06-20T02:11:42 fastq-dump.2.3.2 err: name not found while resolving tree within virtual file system module - failed to open 'DRR002191.sra'
</span><span class='line'>Written 0 spots total</span></code></pre></td></tr></table></div></figure>


<h4>絶対パスを指定するか./を付け加える</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump /path/to/DRR002191.sra
</span><span class='line'>$ fastq-dump ./DRR002191.sra</span></code></pre></td></tr></table></div></figure>




<br/>


<p>また，他のオプションなどの情報はfastq-dumpのhelpから見ることができる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fastq-dump --help
</span><span class='line'>
</span><span class='line'>Usage:
</span><span class='line'>  /home/is/yuki-ok/biolocal/src/sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] &lt;path [path...]&gt;
</span><span class='line'>  /home/is/yuki-ok/biolocal/src/sratoolkit.2.3.2-5-centos_linux64/bin/fastq-dump [options] [ -A ] &lt;accession&gt;
</span><span class='line'>
</span><span class='line'>INPUT
</span><span class='line'>  -A|--accession &lt;accession&gt;       Replaces accession derived from &lt;path&gt; in
</span><span class='line'>                                   filename(s) and deflines (only for single
</span><span class='line'>                                   table dump)
</span><span class='line'>  --table &lt;table-name&gt;             Table name within cSRA object, default is
</span><span class='line'>                                   "SEQUENCE"
</span><span class='line'>
</span><span class='line'>[...]</span></code></pre></td></tr></table></div></figure>


<h4>環境</h4>

<ul>
<li>NCBI SRA Toolkit : May 9 2013, version 2.3.2-5 release</li>
<li>fastq-dump : 2.3.2</li>
</ul>


<h4>参考</h4>

<ul>
<li><a href="http://www.ncbi.nlm.nih.gov/sra/?term=DRR002191">Whole genome resequencing of Masaru Tomita - SRA - NCBI</a></li>
<li><a href="http://trace.ddbj.nig.ac.jp/DRASearch/run?acc=DRR002191">DRR002191 - DRA Search</a></li>
<li><a href="ftp://ftp.ddbj.nig.ac.jp/ddbj_database/dra/fastq/DRA000/DRA000583/DRX001619">/ddbj_database/dra/fastq/DRA000/DRA000583/DRX001619 のインデックス</a> (こちらではまだfastq.bz2で配布されている)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacPortsで使用していない過去のバージョンのパッケージを消す＆ちょっと可視化]]></title>
    <link href="http://yagays.github.io/blog/2013/06/19/macports-distfile-clean/"/>
    <updated>2013-06-19T04:29:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/19/macports-distfile-clean</id>
    <content type="html"><![CDATA[<p>MacBook Airの128GBしかない非力なSSDもそろそろ満杯になってきた．最近ではスワップが溜まっては再起動するということを繰り返していたので，色々と容量の食っているファイルをちまちま消していた．そういった地道な努力をしつつ無駄なキャッシュや不必要なファイルなどをちゃんと調べてみると，どうやらMacPortsが10GB近く占めていることがわかり，しかもバージョンアップによって使用されなくなったgccやllvm，boostなどの古いバージョンのせいだとわかったので，思い切って消してみることにした．</p>

<h3>古いバージョンのパッケージを消す</h3>

<p>MacPortsでは，バージョンが上がってアップデートする際に，インストールされているバージョンのパッケージをInactiveとし最新バージョンをActiveにするという方法で，過去のバージョンのパッケージが残される仕組みになっている．ということで，Inactiveなパッケージを全て消去するために，以下のコマンドを使用する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo port -u uninstall</span></code></pre></td></tr></table></div></figure>


<p>これでInactiveなパッケージを全て消去することができる．なお，古いバージョンとして残っていたパッケージを消すということは気軽にバージョンを変更したりできなくなるということなので，もしバージョンアップによる不具合に対応する必要が出てくると後々面倒になることを注意しておきたい．</p>

<h3>結果</h3>

<p><strong>/opt/localが10GBから5GB</strong>になり，かなりのダイエットに成功した！容量の少ないMacBook Airでも，これでもうちょっと耐えられそう．</p>

<h3>DaisyDiskによる可視化</h3>

<p>さて，ここからは完全に蛇足なのだが，最近使用しているのDaisyDiskという可視化ソフトウェアがなかなか気に入っているので，今回のファイル消去の前後で/opt/localがどれだけ変化するかを確かめてみた．DaisyDiskは本来はインタラクティブに操作することができて，気になる箇所をクリックしたりして拡大することができるのだが，今回は前後の画像比較だけとなっている．使い心地が気になる人は以下のリンクから公式ページに飛んで紹介動画を見ていただきたい．</p>

<p><a href="http://www.daisydiskapp.com/">DaisyDisk - Analyze disk usage and free up disk space on Mac</a></p>

<h4>Before</h4>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/macports_before.png" alt="" /></p>

<h4>After</h4>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/macports_after.png" alt="" /></p>

<p>まず全体の容量が10GBから5GBに変わっており，それぞれの面積は単純に割合を示していることに注意．Inactiveなパッケージを消去する前は少数の比較的大きな領域がいくつも見られる．具体的には，左側の外縁から2つ目のピンクの領域がgccやllvm，boost，emacsなどのパッケージのディレクトリを表しており，一番外縁にある灰色がそれぞれのパッケージのファイルになっている．一つのパッケージに複数の灰色で区切られた細かいディレクトリが含まれているのは，それだけアップデートして古いバージョンのパッケージが溜まっていったからだ．そしてInactiveなパッケージを消去した後は，複数の小さな領域が組み合わさっていることがわかり，それぞれのパッケージに含まれるファイルの容量が小さくなったことを示している．</p>

<h4>参考</h4>

<ul>
<li><a href="http://guide.macports.org/">MacPorts Guide</a></li>
<li><a href="http://d.hatena.ne.jp/kanonji/20091025/1256495516">MacPortsをちゃんと使うために調べてみた - kanonjiの日記</a></li>
<li><a href="http://d.hatena.ne.jp/t_mimori/20101020/1287568644">Macでディスク容量が足りなくなってきたら1【portをcleanしよう】 - En blanc et noir</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[スライドメモ："Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson"]]></title>
    <link href="http://yagays.github.io/blog/2013/06/18/slide-diffexp-denovo-assembled-transcriptomes/"/>
    <updated>2013-06-18T13:43:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/18/slide-diffexp-denovo-assembled-transcriptomes</id>
    <content type="html"><![CDATA[<p>de novoトランスクリプトームアセンブリに関して配列クラスタリングを解説しているスライドがあったので，ちょっと読んで大まかな流れを追ってみた．ただし後半のクラスタリングの具体的な部分は少し割愛しているほか，内容の正確性は保証できないので注意．</p>

<h2>de novoトランスクリプトームアセンブリの発現差異解析</h2>

<iframe src="http://www.slideshare.net/slideshow/embed_code/18507979?rel=0" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/AustralianBioinformatics/differential-expression-analysis-of-de-novo-assembled-transcriptomes" title="Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson" target="_blank">Differential expression analysis of de novo assembled transcriptomes - Nadia Davidson</a> </strong> from <strong><a href="http://www.slideshare.net/AustralianBioinformatics" target="_blank">Australian Bioinformatics Network</a></strong> </div></p>

<h4>非モデル生物におけるRNA-Seq</h4>

<ul>
<li>非モデル生物におけるRNA-Seq；トランスクリプトームのde novoアセンブル

<ul>
<li>ゲノムアノテーションやゲノム配列が無い状態での解析</li>
</ul>
</li>
</ul>


<h4>トランスクリプトームアセンブラ</h4>

<p>ゲノムアセンブリにおいてはカバレッジに合わせてk-merの長さを最適化していたが，トランスクリプトームアセンブリでは遺伝子ごとに発現量が違うためカバレッジに大きな幅がある</p>

<ul>
<li>解決方法1：ゲノムアセンブラを使って異なるk-merのアセンブル結果を組み合わせる

<ul>
<li><a href="http://www.bcgsc.ca/platform/bioinfo/software/trans-abyss">Trans-ABySS</a>や<a href="http://www.ebi.ac.uk/~zerbino/oases/">Oases</a></li>
</ul>
</li>
<li>解決方法2：トランスクリプトームに特化したアセンブラで単一のk-merでアセンブルする

<ul>
<li><a href="http://trinityrnaseq.sourceforge.net/">Trinity</a></li>
</ul>
</li>
</ul>


<h4>リード数の増加によるアセンブル配列の増加</h4>

<p>横軸がNGSで得られたリード数，縦軸がアセンブルされた配列数を表しており，データ数が増加するにしたがってアセンブルされる配列数も線形に増加する．</p>

<p>スライドでは著者名が間違っているが，ここで引用している図は以下の論文のもの．</p>

<blockquote><p>Francis, W. R. et al. A comparison across non-model animals suggests an optimal sequencing depth for de novo transcriptome assembly. BMC Genomics 14, 167 (2013).</p>

<p><a href="http://www.biomedcentral.com/1471-2164/14/167">http://www.biomedcentral.com/1471-2164/14/167</a></p></blockquote>

<p>余談だが，この論文によるとデータ数が増加するとアセンブルされた配列数が増えるものの，アセンブルされた配列の平均長やN50は途中でサチることが確認されている．この論文の結論としてはデータ数は20M〜30M付近で十分だよねという感じらしい．</p>

<h4>De Bruijn Graphの複雑性</h4>

<p>シーケンスエラーやヘテロ接合箇所などの僅かな配列の違いも異なる配列として出力</p>

<h4>カバレッジの変動</h4>

<p>単一遺伝子内でNGSのショートリードのカバレッジに差があると，複数本の細切れの配列になってしまうことがある</p>

<h4>アイソフォーム単位で発現解析するか遺伝子単位で発現解析するか？</h4>

<ul>
<li>アイソフォーム単位

<ul>
<li>扱う配列が多くなるので大変</li>
<li>発現量解析のときに複数箇所にマッピングされる配列が生じるので発現量推定が大変</li>
<li>全ての転写物に関してアイソフォームがあるわけではない</li>
</ul>
</li>
<li>遺伝子単位

<ul>
<li>スプライシングなどを無視することになる</li>
<li>どうやって複数の転写物を幾つかの遺伝子にまとめるのか？</li>
</ul>
</li>
</ul>


<h4>どうやってアセンブルされた転写物を遺伝子単位にクラスタリングするのか</h4>

<p>アセンブルされた転写物のクラスタリングにおいて決定打は無いものの，幾つか方法はある（配列で共通している箇所を見つけてクラスタリングするとか）</p>

<h4>クラスタリングする際に使える情報</h4>

<ul>
<li>アセンブルする際に出力されるlocus/componentの情報</li>
<li>CD-HITやBlastclustなどの配列相同性によるクラスタリングツール</li>
</ul>


<h4>クラスタリングにはTP・TN・FP・FNを数えて適合率Precisionや再現率Recallを見る</h4>

<p>どうやらTrinityのクラスタリングは良くて，OasesとCD-HIT-ESTの組み合わせは良くないらしい（Trinity&#8217;s clusteringはRSEMのこと？）</p>

<ul>
<li>CD-HIT-ESTは適合率は高いが再現率は低い

<ul>
<li>配列情報しか使用しないので精度が低い</li>
</ul>
</li>
<li>考えられる他の方法

<ul>
<li>発現量が低い領域は重みを軽くしたい</li>
<li>サンプル間で発現量の違う配列は区別したい</li>
<li>ペアエンドリードを考慮したい</li>
</ul>
</li>
</ul>


<h4>(クラスタリングの具体的な部分は省略)</h4>

<h4>どのようにしてリード数から発現量に変換するのか</h4>

<ul>
<li>TrinityやOasesが推奨する方法

<ul>
<li>アセンブルされた配列に対してマッピング（複数箇所にマップされてもいい）</li>
<li>複数箇所にマップされた配列も考慮して発現量を求めるプログラムを使う(RSEMとか)</li>
</ul>
</li>
</ul>


<p>実際に行われている方法としてよくあるのは，一番長くアセンブルされた配列に代表させてマッピングして発現量を推定するというもの</p>

<h4>まとめ</h4>

<ul>
<li><strong>Q1. なんでそんなにアセンブルされた転写物が出てくるの？</strong>

<ul>
<li>既にアノテーションされている転写物よりも多くアセンブルされるから</li>
<li>de novoトランスクリプトームアセンブリはそもそも難しいから（完全長のアセンブリは目指しているのだけれども）</li>
<li>インタージェニックやノンコーティングの転写物も多くでてくるから</li>
</ul>
</li>
<li><strong>Q2. アイソフォーム単位か遺伝子単位か</strong>

<ul>
<li>遺伝子単位の方がアイソフォーム単位より良さそう</li>
</ul>
</li>
<li><strong>Q3. どうやってアセンブルされた転写物から遺伝子にクラスタリングするか</strong>

<ul>
<li>Trinityのクラスタリングは良くて，OasesとCD-HIT-ESTの組み合わせは良くない</li>
</ul>
</li>
<li><strong>Q4. どのようにしてリード数から発現量に変換するのか</strong>

<ul>
<li>3つの異なる方法で検証したが似た結果を示した</li>
<li>正確なクラスタリングによる結果を得るほうが他のパイプラインを使って発現量差異を見るほうがインパクトがある（と主張している）</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「知の逆転」読了]]></title>
    <link href="http://yagays.github.io/blog/2013/06/02/review-interview-and-intelligence/"/>
    <updated>2013-06-02T11:20:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/02/review-interview-and-intelligence</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4140883952" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>すごい人たちに自分のことについて話して貰いたいという欲求は，ある意味下世話な心理でありながら，インタビューという対話の一つの特徴でもある．それが現在も第一線で活躍し続ける研究者であれば，なおさら気になるものだ．本書「知の逆転」は，著名な学者6人のインタビューをまとめた本であり，執筆した本や今までの研究内容と絡めながら現在の主張を分かりやすく簡潔に追いかける内容となっている．聞き手が日本人であることも影響してか，所々に日本に関連した意見も見られる．彼らがどのように世界を捉え，そして日本を見ているのかを垣間見ることができるちょっと変わった面白い本でもある．</p>

<p>本書でインタビューに応じている人物は，</p>

<ul>
<li>「<a href="http://www.amazon.co.jp/gp/product/4794218788/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4794218788&linkCode=as2&tag=yagays-22">銃・病原菌・鉄</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4794218788" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ジャレド・ダイアモンド</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/400600253X/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=400600253X&linkCode=as2&tag=yagays-22">生成文法の企て</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=400600253X" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ノーム・チョムスキー</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/415050251X/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=415050251X&linkCode=as2&tag=yagays-22">火星の人類学者</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=415050251X" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>オリバー・サックス</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/4782800541/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4782800541&linkCode=as2&tag=yagays-22">心の社会</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4782800541" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>マービン・ミンスキー</strong></li>
<li><a href="http://www.akamai.com/">Akamai</a>創設者でCEOの<strong>トム・レイトン</strong></li>
<li>「<a href="http://www.amazon.co.jp/gp/product/4062577925/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4062577925&linkCode=as2&tag=yagays-22">二重らせん</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4062577925" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」の<strong>ジェームズ・ワトソン</strong></li>
</ul>


<p>と，知っている人ならばこちらが萎縮してしまうなほどの豪華な面々だ．それぞれが自分の研究分野を開拓し，その後の著作や起業，数々の科学への貢献を経て一般にも認識されうる人物ばかりである．その彼らがインタビューを通して語るのは，もちろん今まで積み上げてきた研究成果でありながら，同時にこれからの未来を予想し「今現在」の世界に向けた提言でもある．</p>

<p>本書は，そのような彼らを知らない人にピッタリの入門書でもあると同時に，たとえ著作を読んだことのある人にとっても十分に興味深い内容となっている．とにかく上で述べた名前に心当たりがあるなら，本書はまさにオススメだ．自分から見てもかなりミーハーな精神だとは思うが，それに足る並びではないだろうか．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[段落の冒頭に強い接続語を使ってはいけない…?]]></title>
    <link href="http://yagays.github.io/blog/2013/06/01/paragraph-heading/"/>
    <updated>2013-06-01T10:57:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/06/01/paragraph-heading</id>
    <content type="html"><![CDATA[<p>ここ1週間ほどご無沙汰だったので，最近考えているをちょっと書きだしてみる．</p>

<p>最近は同じ研究室の院生の学振書類を手伝ったり，大学院入学希望者の小論文にアドバイスしたりと，人の書いた文書にひたすら赤を入れる日々だったのだが，やはり添削というのは難しい．まず第一には文章を書いた人間の意図を最大限汲み取り，一方では段落や文章の流れを整え単語の使い方を統一するためにバッサバッサとメスを入れ，原形をとどめつつも改善策を提示していかなければならない．ハッキリと論理的に指摘できる部分もあれば，「この言葉の使い方はなんかダサい」みたいな主観的な意見が入ることもある．個人的には徹底して型にはめて形式化された文章にすべきだというスタンスなのだが，逆に書き手の意図さえ伝われば，たとえ文章が全部ひとつなぎになっていようが口語的口調であったり文学的口調になっていようがそれでもいいという人も中にはいるわけで，あまり押し付けがましいのも良くないかなと思う．そして何より，添削しているお前の書く文章はどうなんだと言い返されそうで，自分のやっていることが本当に正しいのかどうか怪しいのが一番怖い．</p>

<p>まあこんなコトを思いつつも，やってくれと言われれば無慈悲に赤を入れるわけだけれども，たまたま見つけた修論の指導を目的とした配布資料にナルホドと思いつつも少し引っかかることが書いてあったので，ここで紹介する．</p>

<p><a href="http://www-utheal.phys.s.u-tokyo.ac.jp/~maxima/NetEd/To_M2/To_M2.pdf">http://www-utheal.phys.s.u-tokyo.ac.jp/~maxima/NetEd/To_M2/To_M2.pdf</a></p>

<p>この中の「2.2 段落（paragraph）」は非常によくまとまっている．英語では特にパラグラフ・リーディングなどで紹介されることが多い段落の構成法は，文章を書く上で何より意識しなければいけない「決まり事」だ．私も文章をあらかた書いたあとに形を整える作業をする際には，このような体裁に特に気をつけている．</p>

<p>この資料では段落を作る上でやって良いこと悪いことといった約束がいくつか紹介されており，それらは納得のいくものばかりなのだが，その中で一つ気になる項目があった．それは「段落の冒頭に強い接続語を使ってはいけない」という部分で，前後の文章を強く結ぶ言葉は段落の冒頭に使うにはふさわしくないということらしい．これに関しては確かに納得のいく部分もある反面，本当に使ってはいけないと断言してしまっていいのだろうかという感覚もある．特に逆接は禁止するほど悪い方法ではないと個人的に思っていて，前の話題にかぶせる形で議論の対象を変えるのに便利に使っている節がある．おそらくこの使い方は段落をひとまとまりとしてみた時の列挙に近い感覚なので，それは確かに段落に分けるべきではないかもしれない．しかしながら，あまりに膨らんだ内容を無理矢理一つに詰め込むのも良くないので，何かしらの方法で分けるべきだとは思う．やはりそこは，逆接を使わずにいきなり新しい話題でスタートしつつ，ところどころに前の段落との関連をいれていくべきなのだろうか．その方法もできなくはないけど，別に逆接を使ってもいいんじゃない？と思ってしまう．具体例が無い状態でアレコレ言うのは生産性が無いというのは分かっていても，なんとなく気になっている．</p>

<p>以上のようなことをこの1週間の間ぼやーっと考えていたわけだけれども，いまだに結論は出ていない．型にはまった文章を書くというのは，読み手が想定する文章を提示することであって，それは論理構造や正確さ以上に読みやすさという重要な役割を果たしていると思っているのだが，読みやすさの感覚は人それぞれなので，なかなかに統一が難しい．</p>

<br/>


<p>（余談）それにしても，ここの文章はどういうアレで書けばいいのか未だによく分からない．表現の境界線を行ったり来たりという感じで，まあ実験場のようなイメージなので，ここで下手糞な文章を書いてても勘弁して欲しい…．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["The Elements of Statistical Learning"のpdfの余白を削る]]></title>
    <link href="http://yagays.github.io/blog/2013/05/22/esl-pdf-trimming/"/>
    <updated>2013-05-22T09:22:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/22/esl-pdf-trimming</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=0387848576" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>機械学習系の教科書としてはそこそこの知名度のある「The Elements of Statistical Learning」（通称ESLまたはHastie）は，全ページのPDFが公式で配布されている．</p>

<p><a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning: data mining, inference, and prediction. 2nd Edition.</a></p>

<p>これは輪講に使えそうだということで色々と準備をしているのだが，このpdfには一つ気になるところがあって，それはページの余白が大きいということだ．右上にトンボの一部と「Printer: Opaque this」という文字が書かれているように，おそらく印刷所に出す前のpdfをそのまま配布しているらしい．pdfが配布されているだけ有難いというものではあるものの，このままでは少し読みづらい．ということで，余白を良い感じに自動で削れるツールを探して，文字だけの部分を抜き出してみた．</p>

<h3>brissで余白をトリミングする</h3>

<p>今回は「briss」というJavaアプリケーションを使ってみる．</p>

<p><a href="http://appdrill.net/61580/briss.html">[Mac] 自炊に！PDFのページを重ねて一発で余白を切り取る「briss」 « Appdrill</a></p>

<p>使い方は上のリンクを参考にしていただくとして，pdfを読み込んだ画面が以下のようになる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/briss1.png" alt="" /></p>

<p>これは1ページ目だけを除いた残りのページを全て重ねあわせたたもので，右ページ中央に表示されているグチャッとした部分がpdfのテキスト部分となる．水色の透過の部分がトリミング後に残る箇所を表しており，読み込んだ際に自動で設定される．左上と右下の四角をドラッグすると範囲を手動で選択できるが，下手にやると右ページと左ページでpdfサイズがズレることがあるので，今回のESLのpdfの場合は自動で設定されたものをそのまま使った方がいいだろう．</p>

<p>トリミングを実行して出力されたpdfを開くと，このような感じになる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/briss2.png" alt="" /></p>

<p>今回の場合は本当に文字ぎりぎりという感じだが，iPadで眺めたり印刷する際にはちょうどくらいだ．これでストレス無くESLを読む事ができる．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[treeを使ってディレクトリ構造をターミナル上で可視化する]]></title>
    <link href="http://yagays.github.io/blog/2013/05/21/tree/"/>
    <updated>2013-05-21T14:30:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/21/tree</id>
    <content type="html"><![CDATA[<p>最近は色々と忙しくネタが無いわけではないが，書けるだけの内容にまとまりきれていないのが現状．というわけで今回もLinxuの環境周りの話ということで，ディレクトリ構造をターミナル上で可視化するtreeというコマンドについて．ああ，これ以上何も言うことがない…．</p>

<p><a href="http://mama.indstate.edu/users/ice/tree/">The Tree Command for Linux Homepage</a></p>

<h3>インストール</h3>

<p>ソースを取ってきてコンパイルする．以上．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget ftp://mama.indstate.edu/linux/tree/tree-1.6.0.tgz
</span><span class='line'>$ tar zxvf lftp-4.4.6.tar.gz
</span><span class='line'>$ cd tree-1.6.0
</span><span class='line'>$ make</span></code></pre></td></tr></table></div></figure>


<p>あとは，treeをパスの通ったところに移動させれば完了．以上．</p>

<h3>使い方</h3>

<p>何もオプションを指定しないと，以下のようにファイルとディレクトリが表示される．以上．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ tree
</span><span class='line'>.
</span><span class='line'>├── CHANGES
</span><span class='line'>├── INSTALL
</span><span class='line'>├── LICENSE
</span><span class='line'>├── Makefile
</span><span class='line'>├── README
</span><span class='line'>├── TODO
</span><span class='line'>├── color.c
</span><span class='line'>├── color.o
</span><span class='line'>├── doc
</span><span class='line'>│   ├── tree.1
</span><span class='line'>│   ├── tree.1.fr
</span><span class='line'>│   └── xml.dtd
</span><span class='line'>├── hash.c
</span><span class='line'>├── hash.o
</span><span class='line'>├── html.c
</span><span class='line'>├── html.o
</span><span class='line'>├── strverscmp.c
</span><span class='line'>├── tree
</span><span class='line'>├── tree.c
</span><span class='line'>├── tree.h
</span><span class='line'>├── tree.o
</span><span class='line'>├── unix.c
</span><span class='line'>├── unix.o
</span><span class='line'>├── xml.c
</span><span class='line'>└── xml.o
</span><span class='line'>
</span><span class='line'>1 directory, 24 files</span></code></pre></td></tr></table></div></figure>


<p>-dオプションをつけると，ディレクトリだけを表示することができる．詳しくはmanで．以上．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ tree -d
</span><span class='line'>.
</span><span class='line'>└── doc
</span><span class='line'>
</span><span class='line'>1 directory</span></code></pre></td></tr></table></div></figure>


<p>ちなみに，色も付きます（ディレクトリと実行ファイル）．以上．お疲れ様でした．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/tree.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[zaw.zshを使って移動したことのあるディレクトリに一発で飛ぶ]]></title>
    <link href="http://yagays.github.io/blog/2013/05/20/zaw-zsh/"/>
    <updated>2013-05-20T01:47:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/20/zaw-zsh</id>
    <content type="html"><![CDATA[<p>「zsh anything.el-like widget」通称zawは，Emacsのanything.elみたいなインターフェイスでzshを操作できるという拡張プラグインだ．本来は様々な操作に対応しているのだが，とりあえずディレクトリ操作だけでも恐ろしいほどに便利なので，anything.elを知らない人でも導入して損はないと思う．ここでは以下の記事を参考に，zaw.zshの設定をしてみた．</p>

<ul>
<li><a href="https://github.com/zsh-users/zaw">zsh-users/zaw · GitHub</a></li>
<li><a href="http://u7fa9.org/memo/HEAD/archives/2011-02/2011-02-22_1.rst">zsh でも anything.el っぽいの - memo</a></li>
<li><a href="http://shibayu36.hatenablog.com/entry/20120130/1327937835">zaw.zshで最近移動したディレクトリに移動する - $shibayu36->blog;</a></li>
</ul>


<h3>1. zawをダウンロードする</h3>

<p>今回は.zshというディレクトリを$HOME以下に作成して，そこでgit cloneを実行する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir .zsh
</span><span class='line'>$ cd .zsh
</span><span class='line'>$ git clone git://github.com/zsh-users/zaw.git</span></code></pre></td></tr></table></div></figure>


<h3>2. .zshrcを設定する</h3>

<p>以下の内容を.zshrcに書き込む．sourceでzaw.zshを読み込む部分は，先ほどダウンロードしたzawディレクトリを指定する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># zaw.zsh
</span><span class='line'># http://shibayu36.hatenablog.com/entry/20120130/1327937835
</span><span class='line'>autoload -Uz chpwd_recent_dirs cdr add-zsh-hook
</span><span class='line'>add-zsh-hook chpwd chpwd_recent_dirs
</span><span class='line'>zstyle ':chpwd:*' recent-dirs-max 5000
</span><span class='line'>zstyle ':chpwd:*' recent-dirs-default yes
</span><span class='line'>zstyle ':completion:*' recent-dirs-insert both
</span><span class='line'>
</span><span class='line'>source /home/yag_ays/.zsh/zaw/zaw.zsh
</span><span class='line'>zstyle ':filter-select' case-insensitive yes # 絞り込みをcase-insensitiveに
</span><span class='line'>bindkey '^@' zaw-cdr # zaw-cdrをbindkey</span></code></pre></td></tr></table></div></figure>


<h3>3. .zshrcを読み込む</h3>

<p>最後に.zshrcを再度読み込んでエラーがでなければ完了．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ source .zshrc</span></code></pre></td></tr></table></div></figure>


<h3>使い方</h3>

<p>C-@（Control-@）を押すと以下のように，直近で移動したディレクトリの一覧が表示される．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/zaw1.png" alt="" /></p>

<p>この状態でキーワードを入力すると，マッチしたディレクトリだけを選ぶこともできる．深い階層のディレクトリでも名前さえ覚えていれば一発で飛ぶことができるので，非常に便利だ．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/zaw2.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cpanmのセッティング]]></title>
    <link href="http://yagays.github.io/blog/2013/05/16/cpanm/"/>
    <updated>2013-05-16T12:19:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/16/cpanm</id>
    <content type="html"><![CDATA[<p>PerlでCPANを使うなら，cpanmが便利．ライブラリはすべてホームディレクトリ以下に展開されるので，管理者権限が無くても気軽に使用できる．</p>

<ul>
<li><a href="http://search.cpan.org/~miyagawa/App-cpanminus-1.6914/bin/cpanm">cpanm - search.cpan.org</a></li>
</ul>


<h3>パスの通ったディレクトリにインストールする</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd local/bin
</span><span class='line'>$ curl -LOk http://xrl.us/cpanm
</span><span class='line'>$ chmod +x cpanm</span></code></pre></td></tr></table></div></figure>


<h3>.zshrcに設定を書き込む</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export PERL_CPANM_OPT="--local-lib=~/.cpanm/"
</span><span class='line'>export PERL5LIB="$HOME/.cpanm/lib/perl5:$PERL5LIB"</span></code></pre></td></tr></table></div></figure>


<h3>例）ライブラリを入れる</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cpanm local::lib
</span><span class='line'>$ cpanm JSON
</span><span class='line'>$ cpanm Cairo</span></code></pre></td></tr></table></div></figure>


<h4>参考</h4>

<ul>
<li><a href="http://www.omakase.org/perl/cpanm.html">perlモジュールのinstallにcpanmを使う｜perl｜@OMAKASE</a></li>
<li><a href="http://g86.dbcls.jp/~yag/wordpress/archives/1400">Wolf Ears » cpanmでBioPerlを入れる</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[アセンブルの指標であるN50とNG50の違い]]></title>
    <link href="http://yagays.github.io/blog/2013/05/15/n50-ng50/"/>
    <updated>2013-05-15T09:42:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/15/n50-ng50</id>
    <content type="html"><![CDATA[<p>今回は配列をアセンブルするときの指標に使うN50とNG50について少しまとめてみようと思う．</p>

<h3>前置き</h3>

<p>アセンブリというのはシーケンサで得られる短い配列から元のゲノム配列を復元する作業のことで，例えるならば膨大な数のジグソーパズルを形を頼りに完成させるとか，シュレッダーに掛けられて短冊になった書類を元に戻す作業といえる．これだけ聞くと頑張ればできそうな気がするが，実際には使える情報はATGCの配列だけと非常に限られており，場所によっては同じ文字が延々と続く箇所があったり，時々文字が間違っていたりと，手作業では不可能に近いし何より計算機を使ったとしても非常に難しい．それに加えて，そもそも元あった状態である解答を誰も知らないので，結果が合っているかどうかも分からず，答え合わせ（評価）がしづらいということがある．</p>

<p>このアセンブリの評価に関しては，Assemblathonというゲノムアセンブラの精度を争うコンペティションで活発に議論されている．というのも，各研究室で開発されたアセンブラの性能に順位を付けるためという以前に，未知のゲノムに対して今までに誰もアセンブル結果の配列だけで評価してこなかったからだ．もう少し正確に言えば，今までのモデル生物のゲノム配列解読は，過去に実験で確かめられてきた膨大な知見のもとで大量の人材と資金を投入して一歩ずつ進められてきた研究であり，現在のスタイルであるNGSを使った非モデル生物のゲノム配列解読のアプローチとはほとんど別物だと言っても過言ではない．そういった経緯があり，現在のAssemblathonではこれからのゲノム配列解読の基準となるような評価手法について，コンペティションを通して試行錯誤が繰り返されている（この話題に関しては<a href="http://www.slideshare.net/kbradnam/assemblathon-2-talk">Assemblathon2のスライド</a>が詳しい）．</p>

<p>ちなみに，Assemblathonは現在，擬似データでアセンブルを競った第1回は終了し論文も出ており，実際の生物データを使った第2回が終了して論文が出るのを待つだけで（<a href="http://arxiv.org/abs/1301.5406">既にArXivに上がっている</a>），第3回も企画されている．</p>

<p>というわけで，ゲノムを読むといっても今までのようにはいかないし，新しい評価も考えつつやっていかないとねという話．その中でスタンダードな評価指標が今回紹介するN50とNG50になる．</p>

<p><img src="http://dl.dropboxusercontent.com/u/142306/b/n50.png" align="right"></p>

<h3>N50</h3>

<p>N50とは一言でいえば配列長の加重平均なのだが，それでは誰も理解してくれないのでもうちょっと噛み砕こう．簡単に言えば，配列を長い順に並べて上から順に足していった時に，全体の長さの半分に達した時の配列の長さ(単位はbp)のことをN50という．イメージだと右図のように，半分の面積になるときの配列の長さがN50となる．得られた配列の分布を見つつ中間くらいの長さを表しているので，長い配列が多いとN50は大きくなるし，逆に長い配列が少なく短い配列が大量にあるとN50は小さくなる．アセンブルの際には復元したいゲノムに少しでも近づけるよう長い配列がたくさん得られると嬉しいので，N50はアセンブルの結果の良し悪しを判断する指標となっている．</p>

<h3>NG50</h3>

<p>とは言うものの，長けりゃそれでいいのかという疑問から出てきたのがNG50という指標だ．アセンブルで得られた配列全体の長さの代わりに，推定されるゲノム配列の長さを使って配列長の平均を計算している．つまり，予想では100Mbpだと推定された生物のゲノムならば，配列を長い順に並べて上から順に足していって100Mbpに達したときの配列の長さをNG50としている．考え方としては，理想となるゲノム配列の長さに近づけるために，長い配列だけじゃなくてある程度短い配列も評価しようということだろう．また，アセンブラの性能を異なるゲノムサイズの生物間で比較する際にも，NG50を用いることで公平に判断することができる．ただし，ゲノムサイズに関しては実験的に求めるかK-merから推定する必要があるので，必ずしも正確かどうかは難しいところがある．</p>

<h3>まとめ</h3>

<p>以上で，ざっとN50とNG50についてまとめてみた．実はこの議論にも続きがあって，NG50だけでは不十分でNG1からNG99までを検討しないといけないという話もある．今回はそこまでは踏み込まないが，気になる人はAssemblathon2の評価手法に関するページを見ていただきたい（<a href="http://assemblathon.org/post/44431933387/assemblathon-2-basic-assembly-metrics">The Assemblathon • Assemblathon 2 basic assembly metrics</a>）．結論としては一概にどの指標がいいかを決めるのは非常に難しいということで，色々と試してみる必要がある．</p>

<h3>参考</h3>

<ul>
<li>(pdf) <a href="http://korflab.ucdavis.edu/datasets/Assemblathon/Assemblathon1/assemblathon_talk.pdf">http://korflab.ucdavis.edu/datasets/Assemblathon/Assemblathon1/assemblathon_talk.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Trinityのインストールとサンプルデータのアセンブル]]></title>
    <link href="http://yagays.github.io/blog/2013/05/14/trinity-install/"/>
    <updated>2013-05-14T21:25:00+09:00</updated>
    <id>http://yagays.github.io/blog/2013/05/14/trinity-install</id>
    <content type="html"><![CDATA[<p>久しぶりに<a href="http://trinityrnaseq.sourceforge.net/">Trinity</a>を触ろうと思ってインストールしたら，実行に<a href="http://bowtie-bio.sourceforge.net/index.shtml">Bowtie</a>が必要になっていた．de novo transcriptome assemblyの後の解析で使う程度かと思いきや，どうやらChrysalis（2段階目）で必要になるらしい．そのため，Bowtieにパスを通していないとTrinity.pl実行時に以下のようなエラーが出る．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Error, cannot find path to bowtie, which is now needed as part of Chrysalis' read scaffolding step at ../../Trinity.pl line 622.</span></code></pre></td></tr></table></div></figure>


<p>というわけで，TrinityのインストールをしつつBowtieも用意して，試しにソースコードに含まれているテストデータの実行をしてみる．</p>

<h3>1. Trinityのインストール</h3>

<p>SourceForgeからソースコードをダウンロードしてインストールする．解凍したディレクトリでmakeをすればよい．</p>

<p><a href="http://sourceforge.net/projects/trinityrnaseq/files/">Trinity RNA-Seq Assembly - Browse Files at SourceForge.net</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://downloads.sourceforge.net/project/trinityrnaseq/trinityrnaseq_r2013-02-25.tgz 
</span><span class='line'>$ tar zxvf trinityrnaseq_r2013-02-25.tgz 
</span><span class='line'>$ cd trinityrnaseq_r2013-02-25
</span><span class='line'>$ make</span></code></pre></td></tr></table></div></figure>


<h3>2. Bowtieのインストール</h3>

<p>こちらも同様にSourceForgeからダウンロードするが，コンパイル済みのバイナリが配布されているので，makeをせずにそのまま利用できる．OSごとに32bit版と64bit版が用意されているので，環境に合わせてダウンロードする．</p>

<p><a href="http://bowtie-bio.sourceforge.net/index.shtml">Bowtie: An ultrafast, memory-efficient short read aligner</a></p>

<p><a href="http://sourceforge.net/projects/bowtie-bio/files/bowtie/1.0.0/">Bowtie - Browse /bowtie/1.0.0 at SourceForge.net</a></p>

<h3>3. Bowtieのバイナリがあるディレクトリにパスを通す</h3>

<p>今回はとりあえずシェルの設定ファイルをいじらずに，コマンドでbowtieがあるディレクトリをパスに指定する．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export PATH=/Users/yag_ays/Downloads/bowtie-1.0.0:$PATH</span></code></pre></td></tr></table></div></figure>


<p>もし頻繁に使用するなら，bowtieのバイナリをパスが通っている場所に移動させたほうがいいだろう．</p>

<h3>サンプルデータのアセンブル</h3>

<p>ひと通り動くことを確認するために，ソースコードの中に含まれているサンプルデータの解析を動かしてみる．</p>

<p>今回実行するのはsample_data/test_Trinity_Assembly．ディレクトリ内にrunMe.shがあるので，単純にそれを実行すればよい．初回は配列ファイルの解凍が実行されるため，Trinityが実行されるまでに少し時間がかかる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sample_data/test_Trinity_Assembly
</span><span class='line'>$ ./runMe.sh</span></code></pre></td></tr></table></div></figure>


<p>もしきちんとパスを通せていれば，標準出力に以下のようなテキストが表示され，解析が始まる．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ../../Trinity.pl --seqType fq --JM 2G --left reads.left.fq --right reads.right.fq --SS_lib_type RF --CPU 4 --no_cleanup --monitoring --output test
</span><span class='line'>WARNING, --monitoring can only be used on linux. Turning it off.
</span><span class='line'>
</span><span class='line'>Current settings:
</span><span class='line'>core file size          (blocks, -c) 0
</span><span class='line'>data seg size           (kbytes, -d) unlimited
</span><span class='line'>file size               (blocks, -f) unlimited
</span><span class='line'>max locked memory       (kbytes, -l) unlimited
</span><span class='line'>max memory size         (kbytes, -m) unlimited
</span><span class='line'>open files                      (-n) 256
</span><span class='line'>pipe size            (512 bytes, -p) 1
</span><span class='line'>stack size              (kbytes, -s) 8192
</span><span class='line'>cpu time               (seconds, -t) unlimited
</span><span class='line'>max user processes              (-u) 709
</span><span class='line'>virtual memory          (kbytes, -v) unlimited
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Paired mode requires bowtie. Found bowtie at: /Users/yag_ays/Downloads/bowtie-1.0.0/bowtie
</span><span class='line'>
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
</feed>
