<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PRML | Wolfeyes Bioinformatics beta]]></title>
  <link href="http://yagays.github.io/blog/categories/prml/atom.xml" rel="self"/>
  <link href="http://yagays.github.io/"/>
  <updated>2013-06-01T11:15:26+09:00</updated>
  <id>http://yagays.github.io/</id>
  <author>
    <name><![CDATA[yag_ays]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning Advent Calendar 2012でギブスサンプリングの記事を書きました]]></title>
    <link href="http://yagays.github.io/blog/2012/12/02/qiita-ml-advent-calendar-day2/"/>
    <updated>2012-12-02T21:58:00+09:00</updated>
    <id>http://yagays.github.io/blog/2012/12/02/qiita-ml-advent-calendar-day2</id>
    <content type="html"><![CDATA[<ul>
<li><a href="http://qiita.com/advent-calendar/2012/machinelearning">Machine Learning Advent Calendar 2012 - Qiita</a></li>
<li><a href="http://qiita.com/items/5bde6addf228b1fe24e6">PRML 11章 二変量正規分布からのギブスサンプリング #PRML #R - Qiita</a></li>
</ul>


<p>今年も残すところあと1ヶ月ということで，毎年恒例になりつつあるAdvent Calendarが各所で盛り上がっているようです．私もQiitaで開催されているMachine Learning Advent Calendar 2012の2日目で，ギブスサンプリングに関する記事を書きました．こんな動画を撮って遊んでいます．もしよければ本記事の方も御覧ください．</p>

<iframe width="480" height="360" src="http://www.youtube.com/embed/ZaKwpVgmKTY" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「パターン認識と機械学習」1章 多項式曲線フィッティングをRで書いてみた]]></title>
    <link href="http://yagays.github.io/blog/2012/12/01/prml-1-takousiki/"/>
    <updated>2012-12-01T05:17:00+09:00</updated>
    <id>http://yagays.github.io/blog/2012/12/01/prml-1-takousiki</id>
    <content type="html"><![CDATA[<h3>事の発端</h3>

<blockquote class="twitter-tweet"><p>これこれ、これですよ / “PRML読書会第一回 - Seeking for my unique color.” <a href="http://t.co/qtGtiKuO" title="http://htn.to/w8bGen">htn.to/w8bGen</a></p>&mdash; Yokoyama T (@wakuteka) <a href="https://twitter.com/wakuteka/status/272995562582835201" data-datetime="2012-11-26T09:29:48+00:00">November 26, 2012</a></blockquote>


<script src="http://yagays.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>先日「<a href="http://d.hatena.ne.jp/syou6162/20080801/1217528370">PRML読書会第一回 - Seeking for my unique color.</a>」のソースコードが一部界隈で局地的に話題にあがり，やれ</p>

<ul>
<li>takousikiってなんだよロシア人かよ</li>
</ul>


<p>だとか</p>

<ul>
<li>evalが多すぎて引いた</li>
</ul>


<p>だのといった罵詈雑言が（主に僕の口から）飛び交っていたわけだけれども，さすがに言うだけではsyou6162先生に申し訳ないと感じ，改めて彼のコードをRで書きなおしてみることにした．</p>

<h3>多項式曲線フィッティングのコードを書き直す</h3>

<p>```r
f &lt;- function(x){sin(2<em>pi</em>x)}</p>

<p>takousiki &lt;- function(n){
  x &lt;- numeric(0)
  z &lt;- numeric(0)
  for(i in 1:n){</p>

<pre><code>x &lt;- cbind(x, seq(0,1,by=0.1)^i)
z &lt;- cbind(z, seq(0,1,by=0.01)^i)
</code></pre>

<p>  }
  data &lt;- data.frame(f(seq(0,1,by=0.1))+rnorm(11, mean=0, sd=0.5), x)</p>

<p>  colnames(data) &lt;- c("y",paste("x",as.character(1:n),sep=""))
  colnames(z) &lt;- paste("x",as.character(1:n),sep="")</p>

<p>  # predict
  fit &lt;- lm(y ~ ., data=data)</p>

<p>  plot(data[,2], data[,1], ylab="", ylim=c(-1.5,1.5), xlab="x", main=paste("M=",n))
  curve(f(x), add=T, lwd=2)
  lines(seq(0,1,by=0.01), predict(fit,data.frame(z)), col="red", type="l")
}
```</p>

<p>使い方としては，</p>

<p><code>r
takousiki(1)
takousiki(3)
takousiki(8)
</code></p>

<p>とすると，多項式で曲線フィッティングした結果がプロットされる．ただし切片だけの0次は書けない．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/prml_takousiki.png" alt="" /></p>

<p>サンプルデータの生成と推定した曲線の描写用のデータの生成を同時に行なっているため，コードがちょっと見にくいかもしれない．基本的に![x<sup>2</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=x%3Csup%3E2%3C%2Fsup%3E+)や![x<sup>3</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=x%3Csup%3E3%3C%2Fsup%3E+)などの多項式の各項の値をあらかじめデータフレームとして作っておいて，lm()関数で回帰するときのモデルを簡略化している．あと，関数名はオリジナルを踏襲しているので，あしからず．</p>

<h3>他の方法としては</h3>

<p>lm()関数などに渡すモデルは，as.formula()関数を使うことによって文字列から生成することができる．この方法もかなり簡単に使うことができるので，データフレームの列名を規則的に付けている場合などは使い勝手は良いと思われる．</p>

<p>```r</p>

<blockquote><p>n &lt;- 5
as.formula(paste("y~",paste("x",1:n,sep="",collapse="+")))
y ~ x1 + x2 + x3 + x4 + x5
```</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「今度こそわかる!? PRMLの学習の学習」に参加しました]]></title>
    <link href="http://yagays.github.io/blog/2012/10/12/prml-talksession/"/>
    <updated>2012-10-12T10:03:00+09:00</updated>
    <id>http://yagays.github.io/blog/2012/10/12/prml-talksession</id>
    <content type="html"><![CDATA[<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621061240" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621061224" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>昨日池袋ジュンク堂で開催されたPRML同人誌トークセッションに参加してました．講演者の皆様お疲れ様でした＆貴重なお話をありがとうございました．</p>

<ul>
<li><a href="http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk">http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk</a></li>
</ul>


<p>ということで，トークセッションの中で話題に上がった話を私見を交えつつ幾つか．全部の話題を網羅しているわけではないのでご了承下さい．</p>

<br />


<h3>「PRMLの学習」の2版が出た</h3>

<p>なんと「パターン認識と機械学習の学習」が早くも2版ということで，どこかで見覚えのある黄色い表紙．私はもう既に1版を買っているので2版を買おうかどうか悩んでいたら，トークセッションの受付で2版で新たに加わった内容が印刷された小冊子を頂いた．</p>

<h3>PRML翻訳は@shima__shimaさん自らが同僚や出版社に話を持ちかけて実現した</h3>

<p>翻訳に関しては，本の中では分担されているということしか分からなかったので，ちょっとした裏話．あとは，Wikiを立てて訳語の統一をされたということで，fittingやjoint distributionなどで議論が紛糾したという話も．</p>

<h3>モデルというのはモノの見方である</h3>

<p>機械学習全般の話でもよく出てくるが，今回のトークセッションでもやはり話題になったのはモデルに関する問題だった．機械学習では何らかの関数に近似したり分類したりと，未来を予測するためにはモデルを立ててパラメータをチューニングして…ということが必要になるのだが，じゃあどうやってモデルを組み上げていけばいいのか．何が良いモデルで何が悪いモデルなのか．適当にモデルを組んだら機械学習の結果が現実のデータに上手い具合に当てはまった/予想に反して全然当てはまらなかった場合には，どう解釈すればいいのか．モデルを選ぶ以前に特徴量を選ぶところにおいても，どんな工夫をすればいいのか．．．などなど，手法にかぎらず色々な部分で正当性や性能を評価する必要があり，理論が固まった現在でも人が介入する部分というのは多いという話．それを克服(?)したのが今話題のDeep Learningで，膨大なデータ量を元にして，特徴量の選択自体も機械学習の中に含めてしまおうという話とも繋がった．</p>

<h3>贋作を愛でる</h3>

<p>モデルの話に関連して，ノーフリーランチ定理やボックス氏の言葉などが話の中に出てきたが，そういった流れの中で統数研の樋口知之先生が仰られている「贋作」という単語が出てきて，個人的にはそのニュアンスが非常に腑に落ちる感じで印象に残った．最終的には現実問題を解決したいということが根本にあると思うのだが，機械学習なんかは理論などは置いておいて単純に動けばいいやという考えもあり，逆に何でも適用しようとすると論文を量産する機械になってしまったり，SOMみたいな万能性に溺れてしまうという話もあり，なかなか難しい部分ではある．</p>

<ul>
<li><a href="http://d.hatena.ne.jp/what_a_dude/20110902/p1">http://d.hatena.ne.jp/what_a_dude/20110902/p1</a></li>
<li><a href="http://www.ai-gakkai.or.jp/jsai/sig/dmsm/007/discussion.html">http://www.ai-gakkai.or.jp/jsai/sig/dmsm/007/discussion.html</a></li>
<li><a href="http://www.ai-gakkai.or.jp/jsai/activity/241_p162_165.pdf">http://www.ai-gakkai.or.jp/jsai/activity/241_p162_165.pdf</a> (pdf)</li>
</ul>


<h3>余談：PRMLの学習と実装に関する疑問について．</h3>

<p>実装の話はトークセッションではあまり出て来なかったのだが，帰り際の電車の中で実装に関する疑問をふと思いついて「さっき訊いときゃよかった」と思ったので，折角なので書いてみる．</p>

<p>プログラマーなどは，アルゴリズムを自分で実装しないと本当に理解したことにならない，なんてことがよく言われるけれども，そういう環境で育った技術系の人がPRMLを読み始めると「書いてあることをひと通りまんべんなく実装しないといけない」みたいな感じになって，取っ掛かりとしては非常に辛いんじゃないかなーという個人的な印象がある．</p>

<p>例えば，尤度関数でlogを取って対数尤度にするのは小さな確率の値を掛け算していくと非常に小さい値になって計算機で扱うには困るからlogを取って足し算にしているんだよ，という実装上の工夫は非常によくされる説明だけれども，じゃあ他の機械学習の理論においても，実装のことを考慮した勉強をしないといけないんじゃないかという感覚は今でもある．</p>

<p>そのあたり，トークセッションの中で話があったように，理論レベルの難しさと実装レベルの難しさは別物だし，今は良質の機械学習パッケージが充実しているから機械学習を使うだけなら自分で実装する必要性は必ずしも無いけれども，「加減」の問題として，どの程度やったらいいのかという疑問．まあ，現実の問題を解こうと思った時には，やはりその分野（自然言語処理であったりレコメンドであったり）の機械学習本を参考にしたほうが良いのだけれども．</p>

<p>そういえば@Shuyoさんが「数式をnumpyに落としこむコツ」という解説を書かれており，非常に分かりやすくていつも参考にしているのだが，そのあたり「プログラマのためのPRMLの学習」同人誌があるといいんじゃないかと思う．</p>

<ul>
<li><a href="http://d.hatena.ne.jp/n_shuyo/20111016/numpy">http://d.hatena.ne.jp/n_shuyo/20111016/numpy</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PRML復々習レーン#3で発表しました]]></title>
    <link href="http://yagays.github.io/blog/2012/07/18/re-revenge-prml-3/"/>
    <updated>2012-07-18T10:12:00+09:00</updated>
    <id>http://yagays.github.io/blog/2012/07/18/re-revenge-prml-3</id>
    <content type="html"><![CDATA[<p>「パターン認識と機械学習」勉強会の復々習レーン#3で，3章の頭を担当しました．資料はslideshareにアップロードしています．</p>

<div style="width:427px" id="__ss_13661580"> <strong style="display:block;margin:12px 0 4px"><a href="http://www.slideshare.net/yag_ays/re-revenge-chap031" title="Re revenge chap03-1" target="_blank">Re revenge chap03-1</a></strong> <iframe src="http://www.slideshare.net/slideshow/embed_code/13661580" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0" allowfullscreen></iframe> <div style="padding:5px 0 12px"> View more presentations from <a href="http://www.slideshare.net/yag_ays" target="_blank">yag ays</a> </div> </div>


<p>3章の頭は線形回帰モデルの導入部分でPRMLの中では簡単な部類のところなのですが，きちんと説明しようとすればするほど自分の理解がいかに曖昧なものだったか思い知らされるばかりで，非常に勉強になりました．特に「§3.1.最小二乗法の幾何学」に関しては参加者の皆様に頼りっきりという何とも情けない感じになってしまったのですが，懇切丁寧に教えていただいたおかげで何とか図の直感的理解くらいはできるようになりました．これに関しては，その場で指摘していただいたことと自分の解釈をこねくり回しつつある程度纏めたものを，スライドの最後に補足として付け加えてあります．とは言っても，本文に書かれている数式は全くといっていいほど追いきれていないので，「学習の学習」とにらめっこしつつ，もう少し理解を深めたいと思います．</p>

<p>次回もよろしくお願いします．</p>

<ul>
<li><a href="http://atnd.org/events/30181">http://atnd.org/events/30181</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「パターン認識と機械学習の学習」購入]]></title>
    <link href="http://yagays.github.io/blog/2012/07/07/review-prml-learning/"/>
    <updated>2012-07-07T08:43:00+09:00</updated>
    <id>http://yagays.github.io/blog/2012/07/07/review-prml-learning</id>
    <content type="html"><![CDATA[<p><img src="http://dl.dropbox.com/u/142306/b/prml_learning.png" alt="" /></p>

<ul>
<li><a href="http://prml.in/">http://prml.in/</a></li>
<li><a href="https://github.com/herumi/prml">https://github.com/herumi/prml</a></li>
</ul>


<p>「<a href="http://www.amazon.co.jp/gp/product/4621061224/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4621061224&linkCode=as2&tag=yagays-22">パターン認識と機械学習 上</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4621061224" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」や「<a href="http://www.amazon.co.jp/gp/product/4621061240/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4621061240&linkCode=as2&tag=yagays-22">パターン認識と機械学習 下 (ベイズ理論による統計的予測)</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4621061240" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
」などで知られるPRMLの副読本を入手．中身は殆ど公開されているものと同じということだが，書きおろしが少しあるということと，あとは記念ということで購入した．既に公開されているpdfについては印刷して至る所にメモを書きまくっているのだが，この本は大事に取っておこうと思う．</p>
]]></content>
  </entry>
  
</feed>
