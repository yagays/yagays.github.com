<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Reading | Wolfeyes Bioinformatics beta]]></title>
  <link href="http://yagays.github.com/blog/categories/reading/atom.xml" rel="self"/>
  <link href="http://yagays.github.com/"/>
  <updated>2012-11-23T22:57:31+09:00</updated>
  <id>http://yagays.github.com/</id>
  <author>
    <name><![CDATA[yag_ays]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：6章 マルコフ連鎖とランダム・ウォーク]]></title>
    <link href="http://yagays.github.com/blog/2012/11/22/imcmr-6-2/"/>
    <updated>2012-11-22T12:14:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/22/imcmr-6-2</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>練習問題 6.1 自己回帰モデルAR(1)が定常分布を持つことを実験的に示す</h3>

<p>本問題の設定の詳細は<a href="http://www.stat.ufl.edu/~casella/ShortCourse/MCMC-UseR.pdf">MCMC-UseR.pdf</a>のP.130を参照．この問題はいわゆるAR(1)というもの．</p>

<p>![X<sup>{(t+1)}</sup> = \rho X<sup>{(t)}+\epsilon_t</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=X%3Csup%3E%7B%28t%2B1%29%7D%3C%2Fsup%3E+%3D+%5Crho+X%3Csup%3E%7B%28t%29%7D%2B%5Cepsilon_t%3C%2Fsup%3E+)，![\epsilon_t \sim \mathcal{N}(0,1) ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_t+%5Csim+%5Cmathcal%7BN%7D%280%2C1%29+)で定義されるマルコフ連鎖において，系列![{X<sup>{(t)}}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%7BX%3Csup%3E%7B%28t%29%7D%7D%3C%2Fsup%3E+)の定常分布は![\mathcal{N}(0, \frac{\sigma<sup>2}{1-\rho<sup>2})</sup></sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BN%7D%280%2C+%5Cfrac%7B%5Csigma%3Csup%3E2%7D%7B1-%5Crho%3Csup%3E2%7D%29%3C%2Fsup%3E%3C%2Fsup%3E+)で表される．今回は![t=10<sup>4</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=t%3D10%3Csup%3E4%3C%2Fsup%3E+)までのシミュレーション結果をヒストグラムとしてプロットし，定常分布を赤い曲線で示した．</p>

<p>```r
rho &lt;- 0.9
x &lt;- rnorm(1)
for(t in 2:10<sup>4){</sup>
  x[t] &lt;- x[t-1]*rho + rnorm(1)
}</p>

<p>hist(x, nclass=150, freq=F)
curve(dnorm(x, mean=0, sd=(1/sqrt(1-rho<sup>2))),</sup> add=T, col="red")
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6_1.png" alt="" /></p>

<h3>練習問題 6.2 ランダム・ウォークが定常分布を持たないことを実験的に示す</h3>

<p>![X<sup>{(t+1)}=X<sup>{(t)}+\epsilon_t</sup></sup> ](http://chart.apis.google.com/chart?cht=tx&chl=X%3Csup%3E%7B%28t%2B1%29%7D%3DX%3Csup%3E%7B%28t%29%7D%2B%5Cepsilon_t%3C%2Fsup%3E%3C%2Fsup%3E+)で表されるランダム・ウォークが，シミュレーション回数を増やしても定常分布に収束しないことを実験的に確かめる．今回は初期値![X<sup>{(0)}=0</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=X%3Csup%3E%7B%280%29%7D%3D0%3C%2Fsup%3E+)として，![t=10<sup>4</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=t%3D10%3Csup%3E4%3C%2Fsup%3E+)と![t=10<sup>6</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=t%3D10%3Csup%3E6%3C%2Fsup%3E+)までマルコフ連鎖をシミュレートする．
この問題は練習問題6.1における![\rho = 1](http://chart.apis.google.com/chart?cht=tx&chl=%5Crho+%3D+1)の場合と同じになるので，上で書いたコードのパラメータを変更するだけで再現できる．ただ，今回は別の書き方としてcumsum()関数を用いてランダム・ウォークの系列を計算している．</p>

<p>```r
init &lt;- 0
x1 &lt;- cumsum(rnorm(10<sup>4-1))</sup>
x2 &lt;- cumsum(rnorm(10<sup>6-1))</sup></p>

<p>par(mfrow=c(2,2))
hist(x1, nclass=150, main="t=10<sup>4")</sup>
plot(1:10<sup>4,</sup> c(init,x1), type="l", xlab="t", main="t=10<sup>4")</sup>
hist(x2, nclass=150, main="t=10<sup>6")</sup>
plot(1:10<sup>6,</sup> c(init,x2), type="l", xlab="t", main="t=10<sup>6")</sup>
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6_2.png" alt="" /></p>

<p>シミュレーション回数を![t=10<sup>4</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=t%3D10%3Csup%3E4%3C%2Fsup%3E+)と![t=10<sup>6</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=t%3D10%3Csup%3E6%3C%2Fsup%3E+)としてランダム・ウォークを実行した結果が上図である．左図に系列のヒストグラム，右図に試行回数を横軸に取った系列をプロットしている．上のように図示すると，たとえ10<sup>6</sup> 回試行したとしても何らかの分布に収束しないことがわかる．</p>

<h3>雑感</h3>

<p>練習問題 6.1と練習問題6.2を比較すると，ランダム・ウォークにおいて先行する状態に![0 \leq \rho &lt; 1 ](http://chart.apis.google.com/chart?cht=tx&chl=0+%5Cleq+%5Crho+%26lt%3B+1+)のような値を掛け合わせるだけで定常分布を取るというのは，なんとも面白い結果となっている．</p>

<h3>補足：<a href="http://www.stat.ufl.edu/~casella/ShortCourse/MCMC-UseR.pdf">MCMC-UseR.pdf</a>のP.132</h3>

<ul>
<li>練習問題の![\rho ](http://chart.apis.google.com/chart?cht=tx&chl=%5Crho+)と以下図の![\theta ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+)は同じ</li>
<li>pdfにも書いてあるがスケール（x軸とy軸の描写範囲）に注意．いかに![\theta ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+)が効いているかがわかる</li>
<li>左上はマルコフ連鎖ではない</li>
</ul>


<p>```r
par(mfrow=c(2,2))
for(rho in c(0, 0.4, 0.8, 0.95)){
  x &lt;- rnorm(1)
  y &lt;- rnorm(1)
  for(t in 2:1000){</p>

<pre><code>x[t] &lt;- x[t-1]*rho + rnorm(1)
y[t] &lt;- y[t-1]*rho + rnorm(1)
</code></pre>

<p>  }
  plot(x,y, type="l", main=paste("theta=",rho))
}
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6_extra.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：6章 メトロポリス・ヘイスティング・アルゴリズム その1]]></title>
    <link href="http://yagays.github.com/blog/2012/11/18/imcmr-6-1/"/>
    <updated>2012-11-18T14:37:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/18/imcmr-6-1</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>例 6.1 メトロポリス・ヘイスティング・アルゴリズムでベータ分布の乱数を生成する</h3>

<p><a href="http://yagays.github.com/blog/2012/10/22/imcmr-2-2/">例2.7</a>では受理・棄却法を用いてベータ分布の乱数を生成したが，本問題ではメトロポリス・ヘイスティング・アルゴリズムを使ってベータ分布からサンプリングを行うことを目的とする．</p>

<p>```r
a &lt;- 2.7
b &lt;- 6.3
c &lt;- 2.669
Nsim &lt;- 5000
X &lt;- rep(runif(1), Nsim)</p>

<p>for(i in 2:Nsim){
  Y &lt;- runif(1)
  rho &lt;- dbeta(Y, a, b)/dbeta(X[i-1], a, b)
  X[i] &lt;- X[i-1] + (Y - X[i-1])*(runif(1)&lt;rho)
}</p>

<p>plot(1:Nsim, X, type="l", xlab="Iterations", main="Sequence X<sup>t")</sup>
plot(4500:4800, X[4500:4800], type="l", xlab="Iterations", main="Sequence X<sup>t</sup> for t = 4500 - 4800")
```</p>

<p>まず，以下の図は系列![{X<sup>{(t)}}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%7BX%3Csup%3E%7B%28t%29%7D%7D%3C%2Fsup%3E+)をプロットしたものである．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6-1_1.png" alt="" /></p>

<p>そして，その中からシミュレーション回数が4500〜4800回までの系列を抜き出したのが以下の図となっている．このように一部分を拡大して見てみると，系列を表す線が変化せずにx軸に並行に遷移している部分が見られる．これは![\rho(x<sup>{(t)},Y_t)</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Crho%28x%3Csup%3E%7B%28t%29%7D%2CY_t%29%3C%2Fsup%3E+)の確率で棄却されて値が変化していないことを表している．これは対応する![Y_t ](http://chart.apis.google.com/chart?cht=tx&chl=Y_t+)に依存し，マルコフ連鎖の性質から次の状態にも影響するため，値が変化しない状態が一定期間続くことがあるからである．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6-1_2.png" alt="" /></p>

<p>また，得られた系列がベータ分布の乱数となっているかを確かめるため，左側に今回のメトロポリス・ヘイスティング・アルゴリズムで作成した乱数，右側にRの組込み関数rbeta()で生成した乱数をヒストグラムで図示し，今回実験で設定したベータ分布![\mathcal{Be}(2.7,6.3) ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BBe%7D%282.7%2C6.3%29+)を赤の曲線で表した．</p>

<p><code>r
par(mfrow=c(1,2))
hist(X, nclass=100, freq=F, xlim=c(0,1.0), main="Metropolis-Hastings")
curve(dbeta(x,a,b), add=T, col="red", lwd=2)
hist(rbeta(Nsim,a,b), freq=F, nclass=100, xlim=c(0,1.0), main="Direct Generation")
curve(dbeta(x,a,b), add=T, col="red", lwd=2)
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6-1_3.png" alt="" /></p>

<p>この図を見ると，メトロポリス・ヘイスティング・アルゴリズムを使った場合でも，受理・棄却法の時と同じく乱数の生成に成功していることがわかる．</p>

<h3>例 6.2 メトロポリス・ヘイスティング・アルゴリズムでコーシー分布の乱数を生成する</h3>

<p>例6.1のベータ分布の乱数生成における提案分布は![\mathcal{U}_{0,1} ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BU%7D_%7B0%2C1%7D+)だったが，コーシー分布の乱数生成における提案分布には，</p>

<ul>
<li>標準正規分布：![\mathcal{N}(0,1) ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BN%7D%280%2C1%29+)</li>
<li>自由度0.5のスチューデントt分布：![\mathcal{T}_{1/2} ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BT%7D_%7B1%2F2%7D+)</li>
</ul>


<p>を使うことができる．このような提案分布は独立提案分布であり，マルコフ連鎖ではない．</p>

<p>なお，本問題ではこれらの分布を表す単語として候補分布と提案分布という2つの言葉が出てくるが，今回は表記として提案分布で統一する．</p>

<p>```r</p>

<h1>Standard Normal Distributionp</h1>

<p>Nsim &lt;- 10<sup>4</sup>
X &lt;- c(rt(1,1))
for(t in 2:Nsim){
  Y &lt;- rnorm(1)
  rho &lt;- min(1, dt(Y, 1)<em>dnorm(X[t-1]) / (dt(X[t-1],1)</em>dnorm(Y)))
  X[t] &lt;- X[t-1] + (Y - X[t-1])*(runif(1) &lt; rho)
}</p>

<h1>t-distribution (0.5 degree of freedom)</h1>

<p>X2 &lt;- c(rt(1,1))
for(t in 2:Nsim){
  Y &lt;- rt(1, 0.5)
  rho &lt;- min(1, dt(Y, 1)<em>dt(X2[t-1], 0.5) / (dt(X2[t-1],1)</em>dt(Y, 0.5)))
  X2[t] &lt;- X2[t-1] + (Y - X2[t-1])*(runif(1) &lt; rho)
}
```</p>

<p>まずはそれぞれの系列をプロットして特徴を見ていく．左側に提案分布を標準正規分布とした場合，右側に提案分布をスチューデントt分布とした場合で，上から順に系列のプロット，系列のヒストグラム，系列の自己相関プロットを図示したのが以下の図になっている．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6-2_1.png" alt="" /></p>

<p>まずは標準正規分布を使った場合からみていく．系列のプロットは例6.1と同じような形を示しているが，イテレーションが8500を過ぎたあたりで値が一定期間変化しない部分がある．これは乱数から生成する正規候補![Y_t ](http://chart.apis.google.com/chart?cht=tx&chl=Y_t+)が大きい（または小さい）値を取ってしまい，連鎖で長い間引きずることになったからである．この現象は，系列のヒストグラムの右端において値が少し大きくなっていることからも確認することができる．</p>

<p>一方，スチューデントt分布を使った場合においては，系列のプロットは0付近で集中しており，たまに正規候補![Y_t ](http://chart.apis.google.com/chart?cht=tx&chl=Y_t+)が非常に大きい値を取ることはあるものの，それが継続して続くわけではないことがわかる．系列のヒストグラムを見ても，標準正規分布の場合と比べて分布がなだらかでコーシー分布に当てはまっていることがわかる．ただし，実際には両端の外れ値を除外して系列のヒストグラムをプロットしているので，このヒストグラムの描写範囲から外れている値があることには注意が必要である．</p>

<br />


<p>次に，2つの提案分布におけるメトロポリス・ヘイスティング・アルゴリズムによって生成したコーシー乱数の累積収束プロット，すなわち乱数の平均値がどのように収束していくかを図示する．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp6_6-2_2.png" alt="" /></p>

<p>黒線が提案分布に標準正規分布を使った場合，黄色の線が提案分布にスチューデントt分布を使った場合となっている．今回シミュレートしているコーシー分布から求めた正確な値は0.896なので，スチューデントt分布はイテレーション回数を重ねるごとに真の値に収束していくことがわかる．一方で標準正規分布においては，今回のイテレーション回数(N=5000)では真の値に収束していないようにみえる．しかし，平均値の傾向を見ると値が徐々に上昇しては一気に下がり...といった傾向が見られ，理論的にも標準正規分布を提案分布として用いた場合でも真の値に収束することがわかっている．なので，今回の場合はイテレーション回数が少なすぎるために，収束の様子が見られない．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：5章 モンテカルロ最適化 確率的勾配法]]></title>
    <link href="http://yagays.github.com/blog/2012/11/11/imcmr-5-4/"/>
    <updated>2012-11-11T06:17:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/11/imcmr-5-4</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>内容：確率的勾配法(Stochastic Gradient Methods)で関数の最大値を求める</h3>

<p>目的関数からの直接的なシミュレーションが難しい場合，関数の表面をローカルになぞって最大値を探索するという手法が使える．探索に用いる系列は以下のように表す事ができる．</p>

<p>![\theta_{j+1} = \theta_j +  \epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta_%7Bj%2B1%7D+%3D+%5Ctheta_j+%2B++%5Cepsilon_j+)</p>

<p>これは現在のステップに摂動項![\epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_j+)を加えて次のステップとすることを表している．つまりこれはマルコフ連鎖となるのだが，今回はそれほどマルコフ性は重要にはならない．</p>

<p>概念的に目的関数の分布を山の斜面と考えるならば，現在の位置からちょっと動いて…を繰り返して山を登っていくような動きをする．ただし，単純に傾斜の上の方に登っていくのではなく，どちらかに進むかはランダムなので，たまに斜面を下ったりもする．その試行を繰り返していくことで，山の頂上を目指すような方法となっている．</p>

<h4>有限差分法</h4>

<p>有限差分法では，関数の勾配は</p>

<p>![\nabla h(\theta_j) \approx \frac{h(\theta_j+\beta_j\zeta_j)-h(\theta_j-\beta_j\zeta_j)}{2\beta_j}\zeta_j = \frac{\Delta h(\theta_j,\beta_j\zeta_j)}{2\beta_j}\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cnabla+h%28%5Ctheta_j%29+%5Capprox+%5Cfrac%7Bh%28%5Ctheta_j%2B%5Cbeta_j%5Czeta_j%29-h%28%5Ctheta_j-%5Cbeta_j%5Czeta_j%29%7D%7B2%5Cbeta_j%7D%5Czeta_j+%3D+%5Cfrac%7B%5CDelta+h%28%5Ctheta_j%2C%5Cbeta_j%5Czeta_j%29%7D%7B2%5Cbeta_j%7D%5Czeta_j+)</p>

<p>とし，摂動項![\epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_j+)を組み込んだ系列の更新は以のようになる．</p>

<p>![\theta_{j+1} = \theta_j +  \frac{\alpha_j}{2\beta_j}\Delta h(\theta_j,\beta_j\zeta_j)\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta_%7Bj%2B1%7D+%3D+%5Ctheta_j+%2B++%5Cfrac%7B%5Calpha_j%7D%7B2%5Cbeta_j%7D%5CDelta+h%28%5Ctheta_j%2C%5Cbeta_j%5Czeta_j%29%5Czeta_j+)</p>

<h3>例 5.7 有限差分法による関数の最大化において，複数の勾配パラメータがどのように収束に影響するかを調べる</h3>

<p><a href="http://yagays.github.com/blog/2012/11/06/imcmr-5-3/">例5.6</a>の続き．有限差分法を用いて以下の式を最小化することを目的とする．</p>

<p>![h(x,y)=(x\sin(20y)+y\sin(20x))<sup>2\cosh(\sin(10x)x)+</sup> (x\cos(10y)-y\sin(10x))<sup>2\cosh(\cos(20y)y)</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=h%28x%2Cy%29%3D%28x%5Csin%2820y%29%2By%5Csin%2820x%29%29%3Csup%3E2%5Ccosh%28%5Csin%2810x%29x%29%2B%3C%2Fsup%3E+%28x%5Ccos%2810y%29-y%5Csin%2810x%29%29%3Csup%3E2%5Ccosh%28%5Ccos%2820y%29y%29%3C%2Fsup%3E+)</p>

<p>今回は最大化ではなく最小化することが目的なので，この関数をマイナスにする必要がある．</p>

<p>```r
h &lt;- function(x){
  -(x[1]<em>sin(20</em>x[2]) + x[2]<em>sin(20</em>x[1]))<sup>2</sup> * cosh(sin(10<em>x[1])</em>x[1]) -</p>

<pre><code>(x[1]*cos(10*x[2]) - x[2]*sin(10*x[1]))^2 * cosh(cos(20*x[2])*x[2])
</code></pre>

<p>}
```</p>

<h4>アルゴリズム</h4>

<ul>
<li>勾配の値が10<sup>-5</sup> になるまで以下を繰り返す

<ol>
<li>![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)を生成する</li>
<li>![\theta_{j} ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta_%7Bj%7D+)を使って勾配![\frac{\alpha_j}{2\beta_j}\Delta h(\theta_j,\beta_j\zeta_j)\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B%5Calpha_j%7D%7B2%5Cbeta_j%7D%5CDelta+h%28%5Ctheta_j%2C%5Cbeta_j%5Czeta_j%29%5Czeta_j+)を生成する</li>
<li>![\theta<em>{j}+\epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Cem%3E%7Bj%7D%2B%5Cepsilon_j+)を![\theta</em>{j+1} ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3C%2Fem%3E%7Bj%2B1%7D+)とする</li>
<li>![\alpha_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Calpha_j+)と![\beta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_j+)を更新する</li>
</ol>
</li>
</ul>


<p>なお，実際のソースコードでは勾配の値が発散しないように，scaleが1以上になった場合には内部でループをして値を再計算するようにしている．</p>

<br />




<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4274068307" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h4>![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)の生成方法と有限差分法における役割</h4>

<p>さて，ここで有限差分法のポイントとなる![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)に関して，実際にどうやって値を生成するのか，そしてこの値がどういう意味を持っているかについて考えてみる．</p>

<p>まずは![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)の生成方法について．本文中では<strong>「![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)は単位球![\| \zeta_j \| = 1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5C%7C+%5Czeta_j+%5C%7C+%3D+1+)上に一様に分布しています」</strong>とあるだけで，コードの方を参照してみてもzeta / sqrt(t(zeta) %*% zeta)としか書かれていない．ではこれは一体何をしているのかというと，一言で言うと「球面上の一様分布から乱数を生成する」，つまり今回の場合は単位円の円周の上からある一点を取ってきて![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)の値にするということをしている．</p>

<p>このような球面上の一様分布に関しては「<a href="http://www.amazon.co.jp/gp/product/4274068307/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4274068307&linkCode=as2&tag=yagays-22">Ｒによる計算機統計学</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4274068307" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」のP.96に詳しく書かれており，この本ではD次元の球面上の一様乱数を生成する方法についてrunif.sphere()関数を定義している．今回の場合はxとyの2次元について考えているので，「<a href="http://www.amazon.co.jp/gp/product/4274068307/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=4274068307&linkCode=as2&tag=yagays-22">Ｒによる計算機統計学</a><img src="http://www.assoc-amazon.jp/e/ir?t=yagays-22&l=as2&o=9&a=4274068307" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />」に倣ってrunif.sphere(1,2)とすれば，![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)と同様の値が得られる．</p>

<p>```r</p>

<h1>Ｒによる計算機統計学 P.87より</h1>

<h1>http://personal.bgsu.edu/~mrizzo/SCR/SCRch3.R</h1>

<p>runif.sphere &lt;- function(n, d) {</p>

<pre><code># return a random sample uniformly distributed
# on the unit sphere in R ^d
M &lt;- matrix(rnorm(n*d), nrow = n, ncol = d)
L &lt;- apply(M, MARGIN = 1,
           FUN = function(x){sqrt(sum(x*x))})
D &lt;- diag(1 / L)
U &lt;- D %*% M
U
</code></pre>

<p>}
```</p>

<p>次に，有限差分法における![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)の役割について考えてみる．</p>

<p>そもそも，確率的勾配法はローカルに関数の表面をなぞるように探索して，値の大きいポイントを探す確率的なアプローチだった．概念的には，<a href="http://yagays.github.com/blog/2012/11/06/imcmr-5-3/">前回3Dプロットをした</a>ような山みたいになっている関数の上をあちこち動いて頂上を探し出すということだが，あちこち動きまわるためには，歩く「方向」と「距離」を考えなければならない．今回の有限差分法における勾配は![\epsilon_j = \frac{\alpha_j}{2\beta_j}\Delta h(\theta_j,\beta_j\zeta_j)\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_j+%3D+%5Cfrac%7B%5Calpha_j%7D%7B2%5Cbeta_j%7D%5CDelta+h%28%5Ctheta_j%2C%5Cbeta_j%5Czeta_j%29%5Czeta_j+)で表されていたが，その「方向」が![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)，「距離」が![\frac{\alpha_j}{2\beta_j}\Delta h(\theta_j,\beta_j\zeta_j) ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B%5Calpha_j%7D%7B2%5Cbeta_j%7D%5CDelta+h%28%5Ctheta_j%2C%5Cbeta_j%5Czeta_j%29+)に対応している．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-7_3.png" alt="" /></p>

<p>では実際に![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)と勾配![\epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_j+)を図示して，「方向」と「距離」の意味を確かめてみよう．以下の図は，![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)の取りうる単位円と，赤い矢印![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)，黒い矢印![\epsilon_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cepsilon_j+)を示した図になっている（![j=1 ](http://chart.apis.google.com/chart?cht=tx&chl=j%3D1+)）．この図を見ると，赤い矢印で方向が示され，黒い矢印でどれくらいの距離を進むかが見て取れる．![\zeta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Czeta_j+)は乱数で生成された値なので，本書の「そのつど方向をランダムに選択する」という勾配の説明の通り，単位円の円周上からどこか一点を選択して進む方向を決めている．</p>

<p>といったように，この有限差分法では関数の山の傾斜を考えて進む方向を決めるのではなく，何も考えずに動いてみるといったアプローチとなっている．ただし，距離に関しては別で，関数を使って傾斜を見つつ調整しているので，その点は注意．</p>

<p><code>r
plot(runif.sphere(1000,2), xlim=c(-2,2), ylim=c(-2,2), cex=0.1)
arrows(0,0,grad[1],grad[2], lwd=2)             # 黒の矢印が勾配
arrows(0,0,zeta[1],zeta[2], lwd=2, col="red")  # 赤の矢印がゼータ
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-7_2.png" alt="" /></p>

<h4>今回実験を行う4つのシナリオ</h4>

<p>この有限差分法において，![\alpha_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Calpha_j+)と![\beta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_j+)の系列の更新式は任意に設定できる．今回は4つのシナリオを用意して実験を行った．それぞれのシナリオの更新式は以下の通り．</p>

<ul>
<li>シナリオ1：![1/\log(j+1) ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%5Clog%28j%2B1%29+)，![1/\log(j+1)<sup>{0.1}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%5Clog%28j%2B1%29%3Csup%3E%7B0.1%7D%3C%2Fsup%3E+)</li>
<li>シナリオ2：![1/100\log(j+1) ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F100%5Clog%28j%2B1%29+)，![1/\log(j+1)<sup>{0.1}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%5Clog%28j%2B1%29%3Csup%3E%7B0.1%7D%3C%2Fsup%3E+)</li>
<li>シナリオ3：![1/(j+1) ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%28j%2B1%29+)，![1/(j+1)<sup>{0.5}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%28j%2B1%29%3Csup%3E%7B0.5%7D%3C%2Fsup%3E+)</li>
<li>シナリオ4：![1/(j+1) ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%28j%2B1%29+)，![1/(j+1)<sup>{0.1}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=1%2F%28j%2B1%29%3Csup%3E%7B0.1%7D%3C%2Fsup%3E+)</li>
</ul>


<p>(※シナリオ1と2に関しては後述の「本問題の疑問点」を参考)</p>

<h4>例 5.7のソースコード</h4>

<p>前置きがだいぶ長くなってしまったが，本題のソースコードおよび4つのシナリオにおける確率的勾配法の探索過程を示した図は以下のようになる．</p>

<p>```r
h &lt;- function(x){
  -(x[1]<em>sin(20</em>x[2]) + x[2]<em>sin(20</em>x[1]))<sup>2</sup> * cosh(sin(10<em>x[1])</em>x[1]) -</p>

<pre><code>(x[1]*cos(10*x[2]) - x[2]*sin(10*x[1]))^2 * cosh(cos(20*x[2])*x[2])
</code></pre>

<p>}</p>

<p>start &lt;- c(0.65, 0.8)
theta &lt;- matrix(start, ncol=2)
dif &lt;- 1
iter &lt;- 1
alpha &lt;- 1
beta &lt;- 1</p>

<p>while(dif > 10<sup>-5){</sup>
  zeta &lt;- rnorm(2)
  zeta &lt;- zeta / sqrt(t(zeta) %<em>% zeta)
  grad &lt;- alpha[iter]</em>zeta * (h(theta[iter,]+beta[iter]*zeta) -</p>

<pre><code>h(theta[iter,]-beta[iter]*zeta)) / 2*beta[iter]
</code></pre>

<p>  scale <- sqrt(t(grad) %*% grad)
  while(scale > 1){</p>

<pre><code>zeta &lt;- rnorm(2)
zeta &lt;- zeta / sqrt(t(zeta) %*% zeta)
grad &lt;- alpha[iter]*zeta * (h(theta[iter,]+beta[iter]*zeta) - 
  h(theta[iter,]-beta[iter]*zeta)) / 2*beta[iter]
scale &lt;- sqrt(t(grad) %*% grad)
</code></pre>

<p>  }
  theta &lt;- rbind(theta, theta[iter,]+grad)
  dif &lt;- scale
  iter &lt;- iter + 1</p>

<h1>scenario1</h1>

<p>  alpha &lt;- cbind(alpha, 1/log(iter+1))
  beta &lt;- cbind(beta, 1/(log(iter+1))<sup>(0.1))</sup>
}</p>

<p>hcur &lt;- h(start)
hval &lt;- h(start)
x &lt;- seq(-1,1,le=435)
y &lt;- seq(-1,1,le=435)
z &lt;- matrix(0, nrow=435, ncol=435)
for (i in 1:435){
  for (j in 1:435){</p>

<pre><code>z[i,j] &lt;- h(c(x[i], y[j]))
</code></pre>

<p>  }
}</p>

<p>image(x, y, z, col=terrain.colors(150))
lines(theta, lwd=2)
points(theta[1,1], theta[1,2], col="gold", pch=19)
title(main="scenario 1")</p>

<p>```</p>

<p>シナリオ2,3,4の場合は，whileループの最後の![\alpha_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Calpha_j+)と![\beta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_j+)の更新式を以下の通りにする．</p>

<p>```r</p>

<h1>scenario2</h1>

<p>  alpha &lt;- cbind(alpha, 1/(100*log(iter+1)))
  beta &lt;- cbind(beta, 1/(log(iter+1))<sup>(0.1))</sup></p>

<h1>scenario3</h1>

<p>  alpha &lt;- cbind(alpha, 1/(iter+1))
  beta &lt;- cbind(beta, 1/sqrt(iter+1))</p>

<h1>scenario4</h1>

<p>  alpha &lt;- cbind(alpha, 1/(iter+1))
  beta &lt;- cbind(beta, 1/(iter+1)<sup>(0.1))</sup>
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-7_1.png" alt="" /></p>

<p>上の図の解釈だが，この4つのシナリオの場合において</p>

<ul>
<li>シナリオ1では，動きまわる距離が大きすぎて（![\alpha_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Calpha_j+)が収束するのに時間がかかって）なかなかアルゴリズムが停止しない</li>
<li>シナリオ2では，動きまわる距離が小さすぎて（![\alpha_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Calpha_j+)がすぐ収束してしまって）ローカルミニマムにはまっている</li>
<li>シナリオ3では，動きまわる距離はシナリオ2より改善しているが，最大値を見つけられていない</li>
<li>シナリオ4では，上手い具合に最大値を見つけることができている</li>
</ul>


<p>ことが分かる．</p>

<p>なお，この解釈は本書（P.156）に書かれているものとは多少異なっているということを注意していただきたい．それに関しては次の疑問点のところで述べる．</p>

<h3>本問題の疑問点</h3>

<p>さて，この問題を素直に解いてもP.157の図5.7のようにはいかない．色々と試してみた結果，どうやら<strong>本書におけるシナリオ1と2の![\beta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_j+)において，分母にかかっている0.1乗の表記がおかしい気がする．</strong>この0.1乗がlogの中の括弧にかかっていると考えて0.1*log()としてコードを組むと，値が発散しすぎてなかなか収束しなくなる．ということで，ここでは以下のように式を解釈して問題を解いた．正直なところ，これが正しいのかよくわかっていない．</p>

<ul>
<li>シナリオ1と2の![\beta_j ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_j+)：![\frac{1}{\log(j+1)<sup>{0.1}}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B1%7D%7B%5Clog%28j%2B1%29%3Csup%3E%7B0.1%7D%7D%3C%2Fsup%3E+)→![\frac{1}{(\log(j+1))<sup>{0.1}}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B1%7D%7B%28%5Clog%28j%2B1%29%29%3Csup%3E%7B0.1%7D%7D%3C%2Fsup%3E+)</li>
</ul>


<p>これにともなって，シナリオの図示や解釈にも少し違いが出てきている．そのため，今回は本書の解説（P.156）からは少し外れることになったが，自分がスクリプトを回して得た結果を解釈に用いた．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：5章 モンテカルロ最適化 確率的探索 その2]]></title>
    <link href="http://yagays.github.com/blog/2012/11/06/imcmr-5-3/"/>
    <updated>2012-11-06T11:25:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/06/imcmr-5-3</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>例 5.4 数値的最適化法と確率的探索を比較する</h3>

<p><a href="http://yagays.github.com/blog/2012/10/30/imcmr-5-1/">例5.1</a>の続き．前回は数値的最適化法によって![\theta ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+)の値を推定したが，今回は確率的探索による推定を行なって，両者の手法の比較を行う．問題としては，以下の対数尤度を最大化する．</p>

<p>![\log l(\theta| x_1,\ldots,x_n) = \sum_{i=1}<sup>{n}</sup> \log \frac{1}{1+(x_i-\theta)<sup>2}</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Clog+l%28%5Ctheta%7C+x_1%2C%5Cldots%2Cx_n%29+%3D+%5Csum_%7Bi%3D1%7D%3Csup%3E%7Bn%7D%3C%2Fsup%3E+%5Clog+%5Cfrac%7B1%7D%7B1%2B%28x_i-%5Ctheta%29%3Csup%3E2%7D%3C%2Fsup%3E+)</p>

<p>この式は![n \rightarrow \infty ](http://chart.apis.google.com/chart?cht=tx&chl=n+%5Crightarrow+%5Cinfty+)において![\theta<sup>*</sup> =0 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E%2A%3C%2Fsup%3E+%3D0+)になる．すなわちこれから求める推定値の答えは0ということになる．</p>

<h4>ソースコード</h4>

<p>以下のソースコードにおける変数の対応は以下のようになっている．</p>

<ul>
<li>数値的最適化法

<ul>
<li>trul：推定値</li>
<li>truv：対数尤度</li>
</ul>
</li>
<li>確率的探索

<ul>
<li>loc：推定値</li>
<li>maxv：対数尤度</li>
</ul>
</li>
</ul>


<p>なお，mcsmパッケージに付随するデモスクリプトでは，数値的最適化法における乱数を正規乱数としていたり対数尤度の関数が微妙に違っているが，今回は例5.1の解法と同様のコーシー乱数と対数尤度の式を用いている．</p>

<p>```r
ref &lt;- rcauchy(5001)
f &lt;- function(y){-sum(log(1+(x-y)<sup>2))}</sup></p>

<p>maxv &lt;- NULL
loc &lt;- NULL
truv &lt;- NULL
trul &lt;- NULL</p>

<p>for (i in 10:length(ref)){
  # 数値的最適化
  x &lt;- ref[1:i]
  tru &lt;- optimize(f,c(-5,5),maximum=T)
  trul &lt;- c(trul, tru$max)
  truv &lt;- c(truv, tru$ob)</p>

<p>  # 確率的探索
  prop &lt;- runif(10<sup>3,-5,5)</sup>
  vale &lt;- apply(as.matrix(prop),1,f)
  loc &lt;- c(loc, prop[order(-vale)[1]])
  maxv &lt;- c(maxv, max(vale))
}</p>

<p>par(mar=c(4,4,1,1), mfrow=c(2,1))
plot(trul, loc, cex=.5, pch=19, xlab=expression(theta<sup>0),</sup> ylab=expression(hat(theta)))
abline(a=0, b=1, col="grey")
plot(10:length(ref), (truv-maxv)/abs(truv), type="l", lwd=2, xlab="Sample size", ylab="Relative error", ylim=c(0,0.02))
```</p>

<p>まず，上のプロットは，数値的最適化法による推定値を横軸に，確率的探索による推定値を縦軸に取った図となっている．真の値が![\theta<sup>*</sup> =0 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E%2A%3C%2Fsup%3E+%3D0+)なので，当然ながら両者ともに推定値は(0,0)付近に集まっている．また，推定値が外れる場合では，どちらかの値が大きくなればもう片方も大きくなるといった具合に，そのパターンはグレーの線で示した![\theta<sup>0</sup> = \hat{\theta} ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E0%3C%2Fsup%3E+%3D+%5Chat%7B%5Ctheta%7D+)の線分の上に乗るような形となる．</p>

<p>次に，下のプロットは，横軸にサンプルサイズを取り，縦軸に推定値における対数尤度の差を取った図となっている．今回は相対的誤差なので，(数値的最適化法-確率的探索)/(数値的最適化法)といった形で値を求めている．相対的誤差が大きいということは，数値的最適化法と確率的探索それぞれ求めた対数尤度に差がある，すなわち![\theta ](http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+)に差があるということで，上のプロットにおけるグレーの線分から離れれば離れるほど相対的誤差も大きくなるということになる．なお，図示しているサンプルサイズは![n=10 ](http://chart.apis.google.com/chart?cht=tx&chl=n%3D10+)から![n=5001 ](http://chart.apis.google.com/chart?cht=tx&chl=n%3D5001+)までの値となっており，これは両者の手法がどちらも一定サイズのサンプルサイズを必要とするからである（つまり1から10までは図示していない）．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-4.png" alt="" /></p>

<h3>例 5.6 グローバルミニマムのまわりにローカルミニマムが配置されている関数を可視化する</h3>

<p>以下の関数の最小化を検討する．</p>

<p>![h(x,y)=(x\sin(20y)+y\sin(20x))<sup>2\cosh(\sin(10x)x)+</sup> (x\cos(10y)-y\sin(10x))<sup>2\cosh(\cos(20y)y)</sup> ](http://chart.apis.google.com/chart?cht=tx&chl=h%28x%2Cy%29%3D%28x%5Csin%2820y%29%2By%5Csin%2820x%29%29%3Csup%3E2%5Ccosh%28%5Csin%2810x%29x%29%2B%3C%2Fsup%3E+%28x%5Ccos%2810y%29-y%5Csin%2810x%29%29%3Csup%3E2%5Ccosh%28%5Ccos%2820y%29y%29%3C%2Fsup%3E+)</p>

<p>今回は可視化するだけだが，この後の例5.7などで度々登場する関数となる．3Dプロットを描写すると，(0,0)に最小値があり，その周りには山あり谷ありとローカルミニマムがたくさん配置されている関数だとわかる．この図を上から見ると，P.157のプロットのようになる．</p>

<p>```r
h &lt;- function(x,y){
  (x<em>sin(20</em>y) + y<em>sin(20</em>x))<sup>2</sup> * cosh(sin(10<em>x)</em>x) +</p>

<pre><code>(x*cos(10*y) - y*sin(10*x))^2 * cosh(cos(20*y)*y)
</code></pre>

<p>}
x &lt;- seq(-3, 3, le=435)
y &lt;- seq(-3, 3, le=435)
z &lt;- outer(x, y, h)
par(bg="wheat", mar=c(1,1,1,1))
persp(x, y, z, theta=155, phi=30, col="green4",</p>

<pre><code>  ltheta=-120, shade=0.75, border=NA, box=FALSE)
</code></pre>

<p>```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-6.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「データ解析のための統計モデリング入門」読書ノート 7章 GLMMとGLMを比較する]]></title>
    <link href="http://yagays.github.com/blog/2012/11/02/glm-mcmc-chp7/"/>
    <updated>2012-11-02T06:54:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/02/glm-mcmc-chp7</id>
    <content type="html"><![CDATA[<ul>
<li>前回：「データ解析のための統計モデリング入門」読書ノート：<a href="http://yagays.github.com/blog/2012/10/14/review-glm-mcmc/">http://yagays.github.com/blog/2012/10/14/review-glm-mcmc/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=400006973X" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>前回の読書ノートでは全体の流れを簡単に追ったが，今回は「データ解析のための統計モデリング入門」7章の一般化線形混合モデルのシミュレーションを動かしてみた．</p>

<h3>7章の概要</h3>

<p>6章までは，ある植物100個体から得られた生存種子数と葉数のデータから，その関係性をモデル化しGLMで推定してきた．7章では，個体差を取り入れたモデルを作成し，生存種子数の過分散にうまくあてはまるようなパラメータを推定する．ただし，個体差を表すパラメータを個体ごとに割り当てて最尤推定するのではなく，個体差がある分布に従うと仮定した上で，その分布のパラメータを推定する．</p>

<p>大雑把な目標としては，葉の数だけが生存種子数に効いているわけではないんだから，それぞれの個体で観測されていない/できないデータというのを考えようというもの．ただし，個体ごとに何らかの値を振って生存種子数を上手く表すことのできるモデルを組んだとしても予測には使えないので，じゃあ個体差自体を扱えるように分布を仮定してそこからランダムに値が選ばれるとする．そうすると，個体ごとにパラメータを持たせるんじゃなくて，個体のばらつきという一つのパラメータで表すことができるので，より現実的なモデルとなると同時に簡単にパラメータの推定ができるようになる．</p>

<h3>問題設定</h3>

<p>ここでは，ロジスティック回帰におけるGLMMを考える．種子生存数が二項分布</p>

<p>![\mathrm{Bin}(y_i|\beta_1,\beta_2,s) = {{N}\choose{y_i}} q_{i}<sup>{y_i}(1-q_i)<sup>{N-y_i}</sup></sup> ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathrm%7BBin%7D%28y_i%7C%5Cbeta_1%2C%5Cbeta_2%2Cs%29+%3D+%7B%7BN%7D%5Cchoose%7By_i%7D%7D+q_%7Bi%7D%3Csup%3E%7By_i%7D%281-q_i%29%3Csup%3E%7BN-y_i%7D%3C%2Fsup%3E%3C%2Fsup%3E+)</p>

<p>に従うとした上で，そのパラメータ![q_i ](http://chart.apis.google.com/chart?cht=tx&chl=q_i+)を</p>

<p>![\mathrm{logit}(q_i) = \beta_1 + \beta_2 x_i + r_i ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathrm%7Blogit%7D%28q_i%29+%3D+%5Cbeta_1+%2B+%5Cbeta_2+x_i+%2B+r_i+)</p>

<p>とおく．ここで，個体差![r_i ](http://chart.apis.google.com/chart?cht=tx&chl=r_i+)は</p>

<p>![r_i \sim \mathcal{N}(0,s) ](http://chart.apis.google.com/chart?cht=tx&chl=r_i+%5Csim+%5Cmathcal%7BN%7D%280%2Cs%29+)</p>

<p>平均0，標準誤差sの正規分布に従うとする．</p>

<h3>実験</h3>

<p>今回の実験は，なぜ応答変数が過分散の場合にGLMでは駄目でGLMMで推定すべきかを確かめるために，本書に記載されているGLMおよびGLMMの実験を1000回行なって，そのときの切片![\beta_1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+)と傾き![\beta_2 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+)の分布を見ることにする．目標は<a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf">「「個体差」の統計モデリング」</a>(pdf)の図11をプロットすること．</p>

<p>各実験は，葉数が2〜6枚の個体を20個体ずつ合計100個体のデータが得られたという条件で，それぞれの生存種子数をパラメータ![\beta_1 = -4 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+%3D+-4+)，![\beta_2 = 1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+%3D+1+)，![r_i \sim \mathcal{N}(0,3) ](http://chart.apis.google.com/chart?cht=tx&chl=r_i+%5Csim+%5Cmathcal%7BN%7D%280%2C3%29+)の二項乱数から生成した．このように生成したデータを用いて，GLMMとGLMを実行した．</p>

<p>この実験を1000回繰り返し，その時の![\beta_1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+)と![\beta_2 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+)の分布をプロットしたのが以下の図となっている．左図が切片![\beta_1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+)の分布，右図が傾き![\beta_2 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+)の分布で，それぞれ黒の線がGLMが推定した値の分布，赤の線がGLMMが推定した値の分布となっている．また，GLMとGLMMの推定値の平均を縦の点線で表している．</p>

<p>```r
library(glmmML)</p>

<p>logistic &lt;- function(z){ 1/(1+exp(-z))}
Niter &lt;- 1000
sigma &lt;- 3
glm_b &lt;- matrix(0, nrow=Niter, ncol=2)
glmm_b &lt;- matrix(0, nrow=Niter, ncol=2)</p>

<p>for(i in 1:Niter){
  N &lt;- rep(8,100)
  x &lt;- rep(2:6,each=20)
  re &lt;- rnorm(100, 0, sigma)
  y &lt;- rbinom(100, 8, prob=logistic(-4+x+re))
  id &lt;- 1:100
  d &lt;- data.frame(N,x,y,id)</p>

<p>  glm_b[i,] &lt;- glm(cbind(y,N-y) ~ x, data=d, family=binomial)$coefficients
  glmm_b[i,] &lt;- glmmML(cbind(y,N-y) ~ x, data=d, family=binomial, cluster=id)$coefficients
}
```</p>

<p>```r
par(mfrow=c(1,2))
plot(density(glm_b[,1]), xlim=c(-8,0), ylim=c(0,0.8), lwd=2, main="Estimate of beta_1", xlab="")
par(new=T)
plot(density(glmm_b[,1]), xlim=c(-8,0), ylim=c(0,0.8), col="red", lwd=2, main="Estimate of beta_1", xlab="")
abline(v=mean(glm_b[,1]), lty=2)
abline(v=mean(glmm_b[,1]), lty=2, col="red")</p>

<p>plot(density(glm_b[,2]), xlim=c(0,2), ylim=c(0,3), lwd=2, main="Estimate of beta_2", xlab="")
par(new=T)
plot(density(glmm_b[,2]), xlim=c(0,2), ylim=c(0,3), col="red", lwd=2, main="Estimate of beta_2", xlab="")
abline(v=mean(glm_b[,2]), lty=2)
abline(v=mean(glmm_b[,2]), lty=2, col="red")
legend(1.1, 3.0, c("glm","glmmML"), col=c(1:2), lwd=2)
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/glm_mcmc_7.png" alt="" /></p>

<p>さて，この図の解釈だが，今回データセット作成に用いた実験条件は![\beta_1 = -4 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+%3D+-4+)，![\beta_2 = 1 ](http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+%3D+1+)だったので，GLMMにおいて切片・傾きどちらもある程度正しい推定ができていることがわかる．逆にGLMでは，切片は過大推定，傾きは過小推定されている．また，推定値のばらつきである分布の山の裾の拡がりを見てみると，GLMMでは大きく，GLMでは小さくなっている．これはいわゆるバイアス・バリアンスのトレードオフによるものだと思われる．</p>

<h4>参考</h4>

<ul>
<li><a href="http://hosho.ees.hokudai.ac.jp/~kubo/ce/IwanamiBook.html">生態学データ解析 - 本/データ解析のための統計モデリング入門</a></li>
<li><a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/49477/7/kubostat2008f.pdf">http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/49477/7/kubostat2008f.pdf</a></li>
<li><a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf">http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
