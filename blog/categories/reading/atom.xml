<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Reading | Wolfeyes Bioinformatics beta]]></title>
  <link href="http://yagays.github.com/blog/categories/reading/atom.xml" rel="self"/>
  <link href="http://yagays.github.com/"/>
  <updated>2012-11-08T06:45:12+09:00</updated>
  <id>http://yagays.github.com/</id>
  <author>
    <name><![CDATA[yag_ays]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：5章 モンテカルロ最適化 確率的探索 その2]]></title>
    <link href="http://yagays.github.com/blog/2012/11/06/imcmr-5-3/"/>
    <updated>2012-11-06T11:25:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/06/imcmr-5-3</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>例 5.4 数値的最適化法と確率的探索を比較する</h3>

<p><a href="http://yagays.github.com/blog/2012/10/30/imcmr-5-1/">例5.1</a>の続き．前回は数値的最適化法によって<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+>の値を推定したが，今回は確率的探索による推定を行なって，両者の手法の比較を行う．問題としては，以下の対数尤度を最大化する．</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clog+l%28%5Ctheta%7C+x_1%2C%5Cldots%2Cx_n%29+%3D+%5Csum_%7Bi%3D1%7D%3Csup%3E%7Bn%7D%3C%2Fsup%3E+%5Clog+%5Cfrac%7B1%7D%7B1%2B%28x_i-%5Ctheta%29%3Csup%3E2%7D%3C%2Fsup%3E+></p>

<p>この式は<img src=http://chart.apis.google.com/chart?cht=tx&chl=n+%5Crightarrow+%5Cinfty+>において<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E%2A%3C%2Fsup%3E+%3D0+>になる．すなわちこれから求める推定値の答えは0ということになる．</p>

<h4>ソースコード</h4>

<p>以下のソースコードにおける変数の対応は以下のようになっている．</p>

<ul>
<li>数値的最適化法

<ul>
<li>trul：推定値</li>
<li>truv：対数尤度</li>
</ul>
</li>
<li>確率的探索

<ul>
<li>loc：推定値</li>
<li>maxv：対数尤度</li>
</ul>
</li>
</ul>


<p>なお，mcsmパッケージに付随するデモスクリプトでは，数値的最適化法における乱数を正規乱数としていたり対数尤度の関数が微妙に違っているが，今回は例5.1の解法と同様のコーシー乱数と対数尤度の式を用いている．</p>

<p>```r
ref &lt;- rcauchy(5001)
f &lt;- function(y){-sum(log(1+(x-y)<sup>2))}</sup></p>

<p>maxv &lt;- NULL
loc &lt;- NULL
truv &lt;- NULL
trul &lt;- NULL</p>

<p>for (i in 10:length(ref)){
  # 数値的最適化
  x &lt;- ref[1:i]
  tru &lt;- optimize(f,c(-5,5),maximum=T)
  trul &lt;- c(trul, tru$max)
  truv &lt;- c(truv, tru$ob)</p>

<p>  # 確率的探索
  prop &lt;- runif(10<sup>3,-5,5)</sup>
  vale &lt;- apply(as.matrix(prop),1,f)
  loc &lt;- c(loc, prop[order(-vale)[1]])
  maxv &lt;- c(maxv, max(vale))
}</p>

<p>par(mar=c(4,4,1,1), mfrow=c(2,1))
plot(trul, loc, cex=.5, pch=19, xlab=expression(theta<sup>0),</sup> ylab=expression(hat(theta)))
abline(a=0, b=1, col="grey")
plot(10:length(ref), (truv-maxv)/abs(truv), type="l", lwd=2, xlab="Sample size", ylab="Relative error", ylim=c(0,0.02))
```</p>

<p>まず，上のプロットは，数値的最適化法による推定値を横軸に，確率的探索による推定値を縦軸に取った図となっている．真の値が<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E%2A%3C%2Fsup%3E+%3D0+>なので，当然ながら両者ともに推定値は(0,0)付近に集まっている．また，推定値が外れる場合では，どちらかの値が大きくなればもう片方も大きくなるといった具合に，そのパターンはグレーの線で示した<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta%3Csup%3E0%3C%2Fsup%3E+%3D+%5Chat%7B%5Ctheta%7D+>の線分の上に乗るような形となる．</p>

<p>次に，下のプロットは，横軸にサンプルサイズを取り，縦軸に推定値における対数尤度の差を取った図となっている．今回は相対的誤差なので，(数値的最適化法-確率的探索)/(数値的最適化法)といった形で値を求めている．相対的誤差が大きいということは，数値的最適化法と確率的探索それぞれ求めた対数尤度に差がある，すなわち<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+>に差があるということで，上のプロットにおけるグレーの線分から離れれば離れるほど相対的誤差も大きくなるということになる．なお，図示しているサンプルサイズは<img src=http://chart.apis.google.com/chart?cht=tx&chl=n%3D10+>から<img src=http://chart.apis.google.com/chart?cht=tx&chl=n%3D5001+>までの値となっており，これは両者の手法がどちらも一定サイズのサンプルサイズを必要とするからである（つまり1から10までは図示していない）．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-4.png" alt="" /></p>

<h3>例 5.6 グローバルミニマムのまわりにローカルミニマムが配置されている関数を可視化する</h3>

<p>以下の関数の最小化を検討する．</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=h%28x%2Cy%29%3D%28x%5Csin%2820y%29%2By%5Csin%2820x%29%29%3Csup%3E2%5Ccosh%28%5Csin%2810x%29x%29%2B%3C%2Fsup%3E+%28x%5Ccos%2810y%29-y%5Csin%2810x%29%29%3Csup%3E2%5Ccosh%28%5Ccos%2820y%29y%29%3C%2Fsup%3E+></p>

<p>今回は可視化するだけだが，この後の例5.7などで度々登場する関数となる．3Dプロットを描写すると，(0,0)に最小値があり，その周りには山あり谷ありとローカルミニマムがたくさん配置されている関数だとわかる．この図を上から見ると，P.157のプロットのようになる．</p>

<p>```r
h &lt;- function(x,y){
  (x<em>sin(20</em>y) + y<em>sin(20</em>x))<sup>2</sup> * cosh(sin(10<em>x)</em>x) +</p>

<pre><code>(x*cos(10*y) - y*sin(10*x))^2 * cosh(cos(20*y)*y)
</code></pre>

<p>}
x &lt;- seq(-3, 3, le=435)
y &lt;- seq(-3, 3, le=435)
z &lt;- outer(x, y, h)
par(bg="wheat", mar=c(1,1,1,1))
persp(x, y, z, theta=155, phi=30, col="green4",</p>

<pre><code>  ltheta=-120, shade=0.75, border=NA, box=FALSE)
</code></pre>

<p>```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-6.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「データ解析のための統計モデリング入門」読書ノート 7章 GLMMとGLMを比較する]]></title>
    <link href="http://yagays.github.com/blog/2012/11/02/glm-mcmc-chp7/"/>
    <updated>2012-11-02T06:54:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/02/glm-mcmc-chp7</id>
    <content type="html"><![CDATA[<ul>
<li>前回：「データ解析のための統計モデリング入門」読書ノート：<a href="http://yagays.github.com/blog/2012/10/14/review-glm-mcmc/">http://yagays.github.com/blog/2012/10/14/review-glm-mcmc/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=400006973X" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>前回の読書ノートでは全体の流れを簡単に追ったが，今回は「データ解析のための統計モデリング入門」7章の一般化線形混合モデルのシミュレーションを動かしてみた．</p>

<h3>7章の概要</h3>

<p>6章までは，ある植物100個体から得られた生存種子数と葉数のデータから，その関係性をモデル化しGLMで推定してきた．7章では，個体差を取り入れたモデルを作成し，生存種子数の過分散にうまくあてはまるようなパラメータを推定する．ただし，個体差を表すパラメータを個体ごとに割り当てて最尤推定するのではなく，個体差がある分布に従うと仮定した上で，その分布のパラメータを推定する．</p>

<p>大雑把な目標としては，葉の数だけが生存種子数に効いているわけではないんだから，それぞれの個体で観測されていない/できないデータというのを考えようというもの．ただし，個体ごとに何らかの値を振って生存種子数を上手く表すことのできるモデルを組んだとしても予測には使えないので，じゃあ個体差自体を扱えるように分布を仮定してそこからランダムに値が選ばれるとする．そうすると，個体ごとにパラメータを持たせるんじゃなくて，個体のばらつきという一つのパラメータで表すことができるので，より現実的なモデルとなると同時に簡単にパラメータの推定ができるようになる．</p>

<h3>問題設定</h3>

<p>ここでは，ロジスティック回帰におけるGLMMを考える．種子生存数が二項分布</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathrm%7BBin%7D%28y_i%7C%5Cbeta_1%2C%5Cbeta_2%2Cs%29+%3D+%7B%7BN%7D%5Cchoose%7By_i%7D%7D+q_%7Bi%7D%3Csup%3E%7By_i%7D%281-q_i%29%3Csup%3E%7BN-y_i%7D%3C%2Fsup%3E%3C%2Fsup%3E+></p>

<p>に従うとした上で，そのパラメータ<img src=http://chart.apis.google.com/chart?cht=tx&chl=q_i+>を</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathrm%7Blogit%7D%28q_i%29+%3D+%5Cbeta_1+%2B+%5Cbeta_2+x_i+%2B+r_i+></p>

<p>とおく．ここで，個体差<img src=http://chart.apis.google.com/chart?cht=tx&chl=r_i+>は</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=r_i+%5Csim+%5Cmathcal%7BN%7D%280%2Cs%29+></p>

<p>平均0，標準誤差sの正規分布に従うとする．</p>

<h3>実験</h3>

<p>今回の実験は，なぜ応答変数が過分散の場合にGLMでは駄目でGLMMで推定すべきかを確かめるために，本書に記載されているGLMおよびGLMMの実験を1000回行なって，そのときの切片<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+>と傾き<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+>の分布を見ることにする．目標は<a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf">「「個体差」の統計モデリング」</a>(pdf)の図11をプロットすること．</p>

<p>各実験は，葉数が2〜6枚の個体を20個体ずつ合計100個体のデータが得られたという条件で，それぞれの生存種子数をパラメータ<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+%3D+-4+>，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+%3D+1+>，<img src=http://chart.apis.google.com/chart?cht=tx&chl=r_i+%5Csim+%5Cmathcal%7BN%7D%280%2C3%29+>の二項乱数から生成した．このように生成したデータを用いて，GLMMとGLMを実行した．</p>

<p>この実験を1000回繰り返し，その時の<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+>と<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+>の分布をプロットしたのが以下の図となっている．左図が切片<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+>の分布，右図が傾き<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+>の分布で，それぞれ黒の線がGLMが推定した値の分布，赤の線がGLMMが推定した値の分布となっている．また，GLMとGLMMの推定値の平均を縦の点線で表している．</p>

<p>```r
library(glmmML)</p>

<p>logistic &lt;- function(z){ 1/(1+exp(-z))}
Niter &lt;- 1000
sigma &lt;- 3
glm_b &lt;- matrix(0, nrow=Niter, ncol=2)
glmm_b &lt;- matrix(0, nrow=Niter, ncol=2)</p>

<p>for(i in 1:Niter){
  N &lt;- rep(8,100)
  x &lt;- rep(2:6,each=20)
  re &lt;- rnorm(100, 0, sigma)
  y &lt;- rbinom(100, 8, prob=logistic(-4+x+re))
  id &lt;- 1:100
  d &lt;- data.frame(N,x,y,id)</p>

<p>  glm_b[i,] &lt;- glm(cbind(y,N-y) ~ x, data=d, family=binomial)$coefficients
  glmm_b[i,] &lt;- glmmML(cbind(y,N-y) ~ x, data=d, family=binomial, cluster=id)$coefficients
}
```</p>

<p>```r
par(mfrow=c(1,2))
plot(density(glm_b[,1]), xlim=c(-8,0), ylim=c(0,0.8), lwd=2, main="Estimate of beta_1", xlab="")
par(new=T)
plot(density(glmm_b[,1]), xlim=c(-8,0), ylim=c(0,0.8), col="red", lwd=2, main="Estimate of beta_1", xlab="")
abline(v=mean(glm_b[,1]), lty=2)
abline(v=mean(glmm_b[,1]), lty=2, col="red")</p>

<p>plot(density(glm_b[,2]), xlim=c(0,2), ylim=c(0,3), lwd=2, main="Estimate of beta_2", xlab="")
par(new=T)
plot(density(glmm_b[,2]), xlim=c(0,2), ylim=c(0,3), col="red", lwd=2, main="Estimate of beta_2", xlab="")
abline(v=mean(glm_b[,2]), lty=2)
abline(v=mean(glmm_b[,2]), lty=2, col="red")
legend(1.1, 3.0, c("glm","glmmML"), col=c(1:2), lwd=2)
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/glm_mcmc_7.png" alt="" /></p>

<p>さて，この図の解釈だが，今回データセット作成に用いた実験条件は<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_1+%3D+-4+>，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cbeta_2+%3D+1+>だったので，GLMMにおいて切片・傾きどちらもある程度正しい推定ができていることがわかる．逆にGLMでは，切片は過大推定，傾きは過小推定されている．また，推定値のばらつきである分布の山の裾の拡がりを見てみると，GLMMでは大きく，GLMでは小さくなっている．これはいわゆるバイアス・バリアンスのトレードオフによるものだと思われる．</p>

<h4>参考</h4>

<ul>
<li><a href="http://hosho.ees.hokudai.ac.jp/~kubo/ce/IwanamiBook.html">生態学データ解析 - 本/データ解析のための統計モデリング入門</a></li>
<li><a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/49477/7/kubostat2008f.pdf">http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/49477/7/kubostat2008f.pdf</a></li>
<li><a href="http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf">http://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/26401/1/%E6%97%A5%E6%9C%AC%E7%94%9F%E6%85%8B%E5%AD%A656-2.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：5章 モンテカルロ最適化 確率的探索]]></title>
    <link href="http://yagays.github.com/blog/2012/11/01/imcmr-5-2/"/>
    <updated>2012-11-01T02:18:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/11/01/imcmr-5-2</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>例 5.3 確率的探索を使って，一様分布からのシミュレーションにより関数の最大値を求める</h3>

<p><a href="http://yagays.github.com/blog/2012/10/25/imcmr-3-1/">例 3.3</a>で積分を考えた関数<img src=http://chart.apis.google.com/chart?cht=tx&chl=h%28x%29+%3D+%5B%5Ccos%2850x%29+%2B+%5Csin%2820x%29+%5D%3Csup%3E2%3C%2Fsup%3E+>について，今度はその最大値について考える．</p>

<p>今回の実験は1000回シミュレーションを行う．それぞれのシミュレーションには，まず一様乱数<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BU%7D%3Cem%3E%7B%5B0%2C1%5D%7D+%3D+u_1%2C+%5Cldots+%2C+u_m+>を1000個作成し，それぞれに関数を掛けた値の中から最大値<img src=http://chart.apis.google.com/chart?cht=tx&chl=h%3C%2Fem%3E%7Bm%7D%3Csup%3E%7B%2A%7D%3C%2Fsup%3E+%3D+%5Cmax%28h%28u_1%29%2C+%5Cldots+%2Ch%28u_m%29%29+>を求める．</p>

<p>確率的探索といっても，今回の場合にやっていることは結局のところ定義域内で乱数を発生させて関数の値が大きいところを探しているだけとなる．今回は一様乱数で定義域内をまんべんなく探すという方法を取っているが，この後の確率的探索の例では「密度」を考えて，関数の値が大きい領域でのシミュレーションの確率を高くし，その他の関数の値の小さい領域での確率を小さくするという方法が取られる．</p>

<h4>今回のシミュレーションの仕組み</h4>

<p>今回のコードは少しややこしいので，まずは試しにシミュレーションを1回だけ行なってみよう．今回は推定値がどのように更新されていくかを見るために，単純にmax()関数を使うのではなく，cummax()関数を使用している．これにより，ベクトルの前から順に値を見ていって，最大値を更新していく様子を見ることができる．以下の図の黒の線が推定値<img src=http://chart.apis.google.com/chart?cht=tx&chl=h_%7Bm%7D%3Csup%3E%7B%2A%7D%3C%2Fsup%3E+>，赤の線がoptimize()関数で求めた定義域内における関数<img src=http://chart.apis.google.com/chart?cht=tx&chl=h%28x%29+>の最大値を示している．</p>

<p><code>r
h &lt;- function(x){(cos(50*x) + sin(20*x))^2}
plot(cummax(h(runif(10^3))),type="l")
abline(h=optimize(h,int=c(0,1), maximum=T)$ob,col="red")
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-3_3.png" alt="" /></p>

<h4>実際に例5.3のシミュレーションを行う</h4>

<p>では実際に1000回のシミュレーションを行なって，推定値がどのように更新されていくのか，そしてその値がどれほどバラけるのかを見ていく．</p>

<p>以下の図が今回の結果となっている．まず，本書に載っている左図ではなく，右の赤い線が重なった図から説明する．この図は1000回シミュレーションした中から10回だけ抜き出してプロットしたものとなっている．これを見ると，最初のうちは0に近いような値から始まっていたりと値が安定しないものの，200回もすると実際の値と殆ど変わらないような推定値になり落ち着いているのがわかる．このように，値がきちんと実際の値に近づくかどうかを見るために，各イテレーションごとの最小値を求めてプロットしたのが左図となっている．灰色の部分は，イテレーションごとの推定値の幅を示しており，これを見ると，800回程度実験すれば一応どのシミュレーションでも実際の値と推定値がほぼ同じになることが分かる．</p>

<p>```r
h &lt;- function(x){(cos(50<em>x) + sin(20</em>x))<sup>2}</sup>
rangom &lt;- h(matrix(runif(10<sup>6),</sup> ncol=10<sup>3))</sup>
monitor &lt;- t(apply(rangom, 1, cummax))</p>

<p>par(mfrow=c(1,2))
plot(monitor[1,], type="l", col="white", ylim=c(0, optimize(h,int=c(0,1), maximum=T)$ob),</p>

<pre><code> xlab="Iterations", ylab="h(theta)") # 描写する領域をplotしているだけ
</code></pre>

<p>par(new=T)
polygon(c(1:10<sup>3,</sup> 10<sup>3:1),</sup> c(apply(monitor,2,max), rev(apply(monitor, 2, min))), col="gray")
abline(h=optimize(h, int=c(0,1), maximum=T)$ob)</p>

<p>plot(monitor[1,],type="l",col="red", ylim=c(0,optimize(h, int=c(0,1), maximum=T)$ob),</p>

<pre><code> xlab="Iterations", ylab="h(theta)")
</code></pre>

<p>for(i in 2:10){
  lines(monitor[i,],col="red")
}
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-3_1.png" alt="" /></p>

<h3>練習問題 5.3 一様分布を用いたシミュレーションのパフォーマンスを向上させるために使用する定義域における制約を可視化する</h3>

<p>まず最初に注意．この問題は解けないわけではないが，厳密には間違っているらしい．訂正は<a href="http://www.ceremade.dauphine.fr/~xian/shortManual.pdf">解答</a>(pdf)に書かれているが，つまりは定義域<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cit%7B%5CTheta%7D+>の制約は，</p>

<blockquote><p><img src=http://chart.apis.google.com/chart?cht=tx&chl=x%3Csup%3E2%281%2B%5Csin%283y%29%5Ccos%288x%29%29%3C%2Fsup%3E+%2B+y%3Csup%3E2%282%2B%5Ccos%285x%29%5Ccos%288y%29%29%3C%2Fsup%3E+%5Cleq+1+></p></blockquote>

<p>ではなく，</p>

<blockquote><p> <img src=http://chart.apis.google.com/chart?cht=tx&chl=x%3Csup%3E2%281%2B%5Csin%28%5Cfrac%7B3%7D%7By%7D%29%5Ccos%288x%29%29%3C%2Fsup%3E+%2B+y%3Csup%3E2%282%2B%5Ccos%285x%29%5Ccos%288y%29%29%3C%2Fsup%3E+%5Cleq+1+></p></blockquote>

<p>としなければいけないようだ．今回はどちらの式も試してみて，本に書かれている式の何がおかしかったのを見てみる．</p>

<p>まず，左図は解答に記載されている修正後の式を使った場合，右図は本書に載っているオリジナルの式を使った場合となっている．これを見ると，左図では黒の円の中に式で表された制約の定義域が収まっているのに対し，右図では黒の円から一部制約の定義域がはみ出ているように見える．</p>

<p>```r
theta=runif(10<sup>5)<em>2</em>pi</sup>
rho &lt;- runif(10<sup>5)</sup>
xunif &lt;- rho<em>cos(theta)/.77
yunif &lt;- rho</em>sin(theta)</p>

<p>par(mfrow=c(1,2))
plot(xunif, yunif, pch=19, cex=.4, xlab="x", ylab="y")
const &lt;- (xunif<sup>2<em>(1+sin(yunif/3)</em>cos(xunif*8))+</sup>
  yunif<sup>2<em>(2+cos(5</em>xunif)<em>cos(8</em>yunif))&lt;1)</sup>
points(xunif[const], yunif[const], col="cornsilk2", pch=19,cex=.4)</p>

<p>plot(xunif, yunif, pch=19, cex=.4, xlab="x", ylab="y")
const2 &lt;- (xunif<sup>2<em>(1+sin(yunif</em>3)<em>cos(xunif</em>8))+</sup>
  yunif<sup>2<em>(2+cos(5</em>xunif)<em>cos(8</em>yunif))&lt;1)</sup>
points(xunif[const2], yunif[const2], col="red", pch=19, cex=.4)
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-3_2.png" alt="" /></p>

<p>また，この方法のパフォーマンスを測定するために，棄却された点の平均数で評価してみる．生成した乱数の総数が10<sup>5</sup> 個なので，74%は受理され，26%が棄却されたことがわかる．</p>

<p>```r
summary(const)</p>

<h1>Mode   FALSE    TRUE    NA's</h1>

<h1>logical   26285   73715       0</h1>

<p>```</p>

<p>それにしても，この図に何とも言えない可愛らしさを感じる，宇宙の侵略者を撃ち落とす某ゲームで見たことがあるような…．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：5章 モンテカルロ最適化 数値的最適化法]]></title>
    <link href="http://yagays.github.com/blog/2012/10/30/imcmr-5-1/"/>
    <updated>2012-10-30T15:54:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/10/30/imcmr-5-1</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>本章の概要</h3>

<p>乱数を使って最適化問題を解くには2つの方法がある．</p>

<ul>
<li>確率的探索を用いて関数の最大値を探す（本章前半）</li>
<li>シミュレーションを使って最適化する関数を近似する（本章後半，5.4以降）</li>
</ul>


<h3>内容：モンテカルロ最適化で関数の最大値を求める</h3>

<h4>最適化問題には2種類ある</h4>

<p>最適化問題には大きくわけて2種類存在する．</p>

<ul>
<li>定義域<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cit%7B%5CTheta%7D+>で関数<img src=http://chart.apis.google.com/chart?cht=tx&chl=h%28%5Ctheta%29>の極値を求める</li>
<li>定義域<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cit%7B%5CTheta%7D+>で陰的方程式<img src=http://chart.apis.google.com/chart?cht=tx&chl=g%28%5Ctheta%29+%3D+0+>の解を求める</li>
</ul>


<p>問題設定はそれぞれ違うのだが，今回は最大化問題だけを取り上げる．つまり，以下の式のように定義域<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cit%7B%5CTheta%7D+>で一番大きな値を求めたい．</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=max_%7B%5Ctheta+%5Cin+%5CTheta%7D%3Dh%28%5Ctheta%29></p>

<h4>解き方にも2種類ある</h4>

<ul>
<li>数値的手法（5.2 数値的最適化法）

<ul>
<li>目標関数の特性に強く依存する</li>
</ul>
</li>
<li>確率的手法（5.3 確率的探索）</li>
</ul>


<p>さて，ある関数のある定義域内における最大値を求めようと思った場合にじゃあどうするかというと，定義域内の値を片っ端から入れていって大きいものを探すといったアプローチと，別の分布をシミュレーションしてそこから最大値が現れるまでシミュレーションし続けるという方法の2通りがあるということになる．ここで重要になるポイントが定義域で，3章のモンテカルロ積分において積分する範囲を決めていたように，5章モンテカルロ最適化においても定義域内で最大値を探すということに繋がる．ただし，モンテカルロ積分よりもモンテカルロ最適化のほうが，全体の平均を求めるよりもピンポイントで値の大きい極値を求めるという点で，一般的に難しい．</p>

<h3>例 5.1 コーシー分布から得られたサンプルの尤度を最大化する</h3>

<h4>左の図</h4>

<p>何かしらのコーシー分布から生成された乱数<img src=http://chart.apis.google.com/chart?cht=tx&chl=%7Bx_1%2C+%5Cdots+%2Cx_n%7D>が与えられた時に，それを生成したコーシー分布<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BC%7D%28%5Ctheta%2C1%29+>の<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Ctheta+>を最尤法で推定しようという問題．尤度は以下の式で表される</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=l%28%5Ctheta%7C+x_1%2C%5Cldots%2Cx_n%29+%3D+%5Cprod_%7Bi%3D1%7D%3Csup%3E%7Bn%7D%5Cfrac%7B1%7D%7B1%2B%28x_i-%5Ctheta%29%3Csup%3E2%7D%3C%2Fsup%3E%3C%2Fsup%3E+></p>

<p>実際に計算するときには以下の対数尤度を使う．</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clog+l%28%5Ctheta%7C+x_1%2C%5Cldots%2Cx_n%29+%3D+%5Csum_%7Bi%3D1%7D%3Csup%3E%7Bn%7D%3C%2Fsup%3E+%5Clog+%5Cfrac%7B1%7D%7B1%2B%28x_i-%5Ctheta%29%3Csup%3E2%7D%3C%2Fsup%3E+></p>

<p>今回は実験として，上の尤度と対数尤度の2パターンで計算したときに，どういう違いが現れるかを示す．数式上は違いはないが，計算機上では値の取り扱いにおいて違いが出てくる．</p>

<p>```r
xm &lt;- rcauchy(500)
loglik &lt;- function(y){-sum(log(1+(x-y)<sup>2))}</sup>
lik &lt;- function(y){prod(1/(1+(x-y)<sup>2))}</sup></p>

<p>mi &lt;- numeric(500)
mi2 &lt;- numeric(500)
for(i in 1:500){
  x &lt;- xm[1:i]
  mi[i] &lt;- optimize(loglik, interval=c(-10,10), maximum=T)$max
  mi2[i] &lt;- optimize(lik, interval=c(-10,10),maximum=T)$max
}</p>

<p>plot(mi, type="l", ylim=c(-5, 10), ylab="theta", xlab="sample size")
par(new=T)
plot(mi2, type="l", ylim=c(-5,10),col="red", ylab="theta", xlab="sample size")
```</p>

<p>尤度で計算した(赤)と対数尤度で計算した計算(黒)を以下のプロットの左側の図に示してある．これをみると，対数尤度で計算した場合では着実に推定値が収束していくのに対し，尤度で計算した場合ではある地点で値が発散してるのがわかる．これは，Rのoptimizeにおいて，尤度を直接観察すると値が発散することがあるからである．このように，確率を扱う場合には，値が非常に小さくなることに注意しなければならない．</p>

<h4>右の図</h4>

<p>次に，上記の尤度を，摂動が加えられた尤度に置き換える．これは<img src=http://chart.apis.google.com/chart?cht=tx&chl=-%5B%5Csin%28100y%29%5D%3Csup%3E2%3C%2Fsup%3E+>という0から1の間の値を取るノイズを加えている．</p>

<p>```r
perturbedloglik &lt;- function(y){-sin(y<em>100)<sup>2</sup> - sum(log(1+(x-y)<sup>2))}</sup>
perturbedlik &lt;- function(y){-sin(y</em>100)<sup>2</sup> + prod(1/(1+(x-y)<sup>2))}</sup>
mi &lt;- numeric(500)
mi2 &lt;- numeric(500)
for(i in 1:500){
  x &lt;- xm[1:i]
  mi[i] &lt;- optimize(perturbedloglik, interval=c(-10,10), maximum=T)$max
  mi2[i] &lt;- optimize(perturbedlik, interval=c(-10,10),maximum=T)$max
}</p>

<p>plot(mi, type="l", ylim=c(-5, 10), ylab="theta", xlab="sample size")
par(new=T)
plot(mi2, type="l", ylim=c(-5,10),col="red", ylab="theta", xlab="sample size")</p>

<p>```</p>

<p>これをみると，尤度に関しては左図で示したように発散してしまっている．対数尤度を見てみると，左図と比べて値がばらつきグラフがギザギザになっている．当然だが，ノイズを加えれば値がばらついてしまう．</p>

<p>(個人的な感想)右図は何が言いたいのかちょっと分からない．ここの解釈は本書では出てこないと思うのだが，尤度関数に何らかの関数が加わる状況といえば，最小二乗法などにおける正則化項を加えて過学習を押さえる場合だろうか？　正則化項は摂動というよりかは，lassoやridgeのように1次/2次の関数になるので，今回の場合みたいにノイズのようにはならないと思うのだが…．</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-1.png" alt="" /></p>

<h3>例 5.2 Newton-Raphson法を用いて混合正規分布モデルから得られたサンプルの尤度を最大化する</h3>

<p>今回の混合正規モデルでは，</p>

<p><img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B1%7D%7B4%7D%5Cmathcal%7BN%7D%28%5Cmu_1%2C1%29%2B%5Cfrac%7B3%7D%7B4%7D%5Cmathcal%7BN%7D%28%5Cmu_2%2C1%29+></p>

<p>を考える．まずは分布の形を調べるために，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmu_1+%3D+0+>，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmu_2+%3D+2.5+>における対数尤度の分布を示してみる．そして，初期値(1,1)からスタートして，Newton-Raphson法がどのように推定値に収束していくかを図示する．</p>

<p>```r
da &lt;- sample(c(rnorm(10<sup>2),</sup> 2.5+rnorm(3<em>10<sup>2)))</sup>
like &lt;- function(mu){ -sum(log((0.25</em>dnorm(da-mu[1]) + 0.75*dnorm(da-mu[2])))) }</p>

<p>mu1 &lt;- seq(-2, 5, le=250)
mu2 &lt;- seq(-2, 5, le=250)
lli=matrix(0,nco=250,nro=250)
for (i in 1:250){
  for (j in 1:250){</p>

<pre><code>lli[i,j] &lt;- like(c(mu1[i],mu2[j]))
</code></pre>

<p>  }
}
image(mu1, mu2, -lli)
contour(mu1, mu2, -lli,nlevels=100, add=T)</p>

<p>sta &lt;- c(1,1)
mmu &lt;- sta
for(i in 1:(nlm(like, sta)$it)){
  mmu &lt;- rbind(mmu, nlm(like, sta, iterlim=i)$est)
}
lines(mmu, pch=19, lwd=2)
```</p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp5_5-2_1.png" alt="" /></p>

<h4>アニメーションにしてみる</h4>

<p>例が1つだけでは面白くないので，初期値をバラバラに選んで何回もシミュレーションしてみて，初期値からどのように推定値が更新されていくのかを見てみる．今回は，初期値として<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmu_1+>，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmu_2+>ともに図に表されている-2から5の範囲内でランダムに値を取るようにして，そこからのNewton-Raphson法による推定値の変化を1ステップごとにアニメーションとして作成した．それを25パターン作成して繋げている．</p>

<p>```r
library(animation)</p>

<p>mixtureloglik &lt;- function(Niter){
  for(n in 1:Niter){</p>

<pre><code>sta &lt;- c(runif(1,2-2,5),runif(1,-2,5))
mmu &lt;- sta
da &lt;- sample(c(rnorm(10^2), 2.5+rnorm(3*10^2)))
for(i in 1:(nlm(like, sta)$it)){
  mmu &lt;- rbind(mmu, nlm(like, sta, iterlim=i)$est)
}
for(j in 1:nrow(mmu)){
  image(mu1, mu2, -lli, main=paste("Iter:",n))
  contour(mu1, mu2, -lli,nlevels=100, add=T)
  lines(mmu[1:j,1],mmu[1:j,2], pch=19, lwd=2)
}
</code></pre>

<p>  }
}</p>

<p>saveMovie(mixtureloglik(25), interval=0.05, moviename="",</p>

<pre><code>      movietype="gif", outdir=getwd(),
      width=640, height=480)
</code></pre>

<p>```</p>

<p>Rのanimationというパッケージを使用して作成している．このパッケージでは標準でmpgも出力できるらしいのだが，今回上手くいかなかったので，gifアニメとして出力したのちにImagemagickとffmpegを使用してmpgの動画として出力した．gifアニメは思いの外重いので，今回はYoutubeにアップロードした．</p>

<p><code>bash
$ convert animation.gif Rplot%d.png
$ ffmpeg -f image2 -i Rplot%d.png animation.mpg
</code></p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/ZbppoIltz3E" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるモンテカルロ法入門」読書ノート：3章 重点サンプリング]]></title>
    <link href="http://yagays.github.com/blog/2012/10/28/imcmr-3-3/"/>
    <updated>2012-10-28T09:45:00+09:00</updated>
    <id>http://yagays.github.com/blog/2012/10/28/imcmr-3-3</id>
    <content type="html"><![CDATA[<ul>
<li>読書ノート アーカイブ：<a href="http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/">http://yagays.github.com/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/</a></li>
</ul>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621065270" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<h3>内容：重点サンプリングにより推定量の分散と効率を改善する</h3>

<h3>例 3.5 <img src=http://chart.apis.google.com/chart?cht=tx&chl=Z+%5Csim+%5Cmathcal%7BN%7D%280%2C1%29+>で<img src=http://chart.apis.google.com/chart?cht=tx&chl=P%28Z+%3E+4.5%29+>の確率を求める</h3>

<p>確率を求めると書いてあるが，要するに正規分布の確率密度関数において4.5からInfまでの面積（<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cint_%7B4.5%7D%3Csup%3E%7B%5Cinfty%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7De%3Csup%3E%7B-%5Cfrac%7Bx%3Csup%3E2%7D%7B2%7D%7D%3C%2Fsup%3E%3C%2Fsup%3E%3C%2Fsup%3E+dx+>）を求めるというもの．ただし，正規分布でxの値が4.5ともなると確率そのものがものすごく小さいので，普通のモンテカルロ積分で正規分布から乱数を生成したとしても，シミュレーション回数を非常に大きくしないと精度の良い値が求まらない．</p>

<p>```r
pnorm(-4.5,log=T)</p>

<h1>[1] -12.59242</h1>

<p>pnorm(-4.5)</p>

<h1>[1] 3.397673e-06</h1>

<p>```</p>

<p>ということで，重点サンプリングでシミュレーション回数を抑える．<img src=http://chart.apis.google.com/chart?cht=tx&chl=g%28y%29+%3D+%5Cfrac%7Be%3Csup%3E%7B-y%7D%7D%7B%5Cint_%7B4.5%7D%3Csup%3E%7B%5Cinfty%7De%3Csup%3E%7B-x%7Ddx%7D%3C%2Fsup%3E%3C%2Fsup%3E%3C%2Fsup%3E+%3D+e%3Csup%3E%7B-%28y-4.5%29%7D%3C%2Fsup%3E+>として，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%3Csup%3E%7Bm%7D%5Cfrac%7Bf%28Y%3Csup%3E%7B%28i%29%7D%29%7D%7Bg%28Y%3Csup%3E%7B%28i%29%7D%29%7D%3C%2Fsup%3E%3C%2Fsup%3E%3C%2Fsup%3E+>を求める．ここで，gの選択の箇所は「gを，4.5で切り詰めた指数分布の密度<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BExp%7D%281%29+>とする」ということ(P.81)．</p>

<p><code>r
Nsim &lt;- 10^3
y &lt;- rexp(Nsim)+4.5
weit &lt;- dnorm(y)/dexp(y-4.5)
plot(cumsum(weit)/1:Nsim, type="l")
abline(h=pnorm(-4.5), col="red", lwd=2)
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp3_3-5_1.png" alt="" /></p>

<h3>練習問題 3.5 切り詰められた指数分布<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Cmathcal%7BExp%7D%28%5Clambda%29+>をg(x)としたときの近似値の分散への影響を調べる</h3>

<p>例3.5で<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clambda+%3D+1+>だった指数分布の値を色々と変えてみて実験する．以下の図は，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clambda+%3D+1%2C5%2C10%2C20+>の時の推定値の変化を示している．左上の図が例3.5と同じ条件となり，そこから右上・左下・右下と<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clambda+>の値を大きくしている．黒の線が推定値の推移，上下の黄色の線が推定値の誤差の範囲（＝分散）となっている．
このプロットを見ると，<img src=http://chart.apis.google.com/chart?cht=tx&chl=%5Clambda+>の値が大きくなるにつれて，分散も大きくなることがわかる．</p>

<p><code>r
par(mfrow=c(2,2))
for(lambda in c(1, 5, 10, 20)){
  Nsim &lt;- 10^4
  y &lt;- rexp(Nsim)/lambda+4.5
  weit &lt;- dnorm(y)/dexp(y-4.5, lambda)
  estint &lt;- cumsum(weit)/1:Nsim
  esterr &lt;- sqrt(cumsum((weit-estint)^2))/(1:Nsim)
  plot(estint, xlab="Mean and error range", ylab="prob", type="l", main=paste("lambda = ",lambda))
  lines(estint+2*esterr, col="gold", lwd=2)
  lines(estint-2*esterr, col="gold", lwd=2)
  abline(h=pnorm(-4.5), col="red")
}
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp3_3-5_2.png" alt="" /></p>

<p>また，がくっと値が変わっている部分は乱数でかなりレアな値を引いて全体的に値が引きずられたからだと思われる．weitの中から値の非常に大きいものを10個選び出して，その時の推定値の変化を見たのが以下の図になる．青の点線で示した箇所が，weitの値が非常に大きくなったポイントを示している．</p>

<p><code>r
abline(v=seq(1,Nsim)[rank(weit)&gt;Nsim-10], col="blue", lty=2)
</code></p>

<p><img src="http://dl.dropbox.com/u/142306/b/imcmr/chp3_3-5_3.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
