<p>1;2c&#8212;
layout: post
title: &#8220;「今度こそわかる!? PRMLの学習の学習」に参加しました&#8221;
date: 2012-10-12 10:03
comments: true</p>

<h2>categories: PRML</h2>

<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621061240" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<iframe src="http://rcm-jp.amazon.co.jp/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=yagays-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=ss_til&asins=4621061224" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" align="right"></iframe>


<p>昨日に池袋ジュンク堂で開催されたPRML同人誌トークセッションに参加してました．講演者の皆様お疲れ様でした＆貴重なお話をありがとうございました．</p>

<ul>
<li><a href="http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk">http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk</a></li>
</ul>


<p>ということで，トークセッションの中で話題に上がった話を私見を交えつつ幾つか．全部の話題を網羅しているわけではないのでご了承下さい．</p>

<br />


<h3>「PRMLの学習」の2版が出た</h3>

<p>なんと「パターン認識と機械学習の学習」が早くも2版ということで，どこかで見覚えのある黄色い表紙．私はもう既に1版を買っているので2版を買おうかどうか悩んでいたら，トークセッションの受付で2版で新たに加わった内容が印刷された小冊子を頂いた．</p>

<h3>PRML翻訳は@shima__shimaさん自らが同僚や出版社に話を持ちかけて実現した</h3>

<p>翻訳に関しては，本の中では分担されているということしか分からなかったので，ちょっとした裏話．あとは，Wikiを立てて訳語の統一をされたということで，fittingやjoint distributionなどで議論が紛糾したという話も．</p>

<h3>モデルというのはモノの見方である</h3>

<p>機械学習全般の話でもよく出てくるが，今回のトークセッションでもやはり話題になったのはモデルに関する問題だった．機械学習では何らかの関数に近似したり分類したりと，未来を予測するためにはモデルを立ててパラメータをチューニングして…ということが必要になるのだが，じゃあどうやってモデルを組み上げていけばいいのか．何が良いモデルで何が悪いモデルなのか．適当にモデルを組んだら機械学習の結果が現実のデータに上手い具合に当てはまった/予想に反して全然当てはまらなかった場合には，どう解釈すればいいのか．モデルを選ぶ以前に特徴量を選ぶところにおいても，どんな工夫をすればいいのか．．．などなど，手法にかぎらず色々な部分で正当性や性能を評価する必要があり，理論が固まった現在でも人が介入する部分というのは多いという話．それを克服(?)したのが今話題のDeep Learningで，膨大なデータ量を元にして，特徴量の選択自体も機械学習の中に含めてしまおうという話とも繋がった．</p>

<h3>贋作を愛でる</h3>

<p>モデルの話に関連して，ノーフリーランチ定理やボックス氏の言葉などが話の中に出てきたが，そういった流れの中で統数研の樋口知之先生が仰られている「贋作」という単語が出てきて，個人的にはそのニュアンスが非常に腑に落ちる感じで印象に残った．最終的には現実問題を解決したいということが根本にあると思うのだが，機械学習なんかは理論などは置いておいて単純に動けばいいやという考えもあり，逆に何でも適用しようとすると論文を量産する機械になってしまったり，SOMみたいな万能性に溺れてしまうという話もあり，なかなか難しい部分ではある．</p>

<ul>
<li><a href="http://d.hatena.ne.jp/what_a_dude/20110902/p1">http://d.hatena.ne.jp/what_a_dude/20110902/p1</a></li>
<li><a href="http://www.ai-gakkai.or.jp/jsai/sig/dmsm/007/discussion.html">http://www.ai-gakkai.or.jp/jsai/sig/dmsm/007/discussion.html</a></li>
<li><a href="http://www.ai-gakkai.or.jp/jsai/activity/241_p162_165.pdf">http://www.ai-gakkai.or.jp/jsai/activity/241_p162_165.pdf</a> (pdf)</li>
</ul>


<h3>余談：PRMLの学習と実装に関する疑問について．</h3>

<p>実装の話はトークセッションではあまり出て来なかったのだが，帰り際の電車の中で実装に関する疑問をふと思いついて「さっき訊いときゃよかった」と思ったので，折角なので書いてみる．</p>

<p>プログラマーなどは，アルゴリズムを自分で実装しないと本当に理解したことにならない，なんてことがよく言われるけれども，そういう環境で育った技術系の人がPRMLを読み始めると「書いてあることをひと通りまんべんなく実装しないといけない」みたいな感じになって，取っ掛かりとしては非常に辛いんじゃないかなーという個人的な印象がある．</p>

<p>例えば，尤度関数でlogを取って対数尤度にするのは小さな確率の値を掛け算していくと非常に小さい値になって計算機で扱うには困るからlogを取って足し算にしているんだよ，という実装上の工夫は非常によくされる説明だけれども，じゃあ他の機械学習の理論においても，実装のことを考慮した勉強をしないといけないんじゃないかという感覚は今でもある．</p>

<p>そのあたり，トークセッションの中で話があったように，理論レベルの難しさと実装レベルの難しさは別物だし，今は良質の機械学習パッケージが充実しているから機械学習を使うだけなら自分で実装する必要性は必ずしも無いけれども，「加減」の問題として，どの程度やったらいいのかという疑問．まあ，現実の問題を解こうと思った時には，やはりその分野（自然言語処理であったりレコメンドであったり）の機械学習本を参考にしたほうが良いのだけれども．</p>

<p>そういえば@Shuyoさんが「数式をnumpyに落としこむコツ」という解説を書かれており，非常に分かりやすくていつも参考にしているのだが，そのあたり「プログラマのためのPRMLの学習」同人誌があるといいんじゃないかと思う．</p>

<ul>
<li><a href="http://d.hatena.ne.jp/n_shuyo/20111016/numpy">http://d.hatena.ne.jp/n_shuyo/20111016/numpy</a></li>
</ul>

